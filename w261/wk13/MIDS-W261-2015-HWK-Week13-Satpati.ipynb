{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sayantan Satpati**\n",
    "* **sayantan.satpati@ischool.berkeley.edu**\n",
    "* **W261**\n",
    "* **Week-13**\n",
    "* **Assignment-13**\n",
    "* **Date of Submission: 08-DEC-2015**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  === Week 13 ASSIGNMENTS ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Initialize Spark in Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SPARK_HOME=\"/Users/ssatpati/0-DATASCIENCE/TOOLS/spark-1.5.1-bin-hadoop2.6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "def init_spark():\n",
    "    #Escape L for line numbers\n",
    "    spark_home = os.environ['SPARK_HOME'] = SPARK_HOME\n",
    "    if not spark_home:\n",
    "        raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "    sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "    sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "    execfile(os.path.join(spark_home,'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.5.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.10 (default, Oct 19 2015 18:31:17)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "init_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW13.1 Spark implementation of basic PageRank\n",
    "---\n",
    "\n",
    "```\n",
    "===HW 13.1: Spark implementation of basic PageRank===\n",
    "\n",
    "Write a basic Spark implementation of the iterative PageRank algorithm\n",
    "that takes sparse adjacency lists as input.\n",
    "Make sure that your implementation utilizes teleportation (1-damping/the number of nodes in the network), \n",
    "and further, distributes the mass of dangling nodes with each iteration\n",
    "so that the output of each iteration is correctly normalized (sums to 1).\n",
    "[NOTE: The PageRank algorithm assumes that a random surfer (walker), starting from a random web page,\n",
    "chooses the next page to which it will move by clicking at random, with probability d,\n",
    "one of the hyperlinks in the current page. This probability is represented by a so-called\n",
    "‘damping factor’ d, where d ∈ (0, 1). Otherwise, with probability (1 − d), the surfer\n",
    "jumps to any web page in the network. If a page is a dangling end, meaning it has no\n",
    "outgoing hyperlinks, the random surfer selects an arbitrary web page from a uniform\n",
    "distribution and “teleports” to that page]\n",
    "\n",
    "In your Spark solution, please use broadcast variables and caching to make sure your code is as efficient as possible.\n",
    "\n",
    "As you build your code, use the following test data to check you implementation:\n",
    "\n",
    "s3://ucb-mids-mls-networks/PageRank-test.txt\n",
    "\n",
    "Set the teleportation parameter  to 0.15 (1-d, where d, the damping factor is set to 0.85), and crosscheck\n",
    "your work with the true result, displayed in the first image\n",
    "in the Wikipedia article:\n",
    "\n",
    "https://en.wikipedia.org/wiki/PageRank\n",
    "\n",
    "and here for reference are the corresponding resulting PageRank probabilities:\n",
    "\n",
    "A,0.033\n",
    "B,0.384\n",
    "C,0.343\n",
    "D,0.039\n",
    "E,0.081\n",
    "F,0.039\n",
    "G,0.016\n",
    "H,0.016\n",
    "I,0.016\n",
    "J,0.016\n",
    "K,0.016\n",
    "\n",
    "Run this experiment locally first. Report the local configuration that you used and how long in minutes and seconds it takes to complete your job.\n",
    "\n",
    "Repeat this experiment on AWS. Report the AWS cluster configuration that you used and how long in minutes and seconds it takes to complete your job. (in your notebook, cat the cluster config file)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spark_13_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spark_13_1.py\n",
    "import ast\n",
    "import pprint\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "def u(s):\n",
    "    return s.decode('utf-8')\n",
    "\n",
    "def parse_line(line):\n",
    "    tokens = line.split('\\t')\n",
    "    key = tokens[0]\n",
    "    adj_list = ast.literal_eval(tokens[1])\n",
    "    return (u(key), [u(k) for k,v in adj_list.iteritems()])\n",
    "\n",
    "def preproc(t):\n",
    "    l = [t]\n",
    "    for x in t[1]:\n",
    "        l.append((u(x),[]))\n",
    "    return l\n",
    "\n",
    "def contributions(t):\n",
    "    l = [(t[0], 0)]\n",
    "    w = t[1][1]\n",
    "    adj_list = t[1][0]\n",
    "    key = None\n",
    "    if len(adj_list) == 0:\n",
    "        l.append(('DANGLING', w))\n",
    "    else:\n",
    "        for n in adj_list:\n",
    "            l.append((n, w/len(adj_list)))\n",
    "    return l\n",
    "\n",
    "def page_rank(t, n, dangling_mass, tp=0.15):\n",
    "    w = t[1]\n",
    "    w = (tp / n) + (1 - tp) * ((dangling_mass/n) + w)\n",
    "    return (t[0], w)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print 'Number of arguments:', len(sys.argv), 'arguments.'\n",
    "    print 'Argument List:', str(sys.argv)\n",
    "    \n",
    "    if len(sys.argv) != 3:\n",
    "        print 'Incorrect number of arguments passed, Aborting...'\n",
    "        sys.exit(1)\n",
    "        \n",
    "    # Init Spark Context\n",
    "    #conf = SparkConf()\n",
    "    sc = SparkContext(appName=\"Page Rank\")\n",
    "    \n",
    "    lines = sc.textFile(sys.argv[1]).map(parse_line)\n",
    "    #print '\\n### Original Dataset:'\n",
    "    #pprint.pprint(lines.sortByKey().collect())\n",
    "\n",
    "    links = lines.flatMap(preproc).reduceByKey(lambda x, y: x + y).cache()\n",
    "    #print '\\n### Pre-Processed Dataset (Links):'\n",
    "    #pprint.pprint(links.collect())\n",
    "\n",
    "    n = links.count()\n",
    "    \n",
    "    ranks = links.map(lambda x: (x[0], float(1)/n))\n",
    "    #print '\\n### Inital Ranks:'\n",
    "    #pprint.pprint(ranks.collect())\n",
    "\n",
    "    sum_partial_diff_PR = float('inf')\n",
    "    cnt = 1\n",
    "\n",
    "    #while sum_partial_diff_PR > .005:\n",
    "    while cnt <= 30:\n",
    "        contribs = links.join(ranks).flatMap(contributions).reduceByKey(lambda x, y: x + y).cache()\n",
    "        dangling_mass = contribs.lookup('DANGLING')\n",
    "        ranks_updated = contribs.filter(lambda x: x[0] != 'DANGLING').map(lambda x: page_rank(x, n, dangling_mass[0]))\n",
    "        print '\\n[Iteration: {0}] Dangling Mass: {1}'.format(cnt, dangling_mass[0])\n",
    "        \n",
    "        #print 'Sum of Ranks: {0}'.format(ranks_updated.values().reduce(lambda x, y: x + y))\n",
    "        #sum_partial_diff_PR = ranks.join(ranks_updated).map(lambda x: abs(x[1][0] - x[1][1])).reduce(lambda x, y: x + y)\n",
    "        #print 'Difference in Ranks: {0}'.format(sum_partial_diff_PR)\n",
    "        ranks = ranks_updated\n",
    "        cnt += 1\n",
    "\n",
    "    ranks.map(lambda x: (x[0],round(x[1],3))).saveAsTextFile(sys.argv[2])\n",
    "    \n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x spark_13_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running it in local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of arguments: 3 arguments.\n",
      "Argument List: ['/Users/ssatpati/0-DATASCIENCE/DEV/github/ml/w261/wk13/./spark_13_1.py', 'PageRank-test.txt', 'output_13_1']\n",
      "2015-12-06 14:47:19.083 java[35243:22269489] Unable to load realm mapping info from SCDynamicStore\n",
      "\n",
      "[Iteration: 1] Dangling Mass: 0.0909090909091\n",
      "\n",
      "[Iteration: 2] Dangling Mass: 0.0592975206612\n",
      "\n",
      "[Iteration: 3] Dangling Mass: 0.0379464062109\n",
      "\n",
      "[Iteration: 4] Dangling Mass: 0.0640190695934\n",
      "\n",
      "[Iteration: 5] Dangling Mass: 0.0375959647951\n",
      "\n",
      "[Iteration: 6] Dangling Mass: 0.0386749363905\n",
      "\n",
      "[Iteration: 7] Dangling Mass: 0.0341177257382\n",
      "\n",
      "[Iteration: 8] Dangling Mass: 0.0346526855821\n",
      "\n",
      "[Iteration: 9] Dangling Mass: 0.0332641479909\n",
      "\n",
      "[Iteration: 10] Dangling Mass: 0.0332687068063\n",
      "\n",
      "[Iteration: 11] Dangling Mass: 0.0329301017862\n",
      "\n",
      "[Iteration: 12] Dangling Mass: 0.0329194443643\n",
      "\n",
      "[Iteration: 13] Dangling Mass: 0.0328282893463\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 14] Dangling Mass: 0.0328197384167\n",
      "\n",
      "[Iteration: 15] Dangling Mass: 0.0327957341511\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 16] Dangling Mass: 0.0327922737609\n",
      "\n",
      "[Iteration: 17] Dangling Mass: 0.0327858041648\n",
      "\n",
      "[Iteration: 18] Dangling Mass: 0.0327845372023\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 19] Dangling Mass: 0.0327827832963\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 20] Dangling Mass: 0.0327823578046\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 21] Dangling Mass: 0.032781877034\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 22] Dangling Mass: 0.0327817396015\n",
      "\n",
      "[Iteration: 23] Dangling Mass: 0.0327816067481\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 24] Dangling Mass: 0.032781563641\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 25] Dangling Mass: 0.0327815266405\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 26] Dangling Mass: 0.0327815133704\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 27] Dangling Mass: 0.0327815029967\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 28] Dangling Mass: 0.0327814989681\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 29] Dangling Mass: 0.0327814960427\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 30] Dangling Mass: 0.0327814948319\n",
      "\n",
      "real\t0m39.249s\n",
      "user\t0m40.728s\n",
      "sys\t0m20.918s\n",
      "(u'A', 0.033)\n",
      "(u'B', 0.384)\n",
      "(u'C', 0.344)\n",
      "(u'D', 0.039)\n",
      "(u'E', 0.081)\n",
      "(u'F', 0.039)\n",
      "(u'G', 0.016)\n",
      "(u'H', 0.016)\n",
      "(u'I', 0.016)\n",
      "(u'J', 0.016)\n",
      "(u'K', 0.016)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf output_13_1\n",
    "!time $SPARK_HOME/bin/spark-submit --name \"Page Rank\" --master local[4] ./spark_13_1.py PageRank-test.txt output_13_1\n",
    "!cat output_13_1/part-000* | sort\n",
    "!end=$(date +%s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running it in AWS\n",
    "\n",
    "#### Following Steps have been done to run this on AWS:\n",
    "\n",
    "1. Create Spark Cluster\n",
    "1. Copy python file to Spark Master\n",
    "2. Run the Program from Spark Master\n",
    "\n",
    "```\n",
    "aws --region us-west-2 ec2 create-key-pair --key-name w261_key --query 'KeyMaterial' --output text > w261_key.pem\n",
    "\n",
    "aws emr create-cluster --name \"spark1\" --ami-version 3.8 --applications Name=Spark --ec2-attributes KeyName=w261_key --log-uri s3://ucb-mids-mls-sayantan-satpati/spark/logs --instance-type m3.xlarge --instance-count 3 --use-default-roles\n",
    "\n",
    "scp -i /Users/ssatpati/0-DATASCIENCE/DEV/AWS/keys_w261/w261_key.pem spark_13_1.py hadoop@ec2-52-27-224-148.us-west-2.compute.amazonaws.com:/home/hadoop\n",
    "\n",
    "/home/hadoop/spark/bin/spark-submit --master yarn-cluster /home/hadoop/spark_13_1.py s3n://ucb-mids-mls-networks/PageRank-test.txt s3n://ucb-mids-mls-sayantan-satpati/spark/hw13_1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00007 to aws_output_13_1/part-00007\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00000 to aws_output_13_1/part-00000\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00003 to aws_output_13_1/part-00003\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00008 to aws_output_13_1/part-00008\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00001 to aws_output_13_1/part-00001\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00006 to aws_output_13_1/part-00006\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/_SUCCESS to aws_output_13_1/_SUCCESS\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00004 to aws_output_13_1/part-00004\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00002 to aws_output_13_1/part-00002\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00009 to aws_output_13_1/part-00009\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00010 to aws_output_13_1/part-00010\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00011 to aws_output_13_1/part-00011\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00014 to aws_output_13_1/part-00014\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00013 to aws_output_13_1/part-00013\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00015 to aws_output_13_1/part-00015\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00017 to aws_output_13_1/part-00017\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00016 to aws_output_13_1/part-00016\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00005 to aws_output_13_1/part-00005\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00019 to aws_output_13_1/part-00019\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00020 to aws_output_13_1/part-00020\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00012 to aws_output_13_1/part-00012\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00021 to aws_output_13_1/part-00021\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00022 to aws_output_13_1/part-00022\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00023 to aws_output_13_1/part-00023\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00025 to aws_output_13_1/part-00025\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00018 to aws_output_13_1/part-00018\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00026 to aws_output_13_1/part-00026\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00024 to aws_output_13_1/part-00024\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00027 to aws_output_13_1/part-00027\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00028 to aws_output_13_1/part-00028\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00029 to aws_output_13_1/part-00029\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00030 to aws_output_13_1/part-00030\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00031 to aws_output_13_1/part-00031\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00032 to aws_output_13_1/part-00032\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00033 to aws_output_13_1/part-00033\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00035 to aws_output_13_1/part-00035\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00034 to aws_output_13_1/part-00034\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00036 to aws_output_13_1/part-00036\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00038 to aws_output_13_1/part-00038\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00037 to aws_output_13_1/part-00037\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00040 to aws_output_13_1/part-00040\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00041 to aws_output_13_1/part-00041\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00042 to aws_output_13_1/part-00042\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00043 to aws_output_13_1/part-00043\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00044 to aws_output_13_1/part-00044\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00045 to aws_output_13_1/part-00045\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00046 to aws_output_13_1/part-00046\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00048 to aws_output_13_1/part-00048\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00047 to aws_output_13_1/part-00047\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00049 to aws_output_13_1/part-00049\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00051 to aws_output_13_1/part-00051\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00052 to aws_output_13_1/part-00052\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00055 to aws_output_13_1/part-00055\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00054 to aws_output_13_1/part-00054\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00053 to aws_output_13_1/part-00053\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00056 to aws_output_13_1/part-00056\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00039 to aws_output_13_1/part-00039\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00057 to aws_output_13_1/part-00057\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00050 to aws_output_13_1/part-00050\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00058 to aws_output_13_1/part-00058\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00059 to aws_output_13_1/part-00059\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00060 to aws_output_13_1/part-00060\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00061 to aws_output_13_1/part-00061\n",
      "(u'A', 0.033000000000000002)\n",
      "(u'B', 0.38400000000000001)\n",
      "(u'C', 0.34399999999999997)\n",
      "(u'D', 0.039)\n",
      "(u'E', 0.081000000000000003)\n",
      "(u'F', 0.039)\n",
      "(u'G', 0.016)\n",
      "(u'H', 0.016)\n",
      "(u'I', 0.016)\n",
      "(u'J', 0.016)\n",
      "(u'K', 0.016)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf aws_output_13_1\n",
    "!aws s3 cp --recursive s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1 aws_output_13_1\n",
    "!cat aws_output_13_1/part-* | sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spark_13_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spark_13_2.py\n",
    "import ast\n",
    "import pprint\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "def u(s):\n",
    "    return s.decode('utf-8')\n",
    "\n",
    "def parse_line(line):\n",
    "    tokens = line.split('\\t')\n",
    "    key = tokens[0]\n",
    "    adj_list = ast.literal_eval(tokens[1])\n",
    "    return (u(key), [u(k) for k,v in adj_list.iteritems()])\n",
    "\n",
    "def preproc(t):\n",
    "    l = [t]\n",
    "    for x in t[1]:\n",
    "        l.append((u(x),[]))\n",
    "    return l\n",
    "\n",
    "def contributions(t):\n",
    "    l = [(t[0], 0)]\n",
    "    w = t[1][1]\n",
    "    adj_list = t[1][0]\n",
    "    key = None\n",
    "    if len(adj_list) == 0:\n",
    "        l.append(('DANGLING', w))\n",
    "    else:\n",
    "        for n in adj_list:\n",
    "            l.append((n, w/len(adj_list)))\n",
    "    return l\n",
    "\n",
    "def page_rank(t, n, dangling_mass, tp=0.15):\n",
    "    w = t[1]\n",
    "    w = (tp / n) + (1 - tp) * ((dangling_mass/n) + w)\n",
    "    return (t[0], w)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sys.stderr.write('\\nNumber of arguments: {0}'.format(len(sys.argv)))\n",
    "    sys.stderr.write('\\nArgument List: {0}'.format(sys.argv))\n",
    "    \n",
    "    if len(sys.argv) != 4:\n",
    "        print 'Incorrect number of arguments passed, Aborting...'\n",
    "        sys.exit(1)\n",
    "        \n",
    "    # Init Spark Context\n",
    "    #conf = SparkConf()\n",
    "    sc = SparkContext(appName=\"Page Rank\")\n",
    "    \n",
    "    lines = sc.textFile(sys.argv[1]).map(parse_line)\n",
    "\n",
    "    links = lines.flatMap(preproc).reduceByKey(lambda x, y: x + y).cache()\n",
    "    n = links.count()\n",
    "    \n",
    "    ranks = links.map(lambda x: (x[0], float(1)/n))\n",
    "\n",
    "    sum_partial_diff_PR = float('inf')\n",
    "    cnt = 1\n",
    "\n",
    "    #while sum_partial_diff_PR > .005:\n",
    "    while cnt <= int(sys.argv[3]):\n",
    "        contribs = links.join(ranks).flatMap(contributions).reduceByKey(lambda x, y: x + y).cache()\n",
    "        dangling_mass = contribs.lookup('DANGLING')\n",
    "        ranks_updated = contribs.filter(lambda x: x[0] != 'DANGLING').map(lambda x: page_rank(x, n, dangling_mass[0]))\n",
    "        sys.stderr.write('\\n[Iteration: {0}] Dangling Mass: {1}'.format(cnt, dangling_mass[0]))\n",
    "        \n",
    "        ranks = ranks_updated\n",
    "        cnt += 1\n",
    "\n",
    "    sc.parallelize(ranks.map(lambda x: (x[0],round(x[1],3))).takeOrdered(3, key=lambda x: -x[1])).saveAsTextFile(sys.argv[2])\n",
    "    \n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod u+x spark_13_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of arguments: 4\n",
      "Argument List: ['/Users/ssatpati/0-DATASCIENCE/DEV/github/ml/w261/wk13/./spark_13_2.py', 'PageRank-test.txt', 'output_13_2', '10']2015-12-06 18:45:48.110 java[55494:22389589] Unable to load realm mapping info from SCDynamicStore\n",
      "\n",
      "[Iteration: 1] Dangling Mass: 0.0909090909091\n",
      "[Iteration: 2] Dangling Mass: 0.0592975206612\n",
      "[Iteration: 3] Dangling Mass: 0.0379464062109\n",
      "[Iteration: 4] Dangling Mass: 0.0640190695934\n",
      "[Iteration: 5] Dangling Mass: 0.0375959647951\n",
      "[Iteration: 6] Dangling Mass: 0.0386749363905\n",
      "[Iteration: 7] Dangling Mass: 0.0341177257382\n",
      "[Iteration: 8] Dangling Mass: 0.0346526855821\n",
      "[Iteration: 9] Dangling Mass: 0.0332641479909\n",
      "[Iteration: 10] Dangling Mass: 0.0332687068063\n",
      "real\t0m12.018s\n",
      "user\t0m16.799s\n",
      "sys\t0m3.164s\n",
      "(u'B', 0.363)\n",
      "(u'C', 0.363)\n",
      "(u'E', 0.081)\n"
     ]
    }
   ],
   "source": [
    "out_dir = \"output_13_2\"\n",
    "!rm -rf $out_dir\n",
    "!time $SPARK_HOME/bin/spark-submit --name \"Page Rank\" --master local[4] ./spark_13_2.py \\\n",
    "                PageRank-test.txt $out_dir 10\n",
    "!cat $out_dir/part-000* | sort\n",
    "!end=$(date +%s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
