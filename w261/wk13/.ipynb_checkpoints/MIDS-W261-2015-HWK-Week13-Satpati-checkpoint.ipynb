{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sayantan Satpati**\n",
    "* **sayantan.satpati@ischool.berkeley.edu**\n",
    "* **W261**\n",
    "* **Week-13**\n",
    "* **Assignment-13**\n",
    "* **Date of Submission: 08-DEC-2015**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  === Week 13 ASSIGNMENTS ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Initialize Spark in Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SPARK_HOME=\"/Users/ssatpati/0-DATASCIENCE/TOOLS/spark-1.5.1-bin-hadoop2.6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.5.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.10 (default, Oct 19 2015 18:31:17)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "#Escape L for line numbers\n",
    "spark_home = os.environ['SPARK_HOME'] = SPARK_HOME\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW13.1 Spark implementation of basic PageRank\n",
    "---\n",
    "\n",
    "```\n",
    "===HW 13.1: Spark implementation of basic PageRank===\n",
    "\n",
    "Write a basic Spark implementation of the iterative PageRank algorithm\n",
    "that takes sparse adjacency lists as input.\n",
    "Make sure that your implementation utilizes teleportation (1-damping/the number of nodes in the network), \n",
    "and further, distributes the mass of dangling nodes with each iteration\n",
    "so that the output of each iteration is correctly normalized (sums to 1).\n",
    "[NOTE: The PageRank algorithm assumes that a random surfer (walker), starting from a random web page,\n",
    "chooses the next page to which it will move by clicking at random, with probability d,\n",
    "one of the hyperlinks in the current page. This probability is represented by a so-called\n",
    "‘damping factor’ d, where d ∈ (0, 1). Otherwise, with probability (1 − d), the surfer\n",
    "jumps to any web page in the network. If a page is a dangling end, meaning it has no\n",
    "outgoing hyperlinks, the random surfer selects an arbitrary web page from a uniform\n",
    "distribution and “teleports” to that page]\n",
    "\n",
    "In your Spark solution, please use broadcast variables and caching to make sure your code is as efficient as possible.\n",
    "\n",
    "As you build your code, use the following test data to check you implementation:\n",
    "\n",
    "s3://ucb-mids-mls-networks/PageRank-test.txt\n",
    "\n",
    "Set the teleportation parameter  to 0.15 (1-d, where d, the damping factor is set to 0.85), and crosscheck\n",
    "your work with the true result, displayed in the first image\n",
    "in the Wikipedia article:\n",
    "\n",
    "https://en.wikipedia.org/wiki/PageRank\n",
    "\n",
    "and here for reference are the corresponding resulting PageRank probabilities:\n",
    "\n",
    "A,0.033\n",
    "B,0.384\n",
    "C,0.343\n",
    "D,0.039\n",
    "E,0.081\n",
    "F,0.039\n",
    "G,0.016\n",
    "H,0.016\n",
    "I,0.016\n",
    "J,0.016\n",
    "K,0.016\n",
    "\n",
    "Run this experiment locally first. Report the local configuration that you used and how long in minutes and seconds it takes to complete your job.\n",
    "\n",
    "Repeat this experiment on AWS. Report the AWS cluster configuration that you used and how long in minutes and seconds it takes to complete your job. (in your notebook, cat the cluster config file)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page Rank Version 1.0 - Using 2 RDDs: Links & Ranks\n",
    "\n",
    "***Didn't scale in AWS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spark_13_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spark_13_1.py\n",
    "import ast\n",
    "import pprint\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "def u(s):\n",
    "    return s.decode('utf-8')\n",
    "\n",
    "def parse_line(line):\n",
    "    tokens = line.split('\\t')\n",
    "    key = tokens[0]\n",
    "    adj_list = ast.literal_eval(tokens[1])\n",
    "    return (u(key), [u(k) for k,v in adj_list.iteritems()])\n",
    "\n",
    "def preproc(t):\n",
    "    l = [t]\n",
    "    for x in t[1]:\n",
    "        l.append((u(x),[]))\n",
    "    return l\n",
    "\n",
    "def contributions(t):\n",
    "    l = [(t[0], 0)]\n",
    "    w = t[1][1]\n",
    "    adj_list = t[1][0]\n",
    "    key = None\n",
    "    if len(adj_list) == 0:\n",
    "        l.append(('DANGLING', w))\n",
    "    else:\n",
    "        for n in adj_list:\n",
    "            l.append((n, w/len(adj_list)))\n",
    "    return l\n",
    "\n",
    "def page_rank(t, n, dangling_mass, tp=0.15):\n",
    "    w = t[1]\n",
    "    w = (tp / n) + (1 - tp) * ((dangling_mass/n) + w)\n",
    "    return (t[0], w)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print 'Number of arguments:', len(sys.argv), 'arguments.'\n",
    "    print 'Argument List:', str(sys.argv)\n",
    "    \n",
    "    if len(sys.argv) != 3:\n",
    "        print 'Incorrect number of arguments passed, Aborting...'\n",
    "        sys.exit(1)\n",
    "        \n",
    "    # Init Spark Context\n",
    "    #conf = SparkConf()\n",
    "    sc = SparkContext(appName=\"Page Rank\")\n",
    "    \n",
    "    lines = sc.textFile(sys.argv[1]).map(parse_line)\n",
    "    #print '\\n### Original Dataset:'\n",
    "    #pprint.pprint(lines.sortByKey().collect())\n",
    "\n",
    "    links = lines.flatMap(preproc).reduceByKey(lambda x, y: x + y).cache()\n",
    "    #print '\\n### Pre-Processed Dataset (Links):'\n",
    "    #pprint.pprint(links.collect())\n",
    "\n",
    "    n = links.count()\n",
    "    \n",
    "    ranks = links.map(lambda x: (x[0], float(1)/n))\n",
    "    #print '\\n### Inital Ranks:'\n",
    "    #pprint.pprint(ranks.collect())\n",
    "\n",
    "    sum_partial_diff_PR = float('inf')\n",
    "    cnt = 1\n",
    "\n",
    "    #while sum_partial_diff_PR > .005:\n",
    "    while cnt <= 30:\n",
    "        contribs = links.join(ranks).flatMap(contributions).reduceByKey(lambda x, y: x + y).cache()\n",
    "        dangling_mass = contribs.lookup('DANGLING')\n",
    "        ranks_updated = contribs.filter(lambda x: x[0] != 'DANGLING').map(lambda x: page_rank(x, n, dangling_mass[0]))\n",
    "        print '\\n[Iteration: {0}] Dangling Mass: {1}'.format(cnt, dangling_mass[0])\n",
    "        \n",
    "        #print 'Sum of Ranks: {0}'.format(ranks_updated.values().reduce(lambda x, y: x + y))\n",
    "        #sum_partial_diff_PR = ranks.join(ranks_updated).map(lambda x: abs(x[1][0] - x[1][1])).reduce(lambda x, y: x + y)\n",
    "        #print 'Difference in Ranks: {0}'.format(sum_partial_diff_PR)\n",
    "        ranks = ranks_updated\n",
    "        cnt += 1\n",
    "\n",
    "    ranks.map(lambda x: (x[0],round(x[1],3))).saveAsTextFile(sys.argv[2])\n",
    "    \n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x spark_13_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running it in local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of arguments: 3 arguments.\n",
      "Argument List: ['/Users/ssatpati/0-DATASCIENCE/DEV/github/ml/w261/wk13/./spark_13_1.py', 'PageRank-test.txt', 'output_13_1']\n",
      "2015-12-06 14:47:19.083 java[35243:22269489] Unable to load realm mapping info from SCDynamicStore\n",
      "\n",
      "[Iteration: 1] Dangling Mass: 0.0909090909091\n",
      "\n",
      "[Iteration: 2] Dangling Mass: 0.0592975206612\n",
      "\n",
      "[Iteration: 3] Dangling Mass: 0.0379464062109\n",
      "\n",
      "[Iteration: 4] Dangling Mass: 0.0640190695934\n",
      "\n",
      "[Iteration: 5] Dangling Mass: 0.0375959647951\n",
      "\n",
      "[Iteration: 6] Dangling Mass: 0.0386749363905\n",
      "\n",
      "[Iteration: 7] Dangling Mass: 0.0341177257382\n",
      "\n",
      "[Iteration: 8] Dangling Mass: 0.0346526855821\n",
      "\n",
      "[Iteration: 9] Dangling Mass: 0.0332641479909\n",
      "\n",
      "[Iteration: 10] Dangling Mass: 0.0332687068063\n",
      "\n",
      "[Iteration: 11] Dangling Mass: 0.0329301017862\n",
      "\n",
      "[Iteration: 12] Dangling Mass: 0.0329194443643\n",
      "\n",
      "[Iteration: 13] Dangling Mass: 0.0328282893463\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 14] Dangling Mass: 0.0328197384167\n",
      "\n",
      "[Iteration: 15] Dangling Mass: 0.0327957341511\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 16] Dangling Mass: 0.0327922737609\n",
      "\n",
      "[Iteration: 17] Dangling Mass: 0.0327858041648\n",
      "\n",
      "[Iteration: 18] Dangling Mass: 0.0327845372023\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 19] Dangling Mass: 0.0327827832963\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 20] Dangling Mass: 0.0327823578046\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 21] Dangling Mass: 0.032781877034\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 22] Dangling Mass: 0.0327817396015\n",
      "\n",
      "[Iteration: 23] Dangling Mass: 0.0327816067481\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 24] Dangling Mass: 0.032781563641\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 25] Dangling Mass: 0.0327815266405\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 26] Dangling Mass: 0.0327815133704\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 27] Dangling Mass: 0.0327815029967\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 28] Dangling Mass: 0.0327814989681\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 29] Dangling Mass: 0.0327814960427\n",
      "                                                                                                                                                                                                            \n",
      "[Iteration: 30] Dangling Mass: 0.0327814948319\n",
      "\n",
      "real\t0m39.249s\n",
      "user\t0m40.728s\n",
      "sys\t0m20.918s\n",
      "(u'A', 0.033)\n",
      "(u'B', 0.384)\n",
      "(u'C', 0.344)\n",
      "(u'D', 0.039)\n",
      "(u'E', 0.081)\n",
      "(u'F', 0.039)\n",
      "(u'G', 0.016)\n",
      "(u'H', 0.016)\n",
      "(u'I', 0.016)\n",
      "(u'J', 0.016)\n",
      "(u'K', 0.016)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf output_13_1\n",
    "!time $SPARK_HOME/bin/spark-submit --name \"Page Rank\" --master local[4] ./spark_13_1.py PageRank-test.txt output_13_1\n",
    "!cat output_13_1/part-000* | sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running it in AWS\n",
    "\n",
    "#### Following Steps have been done to run this on AWS:\n",
    "\n",
    "1. Create Spark Cluster\n",
    "1. Copy python file to Spark Master\n",
    "2. Run the Program from Spark Master\n",
    "\n",
    "```\n",
    "aws --region us-west-2 ec2 create-key-pair --key-name w261_key --query 'KeyMaterial' --output text > w261_key.pem\n",
    "\n",
    "aws emr create-cluster --name \"spark1\" --ami-version 3.8 --applications Name=Spark --ec2-attributes KeyName=w261_key --log-uri s3://ucb-mids-mls-sayantan-satpati/spark/logs --instance-type m3.xlarge --instance-count 3 --use-default-roles\n",
    "\n",
    "scp -i /Users/ssatpati/0-DATASCIENCE/DEV/AWS/keys_w261/w261_key.pem spark_13_1.py hadoop@ec2-52-27-224-148.us-west-2.compute.amazonaws.com:/home/hadoop\n",
    "\n",
    "/home/hadoop/spark/bin/spark-submit --master yarn-cluster /home/hadoop/spark_13_1.py s3n://ucb-mids-mls-networks/PageRank-test.txt s3n://ucb-mids-mls-sayantan-satpati/spark/hw13_1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00007 to aws_output_13_1/part-00007\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00000 to aws_output_13_1/part-00000\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00003 to aws_output_13_1/part-00003\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00008 to aws_output_13_1/part-00008\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00001 to aws_output_13_1/part-00001\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00006 to aws_output_13_1/part-00006\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/_SUCCESS to aws_output_13_1/_SUCCESS\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00004 to aws_output_13_1/part-00004\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00002 to aws_output_13_1/part-00002\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00009 to aws_output_13_1/part-00009\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00010 to aws_output_13_1/part-00010\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00011 to aws_output_13_1/part-00011\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00014 to aws_output_13_1/part-00014\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00013 to aws_output_13_1/part-00013\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00015 to aws_output_13_1/part-00015\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00017 to aws_output_13_1/part-00017\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00016 to aws_output_13_1/part-00016\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00005 to aws_output_13_1/part-00005\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00019 to aws_output_13_1/part-00019\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00020 to aws_output_13_1/part-00020\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00012 to aws_output_13_1/part-00012\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00021 to aws_output_13_1/part-00021\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00022 to aws_output_13_1/part-00022\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00023 to aws_output_13_1/part-00023\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00025 to aws_output_13_1/part-00025\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00018 to aws_output_13_1/part-00018\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00026 to aws_output_13_1/part-00026\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00024 to aws_output_13_1/part-00024\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00027 to aws_output_13_1/part-00027\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00028 to aws_output_13_1/part-00028\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00029 to aws_output_13_1/part-00029\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00030 to aws_output_13_1/part-00030\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00031 to aws_output_13_1/part-00031\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00032 to aws_output_13_1/part-00032\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00033 to aws_output_13_1/part-00033\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00035 to aws_output_13_1/part-00035\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00034 to aws_output_13_1/part-00034\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00036 to aws_output_13_1/part-00036\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00038 to aws_output_13_1/part-00038\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00037 to aws_output_13_1/part-00037\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00040 to aws_output_13_1/part-00040\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00041 to aws_output_13_1/part-00041\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00042 to aws_output_13_1/part-00042\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00043 to aws_output_13_1/part-00043\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00044 to aws_output_13_1/part-00044\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00045 to aws_output_13_1/part-00045\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00046 to aws_output_13_1/part-00046\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00048 to aws_output_13_1/part-00048\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00047 to aws_output_13_1/part-00047\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00049 to aws_output_13_1/part-00049\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00051 to aws_output_13_1/part-00051\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00052 to aws_output_13_1/part-00052\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00055 to aws_output_13_1/part-00055\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00054 to aws_output_13_1/part-00054\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00053 to aws_output_13_1/part-00053\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00056 to aws_output_13_1/part-00056\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00039 to aws_output_13_1/part-00039\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00057 to aws_output_13_1/part-00057\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00050 to aws_output_13_1/part-00050\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00058 to aws_output_13_1/part-00058\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00059 to aws_output_13_1/part-00059\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00060 to aws_output_13_1/part-00060\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1/part-00061 to aws_output_13_1/part-00061\n",
      "(u'A', 0.033000000000000002)\n",
      "(u'B', 0.38400000000000001)\n",
      "(u'C', 0.34399999999999997)\n",
      "(u'D', 0.039)\n",
      "(u'E', 0.081000000000000003)\n",
      "(u'F', 0.039)\n",
      "(u'G', 0.016)\n",
      "(u'H', 0.016)\n",
      "(u'I', 0.016)\n",
      "(u'J', 0.016)\n",
      "(u'K', 0.016)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf aws_output_13_1\n",
    "!aws s3 cp --recursive s3://ucb-mids-mls-sayantan-satpati/spark/hw13_1 aws_output_13_1\n",
    "!cat aws_output_13_1/part-* | sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 13.2\n",
    "___\n",
    "\n",
    "===HW 13.2: Applying PageRank to the Wikipedia hyperlinks network===\n",
    "\n",
    "Run your Spark PageRank implementation on the Wikipedia dataset for 10 iterations,\n",
    "and display the top 100 ranked nodes (with alpha = 0.85).\n",
    "\n",
    "Run your PageRank implementation on the Wikipedia dataset for 50 iterations,\n",
    "and display the top 100 ranked nodes (with teleportation factor of 0.15). \n",
    "Plot the pagerank values for the top 100 pages resulting from the 50 iterations run. Then plot the pagerank values for the same 100 pages that resulted from the 10 iterations run.  Comment on your findings.  Have the top 100 ranked pages changed? Have the pagerank values changed? Explain.\n",
    "\n",
    "Report the AWS cluster configuration that you used and how long in minutes and seconds it takes to complete your job.\n",
    "\n",
    "NOTE: Wikipedia data is located on S3 at  s3://ucb-mids-mls-networks/wikipedia/\n",
    "-- s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt # Graph\n",
    "-- s3://ucb-mids-mls-networks/wikipedia/indices.txt               # Page titles and page Ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page Rank Version 2.0 - Using 1 RDD\n",
    "\n",
    "***Scalable Solution without Joins***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spark_13_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spark_13_2.py\n",
    "import ast\n",
    "import pprint\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "def u(s):\n",
    "    return s.decode('utf-8')\n",
    "\n",
    "def parse_line(line):\n",
    "    tokens = line.split('\\t')\n",
    "    key = tokens[0]\n",
    "    adj_list = ast.literal_eval(tokens[1])\n",
    "    return (u(key), [u(k) for k,v in adj_list.iteritems()])\n",
    "\n",
    "def preproc(t):\n",
    "    l = [t]\n",
    "    for x in t[1]:\n",
    "        l.append((u(x),[]))\n",
    "    return l\n",
    "\n",
    "def contributions(t):\n",
    "    adj_list = t[1][0]\n",
    "    w = t[1][1]\n",
    "    \n",
    "    # Emit the Graph/AdjList\n",
    "    l = [(t[0], (adj_list, 0))]\n",
    "    \n",
    "    #Emit the Weights\n",
    "    if len(adj_list) == 0:\n",
    "        l.append(('DANGLING', ([], w)))\n",
    "    else:\n",
    "        for n in adj_list:\n",
    "            l.append((n, ([], w/len(adj_list))))\n",
    "    return l\n",
    "\n",
    "def page_rank(t, n, dangling_mass, tp=0.15):\n",
    "    adj_list = t[1][0]\n",
    "    w = t[1][1]\n",
    "    w = (tp / n) + (1 - tp) * ((dangling_mass/n) + w)\n",
    "    return (t[0], (adj_list, w))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sys.stderr.write('\\nNumber of arguments: {0}'.format(len(sys.argv)))\n",
    "    sys.stderr.write('\\nArgument List: {0}'.format(sys.argv))\n",
    "    \n",
    "    if len(sys.argv) != 4:\n",
    "        print 'Incorrect number of arguments passed, Aborting...'\n",
    "        sys.exit(1)\n",
    "        \n",
    "    # Init Spark Context\n",
    "    #conf = SparkConf()\n",
    "    sc = SparkContext(appName=\"Page Rank\")\n",
    "    \n",
    "    lines = sc.textFile(sys.argv[1]).map(parse_line)\n",
    "\n",
    "    pr = lines.flatMap(preproc).reduceByKey(lambda x, y: x + y)\n",
    "    n = pr.count()\n",
    "    pr = pr.map(lambda x: (x[0],(x[1], float(1)/n)))\n",
    "    \n",
    "    #pprint.pprint(pr.collect())\n",
    "    \n",
    "    cnt = 1\n",
    "    \n",
    "    \n",
    "    #while sum_partial_diff_PR > .005:\n",
    "    while cnt <= int(sys.argv[3]):\n",
    "        contribs = pr.flatMap(contributions).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))        \n",
    "        #print '\\n'\n",
    "        #pprint.pprint(contribs.collect())\n",
    "        dangling_mass = contribs.filter(lambda x: x[0] == 'DANGLING').collectAsMap()['DANGLING'][1]\n",
    "        sys.stderr.write('\\n[{0}] Dangling Mass: {1}'.format(cnt, dangling_mass))\n",
    "        pr = contribs.filter(lambda x: x[0] != 'DANGLING').map(lambda x: page_rank(x, n, dangling_mass))        \n",
    "        #print '\\n'\n",
    "        #pprint.pprint(contribs.collect())\n",
    "        \n",
    "        cnt += 1\n",
    "    \n",
    "    print '\\n'\n",
    "    pprint.pprint(pr.sortByKey().collect())\n",
    "    sc.parallelize(pr.map(lambda x: (x[0],(x[1][0], round(x[1][1],3))))\n",
    "                        .takeOrdered(3, key=lambda x: -x[1][1])).saveAsTextFile(sys.argv[2])\n",
    "    \n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod u+x spark_13_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test in Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of arguments: 4\n",
      "Argument List: ['/Users/ssatpati/0-DATASCIENCE/DEV/github/ml/w261/wk13/./spark_13_2.py', 'PageRank-test.txt', 'output_13_2', '30']2015-12-07 23:56:34.774 java[59853:23223850] Unable to load realm mapping info from SCDynamicStore\n",
      "[(u'A', ([], 0.09090909090909091)),\n",
      " (u'C', ([u'B'], 0.09090909090909091)),\n",
      " (u'E', ([u'B', u'D', u'F'], 0.09090909090909091)),\n",
      " (u'G', ([u'B', u'E'], 0.09090909090909091)),\n",
      " (u'I', ([u'B', u'E'], 0.09090909090909091)),\n",
      " (u'K', ([u'E'], 0.09090909090909091)),\n",
      " (u'H', ([u'B', u'E'], 0.09090909090909091)),\n",
      " (u'J', ([u'E'], 0.09090909090909091)),\n",
      " (u'B', ([u'C'], 0.09090909090909091)),\n",
      " (u'D', ([u'A', u'B'], 0.09090909090909091)),\n",
      " (u'F', ([u'B', u'E'], 0.09090909090909091))]\n",
      "\n",
      "[1] Dangling Mass: 0.0909090909091\n",
      "[2] Dangling Mass: 0.0592975206612\n",
      "[3] Dangling Mass: 0.0379464062109\n",
      "[4] Dangling Mass: 0.0640190695934\n",
      "[5] Dangling Mass: 0.0375959647951\n",
      "[6] Dangling Mass: 0.0386749363905\n",
      "[7] Dangling Mass: 0.0341177257382\n",
      "[8] Dangling Mass: 0.0346526855821\n",
      "[9] Dangling Mass: 0.0332641479909\n",
      "[10] Dangling Mass: 0.0332687068063\n",
      "[11] Dangling Mass: 0.0329301017862\n",
      "[12] Dangling Mass: 0.0329194443643\n",
      "[13] Dangling Mass: 0.0328282893463\n",
      "[14] Dangling Mass: 0.0328197384167\n",
      "[15] Dangling Mass: 0.0327957341511\n",
      "[16] Dangling Mass: 0.0327922737609\n",
      "[17] Dangling Mass: 0.0327858041648\n",
      "[18] Dangling Mass: 0.0327845372023\n",
      "[19] Dangling Mass: 0.0327827832963\n",
      "[20] Dangling Mass: 0.0327823578046\n",
      "[21] Dangling Mass: 0.032781877034\n",
      "[22] Dangling Mass: 0.0327817396015\n",
      "[23] Dangling Mass: 0.0327816067481\n",
      "[24] Dangling Mass: 0.032781563641\n",
      "[25] Dangling Mass: 0.0327815266405\n",
      "[26] Dangling Mass: 0.0327815133704\n",
      "[27] Dangling Mass: 0.0327815029967\n",
      "[28] Dangling Mass: 0.0327814989681\n",
      "[29] Dangling Mass: 0.0327814960427\n",
      "[30] Dangling Mass: 0.0327814948319\n",
      "\n",
      "[(u'A', ([], 0.03278149400279799)),\n",
      " (u'B', ([u'C'], 0.38359681276778973)),\n",
      " (u'C', ([u'B'], 0.34371441659524926)),\n",
      " (u'D', ([u'A', u'B'], 0.03908709308143793)),\n",
      " (u'E', ([u'B', u'D', u'F'], 0.08088569474079228)),\n",
      " (u'F', ([u'B', u'E'], 0.03908709308143793)),\n",
      " (u'G', ([u'B', u'E'], 0.01616947914609888)),\n",
      " (u'H', ([u'B', u'E'], 0.01616947914609888)),\n",
      " (u'I', ([u'B', u'E'], 0.01616947914609888)),\n",
      " (u'J', ([u'E'], 0.01616947914609888)),\n",
      " (u'K', ([u'E'], 0.01616947914609888))]\n",
      "\n",
      "real\t0m10.042s\n",
      "user\t0m13.860s\n",
      "sys\t0m1.844s\n",
      "(u'B', ([u'C'], 0.384))\n",
      "(u'C', ([u'B'], 0.344))\n",
      "(u'E', ([u'B', u'D', u'F'], 0.081))\n"
     ]
    }
   ],
   "source": [
    "out_dir = \"output_13_2\"\n",
    "!rm -rf $out_dir\n",
    "!time $SPARK_HOME/bin/spark-submit --name \"Page Rank\" --master local[4] ./spark_13_2.py \\\n",
    "                PageRank-test.txt $out_dir 30\n",
    "!cat $out_dir/part-000* | sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 13.4: Criteo Phase 2 baseline\n",
    "___\n",
    "\n",
    "SPECIAL NOTE:\n",
    "Please share your findings as they become available with class via the Google Group. You will get brownie points for this.  Once results are shared please used them and build on them.\n",
    "\n",
    "The  Criteo data is located in the following S3 bucket:  criteo-dataset \n",
    "https://console.aws.amazon.com/s3/home?region=us-west-1#&bucket=criteo-dataset&prefix=\n",
    "\n",
    "Using the training dataset, validation dataset and testing dataset in the Criteo bucket perform the following experiment:\n",
    "\n",
    "-- write spark code (borrow from Phase 1 of this project) to train a logistic regression model with the following hyperparamters:\n",
    "\n",
    "-- Number of buckets for hashing: 1,000\n",
    "-- Logistic Regression: no regularization term\n",
    "-- Logistic Regression: step size = 10\n",
    "\n",
    "Report the AWS cluster configuration that you used and how long in minutes and seconds it takes to complete this job.\n",
    "\n",
    "Report in tabular form the AUC value (https://en.wikipedia.org/wiki/Receiver_operating_characteristic) for the Training, Validation, and Testing datasets.\n",
    "Report in tabular form  the logLossTest for the Training, Validation, and Testing datasets.\n",
    "\n",
    "Dont forget to put a caption on your tables (above each table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spark_13_4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spark_13_4.py\n",
    "import ast\n",
    "import pprint\n",
    "import sys\n",
    "from math import log, exp\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "\n",
    "def parse_line(line):\n",
    "    t = ast.literal_eval(line)\n",
    "    return LabeledPoint(t[0], SparseVector(t[1][0], t[1][1], t[1][2]))\n",
    "\n",
    "def computeLogLoss(p, y):\n",
    "    \"\"\"Calculates the value of log loss for a given probabilty and label.\n",
    "\n",
    "    Note:\n",
    "        log(0) is undefined, so when p is 0 we need to add a small value (epsilon) to it\n",
    "        and when p is 1 we need to subtract a small value (epsilon) from it.\n",
    "\n",
    "    Args:\n",
    "        p (float): A probabilty between 0 and 1.\n",
    "        y (int): A label.  Takes on the values 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The log loss value.\n",
    "    \"\"\"\n",
    "    epsilon = 10e-12\n",
    "    if p == 0:\n",
    "        p = p + epsilon\n",
    "    if p == 1:\n",
    "        p = p - epsilon\n",
    "    return -(y * log(p) + (1-y) * log(1-p))\n",
    "\n",
    "def getP(x, w, intercept):\n",
    "    \"\"\"Calculate the probability for an observation given a set of weights and intercept.\n",
    "\n",
    "    Note:\n",
    "        We'll bound our raw prediction between 20 and -20 for numerical purposes.\n",
    "\n",
    "    Args:\n",
    "        x (SparseVector): A vector with values of 1.0 for features that exist in this\n",
    "            observation and 0.0 otherwise.\n",
    "        w (DenseVector): A vector of weights (betas) for the model.\n",
    "        intercept (float): The model's intercept.\n",
    "\n",
    "    Returns:\n",
    "        float: A probability between 0 and 1.\n",
    "    \"\"\"\n",
    "    rawPrediction = x.dot(w) + intercept\n",
    "\n",
    "    # Bound the raw prediction value\n",
    "    rawPrediction = min(rawPrediction, 20)\n",
    "    rawPrediction = max(rawPrediction, -20)\n",
    "    return 1 / (1 + exp(-rawPrediction))\n",
    "\n",
    "def evaluateResults(model, data):\n",
    "    \"\"\"Calculates the log loss for the data given the model.\n",
    "\n",
    "    Args:\n",
    "        model (LogisticRegressionModel): A trained logistic regression model.\n",
    "        data (RDD of LabeledPoint): Labels and features for each observation.\n",
    "\n",
    "    Returns:\n",
    "        float: Log loss for the data.\n",
    "    \"\"\"\n",
    "    labelPredictions = data.map(lambda lp: (getP(lp.features, model.weights, model.intercept), lp.label))\n",
    "    return labelPredictions.map(lambda (p,l): computeLogLoss(p,l)).sum() / labelPredictions.count()\n",
    "\n",
    "def metrics(model, data, label):\n",
    "    labelsAndScores = data.map(lambda lp:\n",
    "                            (lp.label, getP(lp.features, model0.weights, model0.intercept)))\n",
    "    \n",
    "    metrics = BinaryClassificationMetrics(labelsAndScores)\n",
    "    log_loss = evaluateResults(model0, OHETrainData)\n",
    "    auc = metrics.areaUnderROC\n",
    "    sys.stderr.write('\\n [{0}] LogLoss: {1}'.format(label, log_loss))\n",
    "    sys.stderr.write('\\n [{0}] AUC: {1}\\n'.format(label, auc))\n",
    "    return (label, log_loss, auc)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sys.stderr.write('\\nNumber of arguments: {0}'.format(len(sys.argv)))\n",
    "    sys.stderr.write('\\nArgument List: {0}'.format(sys.argv))\n",
    "    \n",
    "    if len(sys.argv) != 5:\n",
    "        print 'Incorrect number of arguments passed, Aborting...'\n",
    "        sys.exit(1)\n",
    "        \n",
    "    # Init Spark Context\n",
    "    #conf = SparkConf()\n",
    "    sc = SparkContext(appName=\"Logistic Regression\")\n",
    "    \n",
    "    OHETrainData = sc.textFile(sys.argv[1]).map(parse_line).cache()\n",
    "    OHETestData = sc.textFile(sys.argv[2]).map(parse_line).cache()\n",
    "    OHEValidateData = sc.textFile(sys.argv[3]).map(parse_line).cache()\n",
    "    \n",
    "    #print '\\n', OHETrainData.take(3)\n",
    "    \n",
    "    # fixed hyperparameters\n",
    "    numIters = 50\n",
    "    stepSize = 10.\n",
    "    #regParam = 1e-6\n",
    "    regParam = 0. # No Reg\n",
    "    regType = 'l2'\n",
    "    includeIntercept = True\n",
    "    \n",
    "    model0 = LogisticRegressionWithSGD.train(OHETrainData, iterations=numIters, step=stepSize, \n",
    "                                   regParam=regParam, regType=regType, intercept=includeIntercept)\n",
    "    sortedWeights = sorted(model0.weights)\n",
    "    \n",
    "    \n",
    "    \n",
    "    sys.stderr.write('\\n### Model Intercept: {0}'.format(model0.intercept))\n",
    "    sys.stderr.write('\\n### Model Weights (First 5): {0}\\n'.format(sortedWeights[:5]))\n",
    "    \n",
    "    l = []\n",
    "    \n",
    "    l.append(metrics(model0, OHETrainData, 'TRAIN'))\n",
    "    l.append(metrics(model0, OHETestData, 'TEST'))\n",
    "    l.append(metrics(model0, OHEValidateData, 'VALIDATE'))\n",
    "    \n",
    "    sc.parallelize(l).saveAsTextFile(sys.argv[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod u+x spark_13_4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing in Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of arguments: 5\n",
      "Argument List: ['/Users/ssatpati/0-DATASCIENCE/DEV/github/ml/w261/wk13/./spark_13_4.py', 'criteo-parsed-test.5', 'criteo-parsed-test.5', 'criteo-parsed-test.5', 'output_13_4']2015-12-07 23:25:05.438 java[50414:23184532] Unable to load realm mapping info from SCDynamicStore\n",
      "\n",
      "### Model Intercept: 0.375282618437\n",
      "### Model Weights (First 5): [-2.9242343145200196, -2.9242343145200196, -2.5767079097974506, -1.46211715726001, -1.46211715726001]\n",
      "\n",
      " [TRAIN] LogLoss: 2.06115362768e-09\n",
      " [TRAIN] AUC: 1.0\n",
      "\n",
      " [TEST] LogLoss: 2.06115362768e-09\n",
      " [TEST] AUC: 1.0\n",
      "\n",
      " [VALIDATE] LogLoss: 2.06115362768e-09\n",
      " [VALIDATE] AUC: 1.0\n",
      "\n",
      "real\t0m9.875s\n",
      "user\t0m14.310s\n",
      "sys\t0m1.686s\n",
      "('TEST', 2.0611536276785456e-09, 1.0)\n",
      "('TRAIN', 2.0611536276785456e-09, 1.0)\n",
      "('VALIDATE', 2.0611536276785456e-09, 1.0)\n"
     ]
    }
   ],
   "source": [
    "out_dir = \"output_13_4\"\n",
    "!rm -rf $out_dir\n",
    "!time $SPARK_HOME/bin/spark-submit --master local[4] ./spark_13_4.py \\\n",
    "                criteo-parsed-test.5 criteo-parsed-test.5 criteo-parsed-test.5 $out_dir\n",
    "!cat $out_dir/part-000* | sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS\n",
    "\n",
    "***Cluster Size: 6 m3.xlarge***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
