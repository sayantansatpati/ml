{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sayantan Satpati**\n",
    "* **sayantan.satpati@ischool.berkeley.edu**\n",
    "* **W261**\n",
    "* **Week-1**\n",
    "* **Assignment-2**\n",
    "* **Date of Submission: 07-SEP-2015**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#This notebook implements a Spam Filter backed by a Multinomial Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# Import a bunch of libraries.\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# Set the randomizer seed so results are the same each time.\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.0\n",
    "\n",
    "**Define big data. Provide an example of a big data problem in your domain of expertise.**\n",
    "\n",
    "Big data is a broad term for data sets so large or complex that traditional data processing applications are inadequate, and cannot be processed or analyzed in a single computer. Challenges include analysis, capture, data curation, search, sharing, storage, transfer, visualization, and information privacy. The term often refers simply to the use of predictive analytics or other certain advanced methods to extract value from data, and seldom to a particular size of data set. Big Data is also characterized by the 4 V's: Volume, Velocity, Variety, and Veracity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_HW12.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_HW12.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def strip_special_chars(word):\n",
    "    return re.sub('[^A-Za-z0-9]+', '', word)\n",
    "\n",
    "count = 0\n",
    "filename = sys.argv[1]\n",
    "wordList = sys.argv[2]\n",
    "wordList = wordList.split()\n",
    "wordCountDict = {}\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        # Split the line by <TAB> delimeter\n",
    "        email = re.split(r'\\t+', line)\n",
    "        \n",
    "        # Check whether Content is present\n",
    "        if len(email) < 4:\n",
    "            continue\n",
    "        \n",
    "        # Get the content as a list of words\n",
    "        content = email[len(email) - 1].split()\n",
    "        \n",
    "        if len(wordList) == 1 and wordList[0] == '*':\n",
    "            for w in content:\n",
    "                w = strip_special_chars(w)\n",
    "                if w not in wordCountDict:\n",
    "                    wordCountDict[w] = 1\n",
    "                else:\n",
    "                    wordCountDict[w] += 1\n",
    "        else:\n",
    "            for w in content:\n",
    "                w = strip_special_chars(w)\n",
    "                # Check if word is in word list passed to mapper\n",
    "                if w in wordList:\n",
    "                    if w not in wordCountDict:\n",
    "                        wordCountDict[w] = 1\n",
    "                    else:\n",
    "                        wordCountDict[w] += 1\n",
    "       \n",
    "# Print count from each mapper\n",
    "for k,v in wordCountDict.items():\n",
    "    print \"{0}\\t{1}\".format(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper_HW12.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_HW12.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_HW12.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "cnt = 0\n",
    "wordCountDict = {}\n",
    "for file in sys.argv:\n",
    "    if cnt == 0:\n",
    "        cnt += 1\n",
    "        continue\n",
    "        \n",
    "    with open (file, \"r\") as myfile:\n",
    "        for line in myfile:\n",
    "            wc = re.split(r'\\t+', line.strip())\n",
    "            if wc[0] not in wordCountDict:\n",
    "                wordCountDict[wc[0]] = int(wc[1])\n",
    "            else:\n",
    "                wordCountDict[wc[0]] += int(wc[1])\n",
    "                \n",
    "# Print count from each mapper\n",
    "for k,v in wordCountDict.items():\n",
    "    print \"{0}\\t{1}\".format(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer_HW12.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: License.txt.*: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# Remove split files from last runs\n",
    "! rm License.txt.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write control script 'pNaiveBayes.sh' to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## Mapper and Reducer Files are passed to make this script generic\n",
    "mapper=$3\n",
    "reducer=$4\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./${mapper} $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./${reducer} $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*\n",
    "\n",
    "## Display the Output\n",
    "cat $data.output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HW1.1. Read through the provided control script (pNaiveBayes.sh)\n",
    "'''\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Run the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage: usage: pGrepCount filename word chuncksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and\t631\r\n",
      "of\t546\r\n",
      "the\t1217\r\n"
     ]
    }
   ],
   "source": [
    "# Test the Program\n",
    "!./pNaiveBayes.sh 4 'the and of' 'mapper_HW12.py' 'reducer_HW12.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t9\r\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HW1.2. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "'''\n",
    "!./pNaiveBayes.sh 4 'assistance' 'mapper_HW12.py' 'reducer_HW12.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_HW15.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_HW15.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def strip_special_chars(word):\n",
    "    word = word.strip()\n",
    "    \n",
    "    if not word or word == '':\n",
    "        return None\n",
    "    \n",
    "    word = re.sub('[^A-Za-z0-9]+', '', word)\n",
    "    return word.lower()\n",
    "\n",
    "count = 0\n",
    "filename = sys.argv[1]\n",
    "wordList = sys.argv[2]\n",
    "wordList = wordList.split()\n",
    "\n",
    "# (Line#, Spam/Ham, Dict of Word|Count)\n",
    "mapper_output_list = []\n",
    "line_num = 0\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        # Split the line by <TAB> delimeter\n",
    "        email = re.split(r'\\t+', line)\n",
    "        \n",
    "        # Check whether Content is present\n",
    "        if len(email) < 4:\n",
    "            continue\n",
    "            \n",
    "        line_num += 1\n",
    "        \n",
    "        # Get the content as a list of words\n",
    "        content = email[len(email) - 1].split()\n",
    "        \n",
    "        wordCountDict = {}\n",
    "        for w in content:\n",
    "            w = strip_special_chars(w)\n",
    "            \n",
    "            if not w:\n",
    "                continue\n",
    "                \n",
    "            if w not in wordCountDict:\n",
    "                wordCountDict[w] = 1\n",
    "            else:\n",
    "                wordCountDict[w] += 1\n",
    "                \n",
    "        mapper_output_list.append((line_num, email[1], wordCountDict))\n",
    "       \n",
    "# Print output from each mapper\n",
    "for (line_num, spam, wordCountDict) in mapper_output_list:\n",
    "    for word,count in wordCountDict.items():\n",
    "        print \"{0}\\t{1}\\t{2}\\t{3}\".format(line_num, spam, word, count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper_HW15.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_HW15.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_HW15.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "\n",
    "# Totals\n",
    "vocab = 0\n",
    "vocab_spam = 0\n",
    "vocab_ham = 0\n",
    "\n",
    "vocab = {}\n",
    "word_counts = {\n",
    "    \"1\": {},\n",
    "    \"0\": {}\n",
    "}\n",
    "\n",
    "num_spam = 0\n",
    "num_ham = 0\n",
    "\n",
    "cnt = 0\n",
    "# Calculate the totals in Reducer First Pass\n",
    "for file in sys.argv:\n",
    "    if cnt == 0:\n",
    "        cnt += 1\n",
    "        continue\n",
    "        \n",
    "    with open (file, \"r\") as myfile:\n",
    "        last_line_num = -1\n",
    "        last_spam = -1\n",
    "        \n",
    "        for line in myfile:\n",
    "            tokens = re.split(r'\\t+', line.strip())\n",
    "            line_num = int(tokens[0])\n",
    "            spam = int(tokens[1])\n",
    "            word = tokens[2]\n",
    "            count = float(tokens[3])\n",
    "            \n",
    "            # Init\n",
    "            if last_line_num == -1:\n",
    "                last_line_num = line_num\n",
    "                last_spam = spam\n",
    "            \n",
    "            # Add Vocab per line\n",
    "            if word not in vocab:\n",
    "                vocab[word] = 0.0\n",
    "            if word not in word_counts[str(spam)]:\n",
    "                word_counts[str(spam)][word] = 0.0\n",
    "            vocab[word] += count\n",
    "            word_counts[str(spam)][word] += count\n",
    "                    \n",
    "            if last_line_num != line_num:\n",
    "                if last_spam == 1:\n",
    "                    num_spam += 1\n",
    "                else:\n",
    "                    num_ham += 1\n",
    "                \n",
    "            last_line_num = line_num\n",
    "            last_spam = spam\n",
    "            \n",
    "        # Last Line\n",
    "        if last_spam == 1:\n",
    "            num_spam += 1\n",
    "        else:\n",
    "            num_ham += 1\n",
    "                \n",
    "# At the end of first pass\n",
    "print 'Num Spam: {0}, Num Ham: {1}'.format(num_spam, num_ham)\n",
    "print '''Total Vocab: {0},\n",
    "       Total Unique Vocab: {1},\n",
    "       Total Spam Vocab: {2}, \n",
    "       Total Ham Vocab: {3}'''.format(sum(vocab.values()), \n",
    "                                    len(vocab),\n",
    "                                    sum(word_counts['1'].values()), \n",
    "                                    sum(word_counts['0'].values())\n",
    "                                   )\n",
    "                                    \n",
    "\n",
    "prior_spam = (num_spam * 1.0) / (num_spam + num_ham)\n",
    "prior_ham = (num_ham * 1.0) / (num_spam + num_ham)\n",
    "print '[Priors] Spam: {0}, Ham: {1}'.format(prior_spam, prior_ham)\n",
    "\n",
    "spam_likelihood_denom = sum(word_counts['1'].values()) + len(vocab)\n",
    "ham_likelihood_denom = sum(word_counts['0'].values()) + len(vocab)\n",
    "\n",
    "# Calculate the Conditionals/Likelihood in Next Pass\n",
    "reducer_output_list = []\n",
    "cnt = 0\n",
    "for file in sys.argv:\n",
    "    if cnt == 0:\n",
    "        cnt += 1\n",
    "        continue\n",
    "        \n",
    "    with open (file, \"r\") as myfile:\n",
    "        last_line_num = -1\n",
    "        log_prob_spam = 0\n",
    "        log_prob_ham = 0\n",
    "        \n",
    "        for line in myfile:\n",
    "            \n",
    "            tokens = re.split(r'\\t+', line.strip())\n",
    "            line_num = int(tokens[0])\n",
    "            spam = int(tokens[1])\n",
    "            word = tokens[2]\n",
    "            count = int(tokens[3])\n",
    "            \n",
    "            # Init\n",
    "            if last_line_num == -1:\n",
    "                last_line_num = line_num\n",
    "            \n",
    "            if last_line_num != line_num:\n",
    "                # Calculate the Naive Bayes Scores for Document Classification\n",
    "                spam_score = log_prob_spam + math.log(prior_spam)\n",
    "                ham_score = log_prob_ham + math.log(prior_ham)\n",
    "                reducer_output_list.append((spam, spam_score, ham_score))\n",
    "                # Reset log prob\n",
    "                log_prob_spam = 0\n",
    "                log_prob_ham = 0\n",
    "            else:\n",
    "                # Calcuate the log likelihoods Using Laplace Smoothing\n",
    "                spam_likelihood = (word_counts['1'].get(word, 0.0) + 1) / spam_likelihood_denom\n",
    "                ham_likelihood = (word_counts['0'].get(word, 0.0) + 1) / ham_likelihood_denom\n",
    "                log_prob_spam += math.log( spam_likelihood )\n",
    "                log_prob_ham += math.log( ham_likelihood )\n",
    "            \n",
    "            last_line_num = line_num\n",
    "            \n",
    "        # Last Line\n",
    "        spam_score = log_prob_spam + math.log(prior_spam)\n",
    "        ham_score = log_prob_ham + math.log(prior_ham)\n",
    "        reducer_output_list.append((spam, spam_score, ham_score))\n",
    "        \n",
    "total = 0.0\n",
    "miscat = 0.0\n",
    "for (spam, spam_score, ham_score) in reducer_output_list:\n",
    "        total += 1.0\n",
    "        pred_class = 'HAM'\n",
    "        if spam_score > ham_score:\n",
    "            pred_class = 'SPAM'\n",
    "        if (spam == 1 and pred_class == 'HAM') or (spam == 0 and pred_class == 'SPAM'):\n",
    "            miscat += 1.0\n",
    "            \n",
    "        print \"{0}\\t{1}\\t{2}\\t{3}\".format(spam, spam_score, ham_score, pred_class)\n",
    "\n",
    "error = miscat * 100 / total\n",
    "print \"Accuracy: {0}, Error Rate: {1}, # of Miscats: {2}\".format((100 - error), error, miscat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer_HW15.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Spam: 43, Num Ham: 59\r\n",
      "Total Vocab: 30316.0,\r\n",
      "       Total Unique Vocab: 5601,\r\n",
      "       Total Spam Vocab: 17851.0, \r\n",
      "       Total Ham Vocab: 12465.0\r\n",
      "[Priors] Spam: 0.421568627451, Ham: 0.578431372549\r\n",
      "0\t-9.82787146701\t-9.65607518653\tHAM\r\n",
      "0\t-9.31704584324\t-8.55746289786\tHAM\r\n",
      "0\t-2191.1588365\t-2021.72420184\tHAM\r\n",
      "0\t-281.842510908\t-248.630590416\tHAM\r\n",
      "0\t-916.441638904\t-878.151478825\tHAM\r\n",
      "0\t-1653.48319844\t-1492.50829857\tHAM\r\n",
      "1\t-395.32418664\t-381.836278447\tHAM\r\n",
      "1\t-469.326129307\t-514.959604259\tSPAM\r\n",
      "1\t-929.769132314\t-1010.88945311\tSPAM\r\n",
      "0\t-580.653856258\t-614.353545243\tSPAM\r\n",
      "0\t-308.945761337\t-276.031549971\tHAM\r\n",
      "0\t-47.2260842694\t-40.2045304237\tHAM\r\n",
      "0\t-950.866291476\t-848.782383396\tHAM\r\n",
      "1\t-1072.4835333\t-960.299272087\tHAM\r\n",
      "1\t-606.745205434\t-633.062794683\tSPAM\r\n",
      "0\t-640.316398508\t-676.856179047\tSPAM\r\n",
      "0\t-899.109077747\t-813.432461852\tHAM\r\n",
      "0\t-639.476372133\t-553.677146111\tHAM\r\n",
      "1\t-486.410820816\t-462.414233728\tHAM\r\n",
      "1\t-611.928765714\t-658.589558436\tSPAM\r\n",
      "0\t-635.74110717\t-666.787750109\tSPAM\r\n",
      "0\t-696.435672685\t-629.002221062\tHAM\r\n",
      "0\t-681.831135474\t-598.769957542\tHAM\r\n",
      "0\t-369.245483103\t-351.476873109\tHAM\r\n",
      "0\t-718.371237407\t-652.160468206\tHAM\r\n",
      "0\t-0.863772697591\t-0.547435369379\tHAM\r\n",
      "1\t-156.00375198\t-164.804412617\tSPAM\r\n",
      "0\t-4309.48846682\t-4554.2657293\tSPAM\r\n",
      "0\t-449.579304968\t-428.547287677\tHAM\r\n",
      "0\t-4939.1098802\t-4573.78504473\tHAM\r\n",
      "1\t-244.905381381\t-235.58412329\tHAM\r\n",
      "1\t-311.635400148\t-325.911403557\tSPAM\r\n",
      "1\t-831.664842063\t-873.750665201\tSPAM\r\n",
      "0\t-879.275299271\t-925.688581271\tSPAM\r\n",
      "0\t-975.290611491\t-889.980938447\tHAM\r\n",
      "0\t-504.915432143\t-460.268894626\tHAM\r\n",
      "0\t-1898.93680073\t-1639.92363215\tHAM\r\n",
      "1\t-1343.34660959\t-1220.86994229\tHAM\r\n",
      "1\t-1015.09425808\t-1076.04479956\tSPAM\r\n",
      "0\t-770.626722148\t-854.745216758\tSPAM\r\n",
      "1\t-2819.24353941\t-2554.85851653\tHAM\r\n",
      "1\t-611.928765714\t-658.589558436\tSPAM\r\n",
      "1\t-2762.75629967\t-3018.43630913\tSPAM\r\n",
      "1\t-720.096441716\t-773.495055997\tSPAM\r\n",
      "0\t-3625.37852599\t-3812.89495205\tSPAM\r\n",
      "0\t-2212.35443204\t-1998.82931158\tHAM\r\n",
      "0\t-345.56410633\t-307.093053957\tHAM\r\n",
      "0\t-1926.42342673\t-1743.90911183\tHAM\r\n",
      "1\t-3495.99094154\t-3146.43309235\tHAM\r\n",
      "0\t-706.125164166\t-767.919796417\tSPAM\r\n",
      "0\t-1087.79215933\t-962.879871575\tHAM\r\n",
      "0\t-0.863772697591\t-0.547435369379\tHAM\r\n",
      "0\t-214.286501931\t-192.047553524\tHAM\r\n",
      "1\t-2231.83858785\t-1968.36892017\tHAM\r\n",
      "1\t-2137.50598561\t-2353.95473061\tSPAM\r\n",
      "1\t-0.863772697591\t-0.547435369379\tHAM\r\n",
      "0\t-1395.45972054\t-1503.39187624\tSPAM\r\n",
      "1\t-1024.56020362\t-952.885678046\tHAM\r\n",
      "1\t-2137.50598561\t-2353.95473061\tSPAM\r\n",
      "1\t-6305.6482386\t-6829.84991554\tSPAM\r\n",
      "1\t-331.274463215\t-356.956290233\tSPAM\r\n",
      "0\t-595.848385576\t-635.245056997\tSPAM\r\n",
      "0\t-2096.92848754\t-1909.39364105\tHAM\r\n",
      "0\t-583.521554113\t-504.120608554\tHAM\r\n",
      "0\t-1877.48962277\t-1620.69043194\tHAM\r\n",
      "0\t-683.378112888\t-667.261866064\tHAM\r\n",
      "1\t-446.285412684\t-414.211308778\tHAM\r\n",
      "0\t-137.188458245\t-148.700692887\tSPAM\r\n",
      "0\t-1266.05455359\t-1156.39609571\tHAM\r\n",
      "0\t-664.544822194\t-577.929869077\tHAM\r\n",
      "1\t-555.988884539\t-533.155138506\tHAM\r\n",
      "1\t-9632.34558359\t-10393.7479119\tSPAM\r\n",
      "0\t-1152.86690174\t-1213.90891261\tSPAM\r\n",
      "0\t-898.397412541\t-816.518631296\tHAM\r\n",
      "0\t-885.658525945\t-784.54536986\tHAM\r\n",
      "1\t-1005.3195121\t-907.783323451\tHAM\r\n",
      "1\t-1942.18713384\t-2080.18120214\tSPAM\r\n",
      "1\t-51.9729403403\t-56.2671149023\tSPAM\r\n",
      "1\t-0.863772697591\t-0.547435369379\tHAM\r\n",
      "0\t-720.940952335\t-755.900616572\tSPAM\r\n",
      "0\t-520.429195616\t-477.113304539\tHAM\r\n",
      "0\t-575.967371306\t-495.978867216\tHAM\r\n",
      "0\t-105.987556167\t-100.816889087\tHAM\r\n",
      "1\t-2678.75130225\t-2458.46997589\tHAM\r\n",
      "1\t-693.871997519\t-750.07779373\tSPAM\r\n",
      "0\t-827.955057653\t-893.647169847\tSPAM\r\n",
      "0\t-590.223872151\t-555.959166237\tHAM\r\n",
      "1\t-856.836476187\t-760.555354691\tHAM\r\n",
      "1\t-693.871997519\t-750.07779373\tSPAM\r\n",
      "1\t-5927.74644702\t-6536.51957165\tSPAM\r\n",
      "1\t-687.909284555\t-746.695734673\tSPAM\r\n",
      "0\t-470.574910569\t-512.368166665\tSPAM\r\n",
      "0\t-333.262156386\t-322.809841773\tHAM\r\n",
      "0\t-1887.55233383\t-1630.89768405\tHAM\r\n",
      "1\t-344.209940278\t-333.512567978\tHAM\r\n",
      "1\t-154.338641003\t-164.418599584\tSPAM\r\n",
      "1\t-832.038481237\t-873.001888834\tSPAM\r\n",
      "0\t-1760.75167144\t-1849.52840305\tSPAM\r\n",
      "1\t-712.500652463\t-698.097109431\tHAM\r\n",
      "1\t-1781.06549208\t-1910.65904177\tSPAM\r\n",
      "1\t-2039.19695882\t-2150.8503704\tSPAM\r\n",
      "1\t-0.863772697591\t-0.547435369379\tHAM\r\n",
      "Accuracy: 65.6862745098, Error Rate: 34.3137254902, # of Miscats: 35.0\r\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HW1.5. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by all words present.\n",
    "'''\n",
    "!./pNaiveBayes.sh 4 '*' 'mapper_HW15.py' 'reducer_HW15.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SPAM</th>\n",
       "      <th>SUBJECT</th>\n",
       "      <th>CONTENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001.1999-12-10.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>christmas tree farm pictures</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001.1999-12-10.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>re: rankings</td>\n",
       "      <td>thank you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001.2000-01-17.beck</td>\n",
       "      <td>0</td>\n",
       "      <td>leadership development pilot</td>\n",
       "      <td>sally:  what timing, ask and you shall receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001.2000-06-06.lokay</td>\n",
       "      <td>0</td>\n",
       "      <td>key dates and impact of upcoming sap implemen...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001.2001-02-07.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>key hr issues going forward</td>\n",
       "      <td>a) year end reviews-report needs generating l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ID  SPAM  \\\n",
       "0    0001.1999-12-10.farmer     0   \n",
       "1  0001.1999-12-10.kaminski     0   \n",
       "2      0001.2000-01-17.beck     0   \n",
       "3     0001.2000-06-06.lokay     0   \n",
       "4   0001.2001-02-07.kitchen     0   \n",
       "\n",
       "                                             SUBJECT  \\\n",
       "0                       christmas tree farm pictures   \n",
       "1                                       re: rankings   \n",
       "2                       leadership development pilot   \n",
       "3   key dates and impact of upcoming sap implemen...   \n",
       "4                        key hr issues going forward   \n",
       "\n",
       "                                             CONTENT  \n",
       "0                                                NaN  \n",
       "1                                         thank you.  \n",
       "2   sally:  what timing, ask and you shall receiv...  \n",
       "3                                                NaN  \n",
       "4   a) year end reviews-report needs generating l...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data into Pandas Dataframe\n",
    "df = pd.read_csv('enronemail_1h.txt', sep='\\t', header=None)\n",
    "df.columns = ['ID', 'SPAM', 'SUBJECT', 'CONTENT']\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID         100\n",
      "SPAM       100\n",
      "SUBJECT     98\n",
      "CONTENT     96\n",
      "dtype: int64\n",
      "ID         94\n",
      "SPAM       94\n",
      "SUBJECT    94\n",
      "CONTENT    94\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Remove missing values\n",
    "print df.count()\n",
    "df = df.dropna()\n",
    "print df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' thank you.'] [0]\n",
      "(75,) (75,)\n",
      "(19,) (19,)\n"
     ]
    }
   ],
   "source": [
    "data = df['CONTENT'].values\n",
    "labels = df['SPAM'].values\n",
    "print data[:1], labels[:1]\n",
    "# Split into Train and Test\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, train_size = 0.8)\n",
    "print train_data.shape, train_labels.shape\n",
    "print test_data.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training/feature vector (94, 5224)\n",
      "Size of the Vocabulary 5224\n",
      "Multinomial NB Training Accuracy: 0.989361702128\n",
      "Bernoulli NB Training Accuracy: 0.765957446809\n"
     ]
    }
   ],
   "source": [
    "# Extract features from Dataset\n",
    "cv = CountVectorizer(analyzer='word')\n",
    "train_counts = cv.fit_transform(data)\n",
    "print \"Shape of training/feature vector\", train_counts.shape\n",
    "print \"Size of the Vocabulary\", len(cv.vocabulary_)\n",
    "\n",
    "# Run Multinomial NB (sklearn)\n",
    "mNB = MultinomialNB()\n",
    "mNB.fit(train_counts, labels)\n",
    "print \"Multinomial NB Training Accuracy: {0}\".format(mNB.score(train_counts, labels))\n",
    "\n",
    "#Run Bernoulli MB (sklearn)\n",
    "bNB = BernoulliNB()\n",
    "bNB.fit(train_counts, labels)\n",
    "print \"Bernoulli NB Training Accuracy: {0}\".format(bNB.score(train_counts, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
