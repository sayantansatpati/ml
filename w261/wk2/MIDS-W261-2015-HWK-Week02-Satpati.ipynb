{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sayantan Satpati**\n",
    "* **sayantan.satpati@ischool.berkeley.edu**\n",
    "* **W261**\n",
    "* **Week-1**\n",
    "* **Assignment-2**\n",
    "* **Date of Submission: 07-SEP-2015**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#This notebook implements a Spam Filter backed by a Multinomial Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# Import a bunch of libraries.\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# Set the randomizer seed so results are the same each time.\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_HW12.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_HW12.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def strip_special_chars(word):\n",
    "    return re.sub('[^A-Za-z0-9]+', '', word)\n",
    "\n",
    "count = 0\n",
    "filename = sys.argv[1]\n",
    "wordList = sys.argv[2]\n",
    "wordList = wordList.split()\n",
    "wordCountDict = {}\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        # Split the line by <TAB> delimeter\n",
    "        email = re.split(r'\\t+', line)\n",
    "        \n",
    "        # Check whether Content is present\n",
    "        if len(email) < 4:\n",
    "            continue\n",
    "        \n",
    "        # Get the content as a list of words\n",
    "        content = email[len(email) - 1].split()\n",
    "        \n",
    "        if len(wordList) == 1 and wordList[0] == '*':\n",
    "            for w in content:\n",
    "                w = strip_special_chars(w)\n",
    "                if w not in wordCountDict:\n",
    "                    wordCountDict[w] = 1\n",
    "                else:\n",
    "                    wordCountDict[w] += 1\n",
    "        else:\n",
    "            for w in content:\n",
    "                w = strip_special_chars(w)\n",
    "                # Check if word is in word list passed to mapper\n",
    "                if w in wordList:\n",
    "                    if w not in wordCountDict:\n",
    "                        wordCountDict[w] = 1\n",
    "                    else:\n",
    "                        wordCountDict[w] += 1\n",
    "       \n",
    "# Print count from each mapper\n",
    "for k,v in wordCountDict.items():\n",
    "    print \"{0}\\t{1}\".format(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper_HW12.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_HW12.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_HW12.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "cnt = 0\n",
    "wordCountDict = {}\n",
    "for file in sys.argv:\n",
    "    if cnt == 0:\n",
    "        cnt += 1\n",
    "        continue\n",
    "        \n",
    "    with open (file, \"r\") as myfile:\n",
    "        for line in myfile:\n",
    "            wc = re.split(r'\\t+', line.strip())\n",
    "            if wc[0] not in wordCountDict:\n",
    "                wordCountDict[wc[0]] = int(wc[1])\n",
    "            else:\n",
    "                wordCountDict[wc[0]] += int(wc[1])\n",
    "                \n",
    "# Print count from each mapper\n",
    "for k,v in wordCountDict.items():\n",
    "    print \"{0}\\t{1}\".format(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer_HW12.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: License.txt.*: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# Remove split files from last runs\n",
    "! rm License.txt.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write control script 'pNaiveBayes.sh' to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## Mapper and Reducer Files are passed to make this script generic\n",
    "mapper=$3\n",
    "reducer=$4\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./${mapper} $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./${reducer} $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*\n",
    "\n",
    "## Display the Output\n",
    "cat $data.output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HW1.1. Read through the provided control script (pNaiveBayes.sh)\n",
    "'''\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Run the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage: usage: pGrepCount filename word chuncksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and\t631\r\n",
      "of\t546\r\n",
      "the\t1217\r\n"
     ]
    }
   ],
   "source": [
    "# Test the Program\n",
    "!./pNaiveBayes.sh 4 'the and of' 'mapper_HW12.py' 'reducer_HW12.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t9\r\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HW1.2. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "'''\n",
    "!./pNaiveBayes.sh 4 'assistance' 'mapper_HW12.py' 'reducer_HW12.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_HW15.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_HW15.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def strip_special_chars(word):\n",
    "    word = word.strip()\n",
    "    \n",
    "    if not word or word == '':\n",
    "        return None\n",
    "    \n",
    "    word = re.sub('[^A-Za-z0-9]+', '', word)\n",
    "    return word.lower()\n",
    "\n",
    "count = 0\n",
    "filename = sys.argv[1]\n",
    "wordList = sys.argv[2]\n",
    "wordList = wordList.split()\n",
    "\n",
    "# (Line#, Spam/Ham, Dict of Word|Count)\n",
    "mapper_output_list = []\n",
    "line_num = 0\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        # Split the line by <TAB> delimeter\n",
    "        email = re.split(r'\\t+', line)\n",
    "        \n",
    "        # Check whether Content is present\n",
    "        if len(email) < 4:\n",
    "            continue\n",
    "            \n",
    "        line_num += 1\n",
    "        \n",
    "        # Get the content as a list of words\n",
    "        content = email[len(email) - 1].split()\n",
    "        \n",
    "        wordCountDict = {}\n",
    "        for w in content:\n",
    "            w = strip_special_chars(w)\n",
    "            \n",
    "            if not w:\n",
    "                continue\n",
    "                \n",
    "            if w not in wordCountDict:\n",
    "                wordCountDict[w] = 1\n",
    "            else:\n",
    "                wordCountDict[w] += 1\n",
    "                \n",
    "        mapper_output_list.append((line_num, email[1], wordCountDict))\n",
    "       \n",
    "# Print output from each mapper\n",
    "for (line_num, spam, wordCountDict) in mapper_output_list:\n",
    "    for word,count in wordCountDict.items():\n",
    "        print \"{0}\\t{1}\\t{2}\\t{3}\".format(line_num, spam, word, count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper_HW15.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_HW15.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_HW15.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "\n",
    "# Totals\n",
    "vocab = 0\n",
    "vocab_spam = 0\n",
    "vocab_ham = 0\n",
    "\n",
    "vocab = {}\n",
    "word_counts = {\n",
    "    \"1\": {},\n",
    "    \"0\": {}\n",
    "}\n",
    "\n",
    "num_spam = 0\n",
    "num_ham = 0\n",
    "\n",
    "cnt = 0\n",
    "# Calculate the totals in Reducer First Pass\n",
    "for file in sys.argv:\n",
    "    if cnt == 0:\n",
    "        cnt += 1\n",
    "        continue\n",
    "        \n",
    "    with open (file, \"r\") as myfile:\n",
    "        last_line_num = 0\n",
    "        last_spam = -1\n",
    "        \n",
    "        for line in myfile:\n",
    "            tokens = re.split(r'\\t+', line.strip())\n",
    "            line_num = int(tokens[0])\n",
    "            spam = int(tokens[1])\n",
    "            word = tokens[2]\n",
    "            count = float(tokens[3])\n",
    "            \n",
    "            if last_line_num == 0:\n",
    "                last_line_num = line_num\n",
    "                last_spam = spam\n",
    "            \n",
    "            # Add Vocab per line\n",
    "            if word not in vocab:\n",
    "                vocab[word] = 0.0\n",
    "            if word not in word_counts[str(spam)]:\n",
    "                word_counts[str(spam)][word] = 0.0\n",
    "            vocab[word] += count\n",
    "            word_counts[str(spam)][word] += count\n",
    "                    \n",
    "            if last_line_num != line_num:\n",
    "                if last_spam == 1:\n",
    "                    num_spam += 1\n",
    "                else:\n",
    "                    num_ham += 1\n",
    "                \n",
    "            last_line_num = line_num\n",
    "            last_spam = spam\n",
    "            \n",
    "        # Last Line\n",
    "        if last_spam == 1:\n",
    "            num_spam += 1\n",
    "        else:\n",
    "            num_ham += 1\n",
    "                \n",
    "# At the end of first pass\n",
    "print 'Num Spam: {0}, Num Ham: {1}'.format(num_spam, num_ham)\n",
    "print '''Total Vocab: {0},\n",
    "       Total Unique Vocab: {1},\n",
    "       Total Spam Vocab: {2}, \n",
    "       Total Ham Vocab: {3}'''.format(sum(vocab.values()), \n",
    "                                    len(vocab),\n",
    "                                    sum(word_counts['1'].values()), \n",
    "                                    sum(word_counts['0'].values())\n",
    "                                   )\n",
    "                                    \n",
    "\n",
    "prior_spam = (num_spam * 1.0) / (num_spam + num_ham)\n",
    "prior_ham = (num_ham * 1.0) / (num_spam + num_ham)\n",
    "print '[Priors] Spam: {0}, Ham: {1}'.format(prior_spam, prior_ham)\n",
    "\n",
    "spam_likelihood_denom = sum(word_counts['1'].values()) + len(vocab)\n",
    "ham_likelihood_denom = sum(word_counts['0'].values()) + len(vocab)\n",
    "\n",
    "# Calculate the Conditionals/Likelihood in Next Pass\n",
    "reducer_output_list = []\n",
    "cnt = 0\n",
    "for file in sys.argv:\n",
    "    if cnt == 0:\n",
    "        cnt += 1\n",
    "        continue\n",
    "        \n",
    "    with open (file, \"r\") as myfile:\n",
    "        last_line_num = 1\n",
    "        log_prob_spam = 0\n",
    "        log_prob_ham = 0\n",
    "        \n",
    "        for line in myfile:\n",
    "            tokens = re.split(r'\\t+', line.strip())\n",
    "            line_num = int(tokens[0])\n",
    "            spam = int(tokens[1])\n",
    "            word = tokens[2]\n",
    "            count = int(tokens[3])\n",
    "            \n",
    "            if last_line_num != line_num:\n",
    "                # Calculate the Naive Bayes Scores for Document Classification\n",
    "                spam_score = log_prob_spam + math.log(prior_spam)\n",
    "                ham_score = log_prob_ham + math.log(prior_ham)\n",
    "                reducer_output_list.append((spam, spam_score, ham_score))\n",
    "                # Reset log prob\n",
    "                log_prob_spam = 0\n",
    "                log_prob_ham = 0\n",
    "            else:\n",
    "                # Calcuate the log likelihoods Using Laplace Smoothing\n",
    "                spam_likelihood = (word_counts['1'].get(word, 0.0) + 1) / spam_likelihood_denom\n",
    "                ham_likelihood = (word_counts['0'].get(word, 0.0) + 1) / ham_likelihood_denom\n",
    "                log_prob_spam += math.log( spam_likelihood )\n",
    "                log_prob_ham += math.log( ham_likelihood )\n",
    "            \n",
    "            last_line_num = line_num\n",
    "            \n",
    "        # Last Line\n",
    "        spam_score = log_prob_spam + math.log(prior_spam)\n",
    "        ham_score = log_prob_ham + math.log(prior_ham)\n",
    "        reducer_output_list.append((spam, spam_score, ham_score))\n",
    "        \n",
    "total = 0.0\n",
    "miscat = 0.0\n",
    "for (spam, spam_score, ham_score) in reducer_output_list:\n",
    "        total += 1.0\n",
    "        pred_class = 'SPAM'\n",
    "        if spam_score <= ham_score:\n",
    "            pred_class = 'HAM'\n",
    "        if (spam == 1 and pred_class == 'HAM') or (spam == 0 and pred_class == 'SPAM'):\n",
    "            miscat += 1.0\n",
    "            \n",
    "        print \"{0}\\t{1}\\t{2}\\t{3}\".format(spam, spam_score, ham_score, pred_class)\n",
    "\n",
    "error = miscat * 100 / total\n",
    "print \"Accuracy: {0}, Error Rate: {1}\".format((100 - error), error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer_HW15.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Spam: 43, Num Ham: 55\r\n",
      "Total Vocab: 30316.0,\r\n",
      "       Total Unique Vocab: 5601,\r\n",
      "       Total Spam Vocab: 17851.0, \r\n",
      "       Total Ham Vocab: 12465.0\r\n",
      "[Priors] Spam: 0.438775510204, Ham: 0.561224489796\r\n",
      "0\t-9.78786613239\t-9.68627411059\tHAM\r\n",
      "0\t-9.27704050863\t-8.58766182192\tHAM\r\n",
      "0\t-2191.11883116\t-2021.75440076\tHAM\r\n",
      "0\t-281.802505573\t-248.66078934\tHAM\r\n",
      "0\t-916.401633569\t-878.181677749\tHAM\r\n",
      "0\t-1653.44319311\t-1492.5384975\tHAM\r\n",
      "1\t-395.284181305\t-381.866477371\tHAM\r\n",
      "1\t-469.286123972\t-514.989803183\tSPAM\r\n",
      "1\t-929.72912698\t-1010.91965204\tSPAM\r\n",
      "0\t-580.613850923\t-614.383744167\tSPAM\r\n",
      "0\t-308.905756003\t-276.061748895\tHAM\r\n",
      "0\t-47.1860789348\t-40.2347293477\tHAM\r\n",
      "0\t-950.826286141\t-848.81258232\tHAM\r\n",
      "1\t-1072.44352797\t-960.329471011\tHAM\r\n",
      "1\t-606.705200099\t-633.092993607\tSPAM\r\n",
      "0\t-640.276393173\t-676.886377971\tSPAM\r\n",
      "0\t-899.069072412\t-813.462660776\tHAM\r\n",
      "0\t-639.436366799\t-553.707345035\tHAM\r\n",
      "1\t-486.370815481\t-462.444432652\tHAM\r\n",
      "1\t-611.888760379\t-658.61975736\tSPAM\r\n",
      "0\t-635.701101835\t-666.817949033\tSPAM\r\n",
      "0\t-696.395667351\t-629.032419986\tHAM\r\n",
      "0\t-681.79113014\t-598.800156466\tHAM\r\n",
      "0\t-369.205477768\t-351.507072033\tHAM\r\n",
      "0\t-718.331232072\t-652.19066713\tHAM\r\n",
      "1\t-155.963746645\t-164.834611541\tSPAM\r\n",
      "0\t-4309.44846148\t-4554.29592822\tSPAM\r\n",
      "0\t-449.539299634\t-428.577486601\tHAM\r\n",
      "0\t-4939.06987487\t-4573.81524366\tHAM\r\n",
      "1\t-244.865376047\t-235.614322214\tHAM\r\n",
      "1\t-311.595394813\t-325.941602481\tSPAM\r\n",
      "1\t-831.624836729\t-873.780864125\tSPAM\r\n",
      "0\t-879.235293936\t-925.718780195\tSPAM\r\n",
      "0\t-975.250606156\t-890.011137371\tHAM\r\n",
      "0\t-504.875426808\t-460.29909355\tHAM\r\n",
      "0\t-1898.89679539\t-1639.95383108\tHAM\r\n",
      "1\t-1343.30660425\t-1220.90014121\tHAM\r\n",
      "1\t-1015.05425275\t-1076.07499848\tSPAM\r\n",
      "0\t-770.586716813\t-854.775415682\tSPAM\r\n",
      "1\t-2819.20353407\t-2554.88871546\tHAM\r\n",
      "1\t-611.888760379\t-658.61975736\tSPAM\r\n",
      "1\t-2762.71629433\t-3018.46650805\tSPAM\r\n",
      "1\t-720.056436381\t-773.525254921\tSPAM\r\n",
      "0\t-3625.33852065\t-3812.92515098\tSPAM\r\n",
      "0\t-2212.31442671\t-1998.85951051\tHAM\r\n",
      "0\t-345.524100995\t-307.123252881\tHAM\r\n",
      "0\t-1926.38342139\t-1743.93931076\tHAM\r\n",
      "1\t-3495.9509362\t-3146.46329128\tHAM\r\n",
      "0\t-706.085158831\t-767.949995342\tSPAM\r\n",
      "0\t-1087.75215399\t-962.910070499\tHAM\r\n",
      "0\t-214.246496596\t-192.077752448\tHAM\r\n",
      "1\t-2231.79858251\t-1968.3991191\tHAM\r\n",
      "1\t-2137.46598027\t-2353.98492953\tSPAM\r\n",
      "1\t-0.823767362977\t-0.577634293438\tHAM\r\n",
      "0\t-1395.4197152\t-1503.42207516\tSPAM\r\n",
      "1\t-1024.52019829\t-952.91587697\tHAM\r\n",
      "1\t-2137.46598027\t-2353.98492953\tSPAM\r\n",
      "1\t-6305.60823327\t-6829.88011447\tSPAM\r\n",
      "1\t-331.23445788\t-356.986489157\tSPAM\r\n",
      "0\t-595.808380241\t-635.275255921\tSPAM\r\n",
      "0\t-2096.8884822\t-1909.42383997\tHAM\r\n",
      "0\t-583.481548778\t-504.150807478\tHAM\r\n",
      "0\t-1877.44961744\t-1620.72063087\tHAM\r\n",
      "0\t-683.338107553\t-667.292064988\tHAM\r\n",
      "1\t-446.24540735\t-414.241507702\tHAM\r\n",
      "0\t-137.14845291\t-148.730891811\tSPAM\r\n",
      "0\t-1266.01454826\t-1156.42629463\tHAM\r\n",
      "0\t-664.504816859\t-577.960068001\tHAM\r\n",
      "1\t-555.948879204\t-533.18533743\tHAM\r\n",
      "1\t-9632.30557826\t-10393.7781108\tSPAM\r\n",
      "0\t-1152.8268964\t-1213.93911154\tSPAM\r\n",
      "0\t-898.357407207\t-816.54883022\tHAM\r\n",
      "0\t-885.61852061\t-784.575568784\tHAM\r\n",
      "1\t-1005.27950676\t-907.813522375\tHAM\r\n",
      "1\t-1942.1471285\t-2080.21140107\tSPAM\r\n",
      "1\t-51.9329350057\t-56.2973138263\tSPAM\r\n",
      "0\t-720.900947\t-755.930815496\tSPAM\r\n",
      "0\t-520.389190282\t-477.143503463\tHAM\r\n",
      "0\t-575.927365971\t-496.00906614\tHAM\r\n",
      "0\t-105.947550832\t-100.847088011\tHAM\r\n",
      "1\t-2678.71129692\t-2458.50017481\tHAM\r\n",
      "1\t-693.831992184\t-750.107992654\tSPAM\r\n",
      "0\t-827.915052319\t-893.677368771\tSPAM\r\n",
      "0\t-590.183866817\t-555.989365161\tHAM\r\n",
      "1\t-856.796470852\t-760.585553615\tHAM\r\n",
      "1\t-693.831992184\t-750.107992654\tSPAM\r\n",
      "1\t-5927.70644169\t-6536.54977058\tSPAM\r\n",
      "1\t-687.86927922\t-746.725933597\tSPAM\r\n",
      "0\t-470.534905235\t-512.398365589\tSPAM\r\n",
      "0\t-333.222151051\t-322.840040697\tHAM\r\n",
      "0\t-1887.51232849\t-1630.92788297\tHAM\r\n",
      "1\t-344.169934943\t-333.542766902\tHAM\r\n",
      "1\t-154.298635668\t-164.448798508\tSPAM\r\n",
      "1\t-831.998475902\t-873.032087758\tSPAM\r\n",
      "0\t-1760.71166611\t-1849.55860198\tSPAM\r\n",
      "1\t-712.460647129\t-698.127308355\tHAM\r\n",
      "1\t-1781.02548675\t-1910.68924069\tSPAM\r\n",
      "1\t-2039.15695348\t-2150.88056932\tSPAM\r\n",
      "Accuracy: 66.3265306122, Error Rate: 33.6734693878\r\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HW1.5. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by all words present.\n",
    "'''\n",
    "!./pNaiveBayes.sh 4 '*' 'mapper_HW15.py' 'reducer_HW15.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SPAM</th>\n",
       "      <th>SUBJECT</th>\n",
       "      <th>CONTENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001.1999-12-10.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>christmas tree farm pictures</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001.1999-12-10.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>re: rankings</td>\n",
       "      <td>thank you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001.2000-01-17.beck</td>\n",
       "      <td>0</td>\n",
       "      <td>leadership development pilot</td>\n",
       "      <td>sally:  what timing, ask and you shall receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001.2000-06-06.lokay</td>\n",
       "      <td>0</td>\n",
       "      <td>key dates and impact of upcoming sap implemen...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001.2001-02-07.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>key hr issues going forward</td>\n",
       "      <td>a) year end reviews-report needs generating l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ID  SPAM  \\\n",
       "0    0001.1999-12-10.farmer     0   \n",
       "1  0001.1999-12-10.kaminski     0   \n",
       "2      0001.2000-01-17.beck     0   \n",
       "3     0001.2000-06-06.lokay     0   \n",
       "4   0001.2001-02-07.kitchen     0   \n",
       "\n",
       "                                             SUBJECT  \\\n",
       "0                       christmas tree farm pictures   \n",
       "1                                       re: rankings   \n",
       "2                       leadership development pilot   \n",
       "3   key dates and impact of upcoming sap implemen...   \n",
       "4                        key hr issues going forward   \n",
       "\n",
       "                                             CONTENT  \n",
       "0                                                NaN  \n",
       "1                                         thank you.  \n",
       "2   sally:  what timing, ask and you shall receiv...  \n",
       "3                                                NaN  \n",
       "4   a) year end reviews-report needs generating l...  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data into Pandas Dataframe\n",
    "df = pd.read_csv('enronemail_1h.txt', sep='\\t', header=None)\n",
    "df.columns = ['ID', 'SPAM', 'SUBJECT', 'CONTENT']\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID         100\n",
      "SPAM       100\n",
      "SUBJECT     98\n",
      "CONTENT     96\n",
      "dtype: int64\n",
      "ID         94\n",
      "SPAM       94\n",
      "SUBJECT    94\n",
      "CONTENT    94\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Remove missing values\n",
    "print df.count()\n",
    "df = df.dropna()\n",
    "print df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' thank you.'] [0]\n",
      "(75,) (75,)\n",
      "(19,) (19,)\n"
     ]
    }
   ],
   "source": [
    "data = df['CONTENT'].values\n",
    "labels = df['SPAM'].values\n",
    "print data[:1], labels[:1]\n",
    "# Split into Train and Test\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, train_size = 0.8)\n",
    "print train_data.shape, train_labels.shape\n",
    "print test_data.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training/feature vector (94, 5224)\n",
      "Size of the Vocabulary 5224\n",
      "Multinomial NB Training Accuracy: 0.989361702128\n",
      "Bernoulli NB Training Accuracy: 0.765957446809\n"
     ]
    }
   ],
   "source": [
    "# Extract features from Dataset\n",
    "cv = CountVectorizer(analyzer='word')\n",
    "train_counts = cv.fit_transform(data)\n",
    "print \"Shape of training/feature vector\", train_counts.shape\n",
    "print \"Size of the Vocabulary\", len(cv.vocabulary_)\n",
    "\n",
    "# Run Multinomial NB (sklearn)\n",
    "mNB = MultinomialNB()\n",
    "mNB.fit(train_counts, labels)\n",
    "print \"Multinomial NB Training Accuracy: {0}\".format(mNB.score(train_counts, labels))\n",
    "\n",
    "#Run Bernoulli MB (sklearn)\n",
    "bNB = BernoulliNB()\n",
    "bNB.fit(train_counts, labels)\n",
    "print \"Bernoulli NB Training Accuracy: {0}\".format(bNB.score(train_counts, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
