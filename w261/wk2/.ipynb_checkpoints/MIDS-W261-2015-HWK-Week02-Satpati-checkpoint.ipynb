{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sayantan Satpati**\n",
    "* **sayantan.satpati@ischool.berkeley.edu**\n",
    "* **W261**\n",
    "* **Week-1**\n",
    "* **Assignment-2**\n",
    "* **Date of Submission: 07-SEP-2015**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#This notebook implements a Spam Filter backed by a Multinomial Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# Import a bunch of libraries.\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "# Set the randomizer seed so results are the same each time.\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_HW12.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_HW12.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def strip_special_chars(word):\n",
    "    return re.sub('[^A-Za-z0-9]+', '', word)\n",
    "\n",
    "count = 0\n",
    "filename = sys.argv[1]\n",
    "wordList = sys.argv[2]\n",
    "wordList = wordList.split()\n",
    "wordCountDict = {}\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        # Split the line by <TAB> delimeter\n",
    "        email = re.split(r'\\t+', line)\n",
    "        \n",
    "        # Check whether Content is present\n",
    "        if len(email) < 4:\n",
    "            continue\n",
    "        \n",
    "        # Get the content as a list of words\n",
    "        content = email[len(email) - 1].split()\n",
    "        \n",
    "        if len(wordList) == 1 and wordList[0] == '*':\n",
    "            for w in content:\n",
    "                w = strip_special_chars(w)\n",
    "                if w not in wordCountDict:\n",
    "                    wordCountDict[w] = 1\n",
    "                else:\n",
    "                    wordCountDict[w] += 1\n",
    "        else:\n",
    "            for w in content:\n",
    "                w = strip_special_chars(w)\n",
    "                # Check if word is in word list passed to mapper\n",
    "                if w in wordList:\n",
    "                    if w not in wordCountDict:\n",
    "                        wordCountDict[w] = 1\n",
    "                    else:\n",
    "                        wordCountDict[w] += 1\n",
    "       \n",
    "# Print count from each mapper\n",
    "for k,v in wordCountDict.items():\n",
    "    print \"{0}\\t{1}\".format(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper_HW12.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_HW12.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_HW12.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "cnt = 0\n",
    "wordCountDict = {}\n",
    "for file in sys.argv:\n",
    "    if cnt == 0:\n",
    "        cnt += 1\n",
    "        continue\n",
    "        \n",
    "    with open (file, \"r\") as myfile:\n",
    "        for line in myfile:\n",
    "            wc = re.split(r'\\t+', line.strip())\n",
    "            if wc[0] not in wordCountDict:\n",
    "                wordCountDict[wc[0]] = int(wc[1])\n",
    "            else:\n",
    "                wordCountDict[wc[0]] += int(wc[1])\n",
    "                \n",
    "# Print count from each mapper\n",
    "for k,v in wordCountDict.items():\n",
    "    print \"{0}\\t{1}\".format(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer_HW12.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: License.txt.*: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# Remove split files from last runs\n",
    "! rm License.txt.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write control script 'pNaiveBayes.sh' to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## Mapper and Reducer Files are passed to make this script generic\n",
    "mapper=$3\n",
    "reducer=$4\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./${mapper} $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./${reducer} $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*\n",
    "\n",
    "## Display the Output\n",
    "cat $data.output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HW1.1. Read through the provided control script (pNaiveBayes.sh)\n",
    "'''\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Run the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage: usage: pGrepCount filename word chuncksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and\t631\r\n",
      "of\t546\r\n",
      "the\t1217\r\n"
     ]
    }
   ],
   "source": [
    "# Test the Program\n",
    "!./pNaiveBayes.sh 4 'the and of' 'mapper_HW12.py' 'reducer_HW12.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t9\r\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HW1.2. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "'''\n",
    "!./pNaiveBayes.sh 4 'assistance' 'mapper_HW12.py' 'reducer_HW12.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_HW15.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_HW15.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def strip_special_chars(word):\n",
    "    word = word.strip()\n",
    "    \n",
    "    if not word or word == '':\n",
    "        return None\n",
    "    \n",
    "    word = re.sub('[^A-Za-z0-9]+', '', word)\n",
    "    return word.lower()\n",
    "\n",
    "count = 0\n",
    "filename = sys.argv[1]\n",
    "wordList = sys.argv[2]\n",
    "wordList = wordList.split()\n",
    "\n",
    "# (Line#, Spam/Ham, Dict of Word|Count)\n",
    "mapper_output_list = []\n",
    "line_num = 0\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        # Split the line by <TAB> delimeter\n",
    "        email = re.split(r'\\t+', line)\n",
    "        \n",
    "        # Check whether Content is present\n",
    "        if len(email) < 4:\n",
    "            continue\n",
    "            \n",
    "        line_num += 1\n",
    "        \n",
    "        # Get the content as a list of words\n",
    "        content = email[len(email) - 1].split()\n",
    "        \n",
    "        wordCountDict = {}\n",
    "        for w in content:\n",
    "            w = strip_special_chars(w)\n",
    "            \n",
    "            if not w:\n",
    "                continue\n",
    "                \n",
    "            if w not in wordCountDict:\n",
    "                wordCountDict[w] = 1\n",
    "            else:\n",
    "                wordCountDict[w] += 1\n",
    "                \n",
    "        mapper_output_list.append((line_num, email[1], wordCountDict))\n",
    "       \n",
    "# Print output from each mapper\n",
    "for (line_num, spam, wordCountDict) in mapper_output_list:\n",
    "    for word,count in wordCountDict.items():\n",
    "        print \"{0}\\t{1}\\t{2}\\t{3}\".format(line_num, spam, word, count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper_HW15.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_HW15.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_HW15.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "\n",
    "# Totals\n",
    "vocab = 0\n",
    "vocab_spam = 0\n",
    "vocab_ham = 0\n",
    "\n",
    "vocab = {}\n",
    "word_counts = {\n",
    "    \"1\": {},\n",
    "    \"0\": {}\n",
    "}\n",
    "\n",
    "num_spam = 0\n",
    "num_ham = 0\n",
    "\n",
    "cnt = 0\n",
    "# Calculate the totals in Reducer First Pass\n",
    "for file in sys.argv:\n",
    "    if cnt == 0:\n",
    "        cnt += 1\n",
    "        continue\n",
    "        \n",
    "    with open (file, \"r\") as myfile:\n",
    "        last_line_num = 0\n",
    "        last_spam = -1\n",
    "        \n",
    "        for line in myfile:\n",
    "            tokens = re.split(r'\\t+', line.strip())\n",
    "            line_num = int(tokens[0])\n",
    "            spam = int(tokens[1])\n",
    "            word = tokens[2]\n",
    "            count = float(tokens[3])\n",
    "            \n",
    "            if last_line_num == 0:\n",
    "                last_line_num = line_num\n",
    "                last_spam = spam\n",
    "            \n",
    "            # Add Vocab per line\n",
    "            if word not in vocab:\n",
    "                vocab[word] = 0.0\n",
    "            if word not in word_counts[str(spam)]:\n",
    "                word_counts[str(spam)][word] = 0.0\n",
    "            vocab[word] += count\n",
    "            word_counts[str(spam)][word] += count\n",
    "                    \n",
    "            if last_line_num != line_num:\n",
    "                if last_spam == 1:\n",
    "                    num_spam += 1\n",
    "                else:\n",
    "                    num_ham += 1\n",
    "                \n",
    "            last_line_num = line_num\n",
    "            last_spam = spam\n",
    "            \n",
    "        # Last Line\n",
    "        if last_spam == 1:\n",
    "            num_spam += 1\n",
    "        else:\n",
    "            num_ham += 1\n",
    "                \n",
    "# At the end of first pass\n",
    "print 'Num Spam: {0}, Num Ham: {1}'.format(num_spam, num_ham)\n",
    "print '''Total Vocab: {0},\n",
    "       Total Unique Vocab: {1},\n",
    "       Total Spam Vocab: {2}, \n",
    "       Total Ham Vocab: {3}'''.format(sum(vocab.values()), \n",
    "                                    len(vocab),\n",
    "                                    sum(word_counts['1'].values()), \n",
    "                                    sum(word_counts['0'].values())\n",
    "                                   )\n",
    "                                    \n",
    "\n",
    "prior_spam = (num_spam * 1.0) / (num_spam + num_ham)\n",
    "prior_ham = (num_ham * 1.0) / (num_spam + num_ham)\n",
    "print '[Priors] Spam: {0}, Ham: {1}'.format(prior_spam, prior_ham)\n",
    "\n",
    "spam_likelihood_denom = sum(word_counts['1'].values()) + len(vocab)\n",
    "ham_likelihood_denom = sum(word_counts['0'].values()) + len(vocab)\n",
    "\n",
    "# Calculate the Conditionals/Likelihood in Next Pass\n",
    "reducer_output_list = []\n",
    "cnt = 0\n",
    "for file in sys.argv:\n",
    "    if cnt == 0:\n",
    "        cnt += 1\n",
    "        continue\n",
    "        \n",
    "    with open (file, \"r\") as myfile:\n",
    "        last_line_num = 1\n",
    "        log_prob_spam = 0\n",
    "        log_prob_ham = 0\n",
    "        \n",
    "        for line in myfile:\n",
    "            tokens = re.split(r'\\t+', line.strip())\n",
    "            line_num = int(tokens[0])\n",
    "            spam = int(tokens[1])\n",
    "            word = tokens[2]\n",
    "            count = int(tokens[3])\n",
    "            \n",
    "            if last_line_num != line_num:\n",
    "                # Calculate the Naive Bayes Scores for Document Classification\n",
    "                spam_score = log_prob_spam + math.log(prior_spam)\n",
    "                ham_score = log_prob_ham + math.log(prior_ham)\n",
    "                reducer_output_list.append((spam, spam_score, ham_score))\n",
    "                # Reset log prob\n",
    "                log_prob_spam = 0\n",
    "                log_prob_ham = 0\n",
    "            else:\n",
    "                # Calcuate the log likelihoods Using Laplace Smoothing\n",
    "                spam_likelihood = (word_counts['1'].get(word, 0.0) + 1) / spam_likelihood_denom\n",
    "                ham_likelihood = (word_counts['0'].get(word, 0.0) + 1) / ham_likelihood_denom\n",
    "                log_prob_spam += math.log( spam_likelihood )\n",
    "                log_prob_ham += math.log( ham_likelihood )\n",
    "            \n",
    "            last_line_num = line_num\n",
    "            \n",
    "        # Last Line\n",
    "        spam_score = log_prob_spam + math.log(prior_spam)\n",
    "        ham_score = log_prob_ham + math.log(prior_ham)\n",
    "        reducer_output_list.append((spam, spam_score, ham_score))\n",
    "        \n",
    "total = 0.0\n",
    "miscat = 0.0\n",
    "for (spam, spam_score, ham_score) in reducer_output_list:\n",
    "        total += 1.0\n",
    "        pred_class = 'SPAM'\n",
    "        if spam_score <= ham_score:\n",
    "            pred_class = 'HAM'\n",
    "        if (spam == 1 and pred_class == 'HAM') or (spam == 0 and pred_class == 'SPAM'):\n",
    "            miscat += 1.0\n",
    "            \n",
    "        print \"{0}\\t{1}\\t{2}\\t{3}\".format(spam, spam_score, ham_score, pred_class)\n",
    "\n",
    "error = miscat * 100 / total\n",
    "print \"Accuracy: {0}, Error Rate: {1}\".format((100 - error), error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer_HW15.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "HW1.5. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by all words present.\n",
    "'''\n",
    "!./pNaiveBayes.sh 4 '*' 'mapper_HW15.py' 'reducer_HW15.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SPAM</th>\n",
       "      <th>SUBJECT</th>\n",
       "      <th>CONTENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001.1999-12-10.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>christmas tree farm pictures</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001.1999-12-10.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>re: rankings</td>\n",
       "      <td>thank you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001.2000-01-17.beck</td>\n",
       "      <td>0</td>\n",
       "      <td>leadership development pilot</td>\n",
       "      <td>sally:  what timing, ask and you shall receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001.2000-06-06.lokay</td>\n",
       "      <td>0</td>\n",
       "      <td>key dates and impact of upcoming sap implemen...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001.2001-02-07.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>key hr issues going forward</td>\n",
       "      <td>a) year end reviews-report needs generating l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ID  SPAM  \\\n",
       "0    0001.1999-12-10.farmer     0   \n",
       "1  0001.1999-12-10.kaminski     0   \n",
       "2      0001.2000-01-17.beck     0   \n",
       "3     0001.2000-06-06.lokay     0   \n",
       "4   0001.2001-02-07.kitchen     0   \n",
       "\n",
       "                                             SUBJECT  \\\n",
       "0                       christmas tree farm pictures   \n",
       "1                                       re: rankings   \n",
       "2                       leadership development pilot   \n",
       "3   key dates and impact of upcoming sap implemen...   \n",
       "4                        key hr issues going forward   \n",
       "\n",
       "                                             CONTENT  \n",
       "0                                                NaN  \n",
       "1                                         thank you.  \n",
       "2   sally:  what timing, ask and you shall receiv...  \n",
       "3                                                NaN  \n",
       "4   a) year end reviews-report needs generating l...  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data into Pandas Dataframe\n",
    "df = pd.read_csv('enronemail_1h.txt', sep='\\t', header=None)\n",
    "df.columns = ['ID', 'SPAM', 'SUBJECT', 'CONTENT']\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID         100\n",
      "SPAM       100\n",
      "SUBJECT     98\n",
      "CONTENT     96\n",
      "dtype: int64\n",
      "ID         94\n",
      "SPAM       94\n",
      "SUBJECT    94\n",
      "CONTENT    94\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Remove missing values\n",
    "print df.count()\n",
    "df = df.dropna()\n",
    "print df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' thank you.'] [0]\n",
      "(75,) (75,)\n",
      "(19,) (19,)\n"
     ]
    }
   ],
   "source": [
    "data = df['CONTENT'].values\n",
    "labels = df['SPAM'].values\n",
    "print data[:1], labels[:1]\n",
    "# Split into Train and Test\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, train_size = 0.8)\n",
    "print train_data.shape, train_labels.shape\n",
    "print test_data.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training/feature vector (94, 5224)\n",
      "Size of the Vocabulary 5224\n",
      "Multinomial NB Training Accuracy: 0.989361702128\n",
      "Bernoulli NB Training Accuracy: 0.765957446809\n"
     ]
    }
   ],
   "source": [
    "# Extract features from Dataset\n",
    "cv = CountVectorizer(analyzer='word')\n",
    "train_counts = cv.fit_transform(data)\n",
    "print \"Shape of training/feature vector\", train_counts.shape\n",
    "print \"Size of the Vocabulary\", len(cv.vocabulary_)\n",
    "\n",
    "# Run Multinomial NB (sklearn)\n",
    "mNB = MultinomialNB()\n",
    "mNB.fit(train_counts, labels)\n",
    "print \"Multinomial NB Training Accuracy: {0}\".format(mNB.score(train_counts, labels))\n",
    "\n",
    "#Run Bernoulli MB (sklearn)\n",
    "bNB = BernoulliNB()\n",
    "bNB.fit(train_counts, labels)\n",
    "print \"Bernoulli NB Training Accuracy: {0}\".format(bNB.score(train_counts, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
