{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sayantan Satpati**\n",
    "* **sayantan.satpati@ischool.berkeley.edu**\n",
    "* **W261**\n",
    "* **Week-5**\n",
    "* **Assignment-5**\n",
    "* **Date of Submission: 07-OCT-2015**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  === Week 5: mrjob, aws, and n-grams ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.0\n",
    "---\n",
    "\n",
    "***What is a data warehouse? What is a Star schema? When is it used?***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.1\n",
    "---\n",
    "\n",
    "***In the database world What is 3NF? Does machine learning use data in 3NF? If so why? ***\n",
    "\n",
    "***In what form does ML consume data?***\n",
    "\n",
    "***Why would one use log files that are denormalized?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.2\n",
    "---\n",
    "\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, \n",
    "right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)\n",
    ":\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "```\n",
    "(1) Left joining Table Left with Table Right\n",
    "(2) Right joining Table Left with Table Right\n",
    "(3) Inner joining Table Left with Table Right\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.3\n",
    "---\n",
    "\n",
    "For the remainder of this assignment you will work with a large subset \n",
    "of the Google n-grams dataset,\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket on s3:\n",
    "\n",
    "s3://filtered-5grams/\n",
    "\n",
    "In particular, this bucket contains (~200) files in the format:\n",
    "\n",
    "(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (count), i.e., unigrams\n",
    "- Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency (Hint: save to PART-000* and take the head -n 1000)\n",
    "- Distribution of 5-gram sizes (counts) sorted in decreasing order of relative frequency. (Hint: save to PART-000* and take the head -n 1000)\n",
    "OPTIONAL Question:\n",
    "- Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see:\n",
    "https://en.wikipedia.org/wiki/Log%E2%80%93log_plot\n",
    "https://en.wikipedia.org/wiki/Power_law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longest 5-gram (number of characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_hw53_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_hw53_1.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "\n",
    "class LongestNgram(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_ngrams_len,\n",
    "                   reducer=self.reducer_ngrams_len),\n",
    "            MRStep(reducer=self.reducer_find_max_ngram)\n",
    "        ]\n",
    "\n",
    "    def mapper_ngrams_len(self, _, line):\n",
    "        tokens = line.strip().split('\\t')\n",
    "        yield (tokens[0], len(tokens[0]))\n",
    "\n",
    "  \n",
    "    def reducer_ngrams_len(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    # discard the key; it is just None\n",
    "    def reducer_find_max_ngram(self, _, word_count_pairs):\n",
    "        # each item of word_count_pairs is (count, word),\n",
    "        # so yielding one results in key=counts, value=word\n",
    "        yield max(word_count_pairs)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    LongestNgram.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x mrjob_hw53_1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "(159, 'ROPLEZIMPREDASTRODONBRASLPKLSON YHROACLMPARCHEYXMMIOUDAVESAURUS PIOFPILOCOWERSURUASOGETSESNEGCP TYRAVOPSIFENGOQUAPIALLOBOSKENUO OWINFUYAIOKENECKSASXHYILPOYNUAT')\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from mrjob_hw53_1 import LongestNgram\n",
    "mr_job = LongestNgram(args=['s3://filtered-5grams',\n",
    "                            '-r', 'emr', '--no-strict-protocol'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_hw53_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_hw53_2.py\n",
    "#!/usr/bin/python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "from itertools import combinations\n",
    "\n",
    "class FrequentUnigrams(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer),\n",
    "             MRStep(mapper=self.mapper_frequent_unigrams,\n",
    "                   reducer=self.reducer_frequent_unigrams,\n",
    "                   jobconf={\n",
    "                            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'mapred.text.key.comparator.options': '-k1,1rn',\n",
    "                            }\n",
    "                   )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line.strip()\n",
    "        tokens = re.split(\"\\t\",line)\n",
    "        unigrams = tokens[0].split()\n",
    "        count = int(tokens[1])\n",
    "        for unigram in unigrams:\n",
    "            yield unigram, count\n",
    "    \n",
    "    def combiner(self, unigram, counts):\n",
    "        yield unigram, sum(counts)\n",
    "    \n",
    "    def reducer(self, unigram, counts):\n",
    "        yield unigram, sum(counts)\n",
    "        \n",
    "    def mapper_frequent_unigrams(self, unigram, count):\n",
    "        yield count, unigram\n",
    "        \n",
    "    def reducer_frequent_unigrams(self, count, unigrams):\n",
    "        for unigram in unigrams:\n",
    "            yield count, unigram\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    FrequentUnigrams.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mrjob_hw53_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /Users/ssatpati/.mrjob.conf\n",
      "using existing scratch bucket mrjob-d5ed1dc31babbe2c\n",
      "using s3://mrjob-d5ed1dc31babbe2c/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/mrjob_hw53_2.ssatpati.20151003.062810.595234\n",
      "writing master bootstrap script to /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/mrjob_hw53_2.ssatpati.20151003.062810.595234/b.py\n",
      "Copying non-input files into s3://mrjob-d5ed1dc31babbe2c/tmp/mrjob_hw53_2.ssatpati.20151003.062810.595234/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-QGLC3PJPOLBE\n",
      "Created new job flow j-QGLC3PJPOLBE\n",
      "Job launched 30.4s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 61.3s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 91.8s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 122.8s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 153.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 184.0s ago, status STARTING: Configuring cluster software\n",
      "Job launched 214.4s ago, status STARTING: Configuring cluster software\n",
      "Job launched 245.4s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 276.0s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 306.6s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 337.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 368.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 398.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 429.5s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 460.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 491.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 521.5s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 552.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 582.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 613.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 643.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 674.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 705.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 735.9s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 766.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 796.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 827.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 858.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 888.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 919.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 949.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 980.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1010.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1041.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1072.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1102.6s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1133.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1163.5s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1193.9s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1224.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1255.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1285.5s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1316.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1346.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1377.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1408.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1438.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1469.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1500.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1530.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1561.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1591.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1622.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1652.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1683.6s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1714.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1745.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1775.5s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1806.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1836.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1867.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1898.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1928.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1959.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1989.9s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2020.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2051.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2081.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2112.6s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2143.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2173.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2204.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2234.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2265.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2296.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2326.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2357.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2387.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2418.6s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2449.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2479.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2510.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2541.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2571.5s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2602.9s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2633.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2663.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2694.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2725.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2755.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2785.9s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2816.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2847.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2877.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2908.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2939.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2970.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3000.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3031.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3061.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3092.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3123.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3153.6s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3184.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3214.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3245.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3275.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3306.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 2 of 2)\n",
      "Job launched 3336.9s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 2 of 2)\n",
      "Job launched 3367.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 2 of 2)\n",
      "Job launched 3398.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 2 of 2)\n",
      "Job completed.\n",
      "Running time was 3103.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 2156069116\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 5251252\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 292640853\n",
      "    FILE_BYTES_WRITTEN: 323462370\n",
      "    HDFS_BYTES_READ: 23640\n",
      "    HDFS_BYTES_WRITTEN: 5251252\n",
      "    S3_BYTES_READ: 2156069116\n",
      "  Job Counters :\n",
      "    Launched map tasks: 193\n",
      "    Launched reduce tasks: 8\n",
      "    Rack-local map tasks: 191\n",
      "    SLOTS_MILLIS_MAPS: 22513524\n",
      "    SLOTS_MILLIS_REDUCES: 10666442\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 7828220\n",
      "    Combine input records: 307583853\n",
      "    Combine output records: 22094566\n",
      "    Map input bytes: 2156069116\n",
      "    Map input records: 58682266\n",
      "    Map output bytes: 3136729760\n",
      "    Map output materialized bytes: 98214630\n",
      "    Map output records: 293411330\n",
      "    Physical memory (bytes) snapshot: 79829819392\n",
      "    Reduce input groups: 343019\n",
      "    Reduce input records: 7922043\n",
      "    Reduce output records: 343019\n",
      "    Reduce shuffle bytes: 98214630\n",
      "    SPLIT_RAW_BYTES: 23640\n",
      "    Spilled Records: 30016609\n",
      "    Total committed heap usage (bytes): 86414835712\n",
      "    Virtual memory (bytes) snapshot: 188305924096\n",
      "Counters from step 2:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 6161441\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 5251252\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 4010303\n",
      "    FILE_BYTES_WRITTEN: 9264521\n",
      "    HDFS_BYTES_READ: 6164738\n",
      "    S3_BYTES_WRITTEN: 5251252\n",
      "  Job Counters :\n",
      "    Data-local map tasks: 21\n",
      "    Launched map tasks: 21\n",
      "    Launched reduce tasks: 8\n",
      "    SLOTS_MILLIS_MAPS: 239490\n",
      "    SLOTS_MILLIS_REDUCES: 174447\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 46410\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 5251252\n",
      "    Map input records: 343019\n",
      "    Map output bytes: 5251252\n",
      "    Map output materialized bytes: 4500335\n",
      "    Map output records: 343019\n",
      "    Physical memory (bytes) snapshot: 7540547584\n",
      "    Reduce input groups: 41362\n",
      "    Reduce input records: 343019\n",
      "    Reduce output records: 343019\n",
      "    Reduce shuffle bytes: 4500335\n",
      "    SPLIT_RAW_BYTES: 3297\n",
      "    Spilled Records: 686038\n",
      "    Total committed heap usage (bytes): 7062859776\n",
      "    Virtual memory (bytes) snapshot: 26687610880\n",
      "removing tmp directory /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/mrjob_hw53_2.ssatpati.20151003.062810.595234\n",
      "Removing all files in s3://mrjob-d5ed1dc31babbe2c/tmp/mrjob_hw53_2.ssatpati.20151003.062810.595234/\n",
      "Removing all files in s3://mrjob-d5ed1dc31babbe2c/tmp/logs/j-QGLC3PJPOLBE/\n",
      "Terminating job flow: j-QGLC3PJPOLBE\n"
     ]
    }
   ],
   "source": [
    "!python mrjob_hw53_2.py -r emr \\\n",
    " s3://filtered-5grams \\\n",
    " --output-dir=s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words \\\n",
    " --no-output \\\n",
    " --no-strict-protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/_SUCCESS to ./_SUCCESS\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00003 to ./part-00003\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00000 to ./part-00000\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00002 to ./part-00002\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00004 to ./part-00004\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00006 to ./part-00006\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00001 to ./part-00001\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00005 to ./part-00005\n",
      "5375699242\t\"the\"\n",
      "3691308874\t\"of\"\n",
      "2221164346\t\"to\"\n",
      "1387638591\t\"in\"\n",
      "1342195425\t\"a\"\n",
      "1135779433\t\"and\"\n",
      "798553959\t\"that\"\n",
      "756296656\t\"is\"\n",
      "688053106\t\"be\"\n",
      "481373389\t\"as\"\n",
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output;mkdir output;cd output;aws s3 cp --recursive s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words .;\n",
    "!cat output/part-0000* | sort -nrk 1 | head -n 10\n",
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Frequent Words are:\n",
    "\n",
    "```\n",
    "5375699242\t\"the\"\n",
    "3691308874\t\"of\"\n",
    "2221164346\t\"to\"\n",
    "1387638591\t\"in\"\n",
    "1342195425\t\"a\"\n",
    "1135779433\t\"and\"\n",
    "798553959\t\"that\"\n",
    "756296656\t\"is\"\n",
    "688053106\t\"be\"\n",
    "481373389\t\"as\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_hw53_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_hw53_3.py\n",
    "#!/usr/bin/python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "from itertools import combinations\n",
    "\n",
    "class DenseWords(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer,\n",
    "                   ),\n",
    "            MRStep(mapper=self.mapper_max_min,\n",
    "                   reducer=self.reducer_max_min,\n",
    "                   jobconf={\n",
    "                            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'mapred.text.key.comparator.options': '-k1,1rn',\n",
    "                            }\n",
    "                   )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        tokens = line.strip().split('\\t')\n",
    "        unigrams = tokens[0].split()\n",
    "        density = round((int(tokens[1]) * 1.0 / int(tokens[2])), 3)\n",
    "        for unigram in unigrams:\n",
    "            yield unigram, density\n",
    "            \n",
    "    def combiner(self, unigram, densities):\n",
    "        densities = [d for d in densities]\n",
    "        yield unigram, min(densities) \n",
    "        yield unigram, max(densities)\n",
    "        \n",
    "    def reducer(self, unigram, densities):\n",
    "        densities = [d for d in densities]\n",
    "        yield unigram, min(densities)\n",
    "        yield unigram, max(densities)\n",
    "        \n",
    "    def mapper_max_min(self, unigram, density):\n",
    "        yield density, unigram\n",
    "        \n",
    "    def reducer_max_min(self, density, unigrams):\n",
    "        for unigram in unigrams:\n",
    "            yield density, unigram\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    DenseWords.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mrjob_hw53_3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/density/_SUCCESS\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00001\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00000\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00002\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00004\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00003\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00005\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00006\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rm --recursive s3://ucb-mids-mls-sayantan-satpati/hw53/density/\n",
    "!python mrjob_hw53_3.py -q -r emr \\\n",
    " s3://filtered-5grams \\\n",
    " --output-dir=s3://ucb-mids-mls-sayantan-satpati/hw53/density \\\n",
    " --no-output \\\n",
    " --no-strict-protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/_SUCCESS to ./_SUCCESS\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00003 to ./part-00003\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00004 to ./part-00004\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00005 to ./part-00005\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00006 to ./part-00006\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00000 to ./part-00000\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00001 to ./part-00001\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00007 to ./part-00007\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00002 to ./part-00002\n",
      "29.704999999999998\t\"Death\"\n",
      "14.912000000000001\t\"write\"\n",
      "14.912000000000001\t\"and\"\n",
      "12.380000000000001\t\"NA\"\n",
      "11.557\t\"xxxx\"\n",
      "11.557\t\"xxxx\"\n",
      "10.882\t\"Sc\"\n",
      "10.6\t\"beep\"\n",
      "10.022\t\"blah\"\n",
      "9.8040000000000003\t\"whole\"\n",
      "1.0\t\"AAAS\"\n",
      "1.0\t\"AAAI\"\n",
      "1.0\t\"AAAE\"\n",
      "1.0\t\"AAAE\"\n",
      "1.0\t\"AAAA\"\n",
      "1.0\t\"AAAA\"\n",
      "1.0\t\"AAA\"\n",
      "1.0\t\"AA\"\n",
      "1.0\t\"A's\"\n",
      "1.0\t\"A\"\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output;mkdir output;cd output;aws s3 cp --recursive s3://ucb-mids-mls-sayantan-satpati/hw53/density/ .;\n",
    "!cat output/part-0000* | sort -nrk 1 > merged\n",
    "!head -n 10 merged\n",
    "!tail -n 10 merged\n",
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Most Dense Words\n",
    "\n",
    "```\n",
    "29.704999999999998\t\"Death\"\n",
    "14.912000000000001\t\"write\"\n",
    "14.912000000000001\t\"and\"\n",
    "12.380000000000001\t\"NA\"\n",
    "11.557\t\"xxxx\"\n",
    "11.557\t\"xxxx\"\n",
    "10.882\t\"Sc\"\n",
    "10.6\t\"beep\"\n",
    "10.022\t\"blah\"\n",
    "9.8040000000000003\t\"whole\"\n",
    "```\n",
    "\n",
    "### Top 10 Least Dense Words\n",
    "\n",
    "```\n",
    "1.0\t\"AAAS\"\n",
    "1.0\t\"AAAI\"\n",
    "1.0\t\"AAAE\"\n",
    "1.0\t\"AAAE\"\n",
    "1.0\t\"AAAA\"\n",
    "1.0\t\"AAAA\"\n",
    "1.0\t\"AAA\"\n",
    "1.0\t\"AA\"\n",
    "1.0\t\"A's\"\n",
    "1.0\t\"A\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (D)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_hw53_4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_hw53_4.py\n",
    "#!/usr/bin/python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "from itertools import combinations\n",
    "\n",
    "class DistributionNgram(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   reducer=self.reducer,\n",
    "                   jobconf={\n",
    "                            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'mapred.text.key.comparator.options': '-k1,1rn',\n",
    "                            }\n",
    "                   )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line.strip()\n",
    "        tokens = re.split(\"\\t\",line)\n",
    "        yield int(tokens[1]), tokens[0]\n",
    "    \n",
    "    def reducer(self, count, ngrams):\n",
    "        for ngram in ngrams:\n",
    "            yield count, ngram\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    DistributionNgram.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mrjob_hw53_4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/_SUCCESS\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00001\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00006\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00000\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00003\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00002\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00004\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00005\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rm --recursive s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/\n",
    "!python mrjob_hw53_4.py -q -r emr \\\n",
    " s3://filtered-5grams/ \\\n",
    " --output-dir=s3://ucb-mids-mls-sayantan-satpati/hw53/distribution \\\n",
    " --no-output \\\n",
    " --no-strict-protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/_SUCCESS to ./_SUCCESS\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00000 to ./part-00000\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00002 to ./part-00002\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00001 to ./part-00001\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00004 to ./part-00004\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00003 to ./part-00003\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00005 to ./part-00005\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00006 to ./part-00006\n",
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n",
      "cat: stdout: Broken pipe\n",
      "2223343\t\"on the basis of the\"\n",
      "1462063\t\"at the head of the\"\n",
      "1316960\t\"as well as in the\"\n",
      "871158\t\"at the same time the\"\n",
      "833764\t\"as a matter of fact\"\n",
      "590885\t\"as a part of the\"\n",
      "544783\t\"as one of the most\"\n",
      "533464\t\"and at the end of\"\n",
      "447364\t\"I do not think that\"\n",
      "441568\t\"from the beginning of the\"\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output;mkdir output;cd output;aws s3 cp --recursive s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/ .;\n",
    "!cat output/part-0000* | sort -nrk 1 > output_hw53_4.txt\n",
    "!rm -rf ./output\n",
    "!head -n 10 output_hw53_4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 NGram Distribution\n",
    "\n",
    "```\n",
    "2223343\t\"on the basis of the\"\n",
    "1462063\t\"at the head of the\"\n",
    "1316960\t\"as well as in the\"\n",
    "871158\t\"at the same time the\"\n",
    "833764\t\"as a matter of fact\"\n",
    "590885\t\"as a part of the\"\n",
    "544783\t\"as one of the most\"\n",
    "533464\t\"and at the end of\"\n",
    "447364\t\"I do not think that\"\n",
    "441568\t\"from the beginning of the\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.3\n",
    "---\n",
    "\n",
    "In this part of the assignment we will focus on developing methods\n",
    "for detecting synonyms, using the Google 5-grams dataset. To accomplish\n",
    "this you must script two main tasks using MRJob:\n",
    "\n",
    "(1) Build stripes of word co-ocurrence for the top 10,000\n",
    "most frequently appearing words across the entire set of 5-grams,\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "(2) Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "==Design notes for (1)==\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2).\n",
    "\n",
    "==Design notes for (2)==\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Cosine similarity\n",
    "- Kendall correlation\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 5.3: Part 1 > Step 1:\n",
    "\n",
    "***Find the frequent unigrams for the top 10,000 most frequently appearing words across the entire set of 5-grams***\n",
    "\n",
    "* Use the freqency count output from earlier step to get the top 10K most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/_SUCCESS to ./_SUCCESS\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00006 to ./part-00006\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00002 to ./part-00002\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00000 to ./part-00000\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00005 to ./part-00005\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00003 to ./part-00003\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00004 to ./part-00004\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00001 to ./part-00001\n",
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n",
      "5375699242\t\"the\"\n",
      "3691308874\t\"of\"\n",
      "2221164346\t\"to\"\n",
      "1387638591\t\"in\"\n",
      "1342195425\t\"a\"\n",
      "1135779433\t\"and\"\n",
      "798553959\t\"that\"\n",
      "756296656\t\"is\"\n",
      "688053106\t\"be\"\n",
      "481373389\t\"as\"\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output;mkdir output;cd output;aws s3 cp --recursive s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words .;\n",
    "!cat output/part-0000* | sort -nrk 1 | head -n 10000 > output/frequent_unigrams.txt\n",
    "!head -n 10 output/frequent_unigrams.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 5.3: Part 1 > Step 2:\n",
    "\n",
    "***Build stripes of word co-ocurrence for the top 10,000\n",
    "most frequently appearing words across the entire set of 5-grams***\n",
    "\n",
    "* Pass the 1 item frequent set generated from earlier step to the mapper(s) for filtering\n",
    "* The reducers would sum and emit Word Co-Occurrence if support count of 10,000 is met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting word_cooccurrences_set_hw54.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile word_cooccurrences_set_hw54.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "class WordCoOccurrenceFrequentSet(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer)\n",
    "        ]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        # Load the file into memory\n",
    "        self.unigrams = {}\n",
    "        with open('frequent_unigrams.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split('\\t')\n",
    "                self.unigrams[tokens[1].replace(\"\\\"\",\"\")] = int(tokens[0])\n",
    "        sys.stderr.write('### of unigrams: {0}'.format(self.unigrams))\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        tokens = line.strip().split('\\t')\n",
    "        # List of 5-grams\n",
    "        words = tokens[0].split()\n",
    "        # Filter 5-grams to only those in list\n",
    "        words = [w for w in words if w in self.unigrams.keys()]\n",
    "        l = len(words)\n",
    "        for i in xrange(l):\n",
    "            d = {}\n",
    "            for j in xrange(l):\n",
    "                if i != j:\n",
    "                    d[words[j]] = d.get(words[j], 0) + 1\n",
    "            # Emit word, stripe\n",
    "            yield words[i],d\n",
    "\n",
    "    def combiner(self, word, stripes):\n",
    "        d = {}\n",
    "        # Aggregate stripes\n",
    "        for s in stripes:\n",
    "            for k, v in s.iteritems():\n",
    "                d[k] = d.get(k, 0) + v\n",
    "        yield word,d\n",
    "        \n",
    "    def reducer(self, word, stripes):\n",
    "        d = {}\n",
    "        # Aggregate stripes\n",
    "        for s in stripes:\n",
    "            for k, v in s.iteritems():\n",
    "                d[k] = d.get(k, 0) + v\n",
    "        '''\n",
    "        # Filter based on support count (Not a requirement!)\n",
    "        d_final = {}\n",
    "        for k,v in d.iteritems():\n",
    "            if v >= 10000:\n",
    "                d_final[k] = v\n",
    "        '''\n",
    "        # Combine stripes\n",
    "        yield word,d\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    WordCoOccurrenceFrequentSet.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x word_cooccurrences_set_hw54.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!aws s3 rm --recursive s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/\n",
    "!python word_cooccurrences_set_hw54.py -q -r emr \\\n",
    " s3://filtered-5grams/ \\\n",
    " --file output/frequent_unigrams.txt \\\n",
    " --output-dir=s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur \\\n",
    " --no-output \\\n",
    " --no-strict-protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 5.3 Part 2 > Step 1\n",
    "\n",
    "***From the output of Part 1 > Step 2, create a single file with all the stripes - frequent itemsets of size 2 (s=10000)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00000 to ./part-00000\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00006 to ./part-00006\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00002 to ./part-00002\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00007 to ./part-00007\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00004 to ./part-00004\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00005 to ./part-00005\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00009 to ./part-00009\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00003 to ./part-00003\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/_SUCCESS to ./_SUCCESS\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00010 to ./part-00010\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00008 to ./part-00008\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00001 to ./part-00001\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00011 to ./part-00011\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00013 to ./part-00013\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00012 to ./part-00012\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00014 to ./part-00014\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00016 to ./part-00016\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00015 to ./part-00015\n",
      "    1549 output/frequent_stripes.txt\n",
      "\"Act\"\t22567\n",
      "\"Africa\"\t16986\n",
      "\"African\"\t15020\n",
      "\"After\"\t17243\n",
      "\"America\"\t37706\n",
      "\"American\"\t93300\n",
      "\"And\"\t79322\n",
      "\"Asia\"\t10677\n",
      "\"Assembly\"\t10348\n",
      "\"Association\"\t12421\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output;mkdir output;cd output;aws s3 cp --recursive s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/ .;\n",
    "!cat output/part-0000* | sort -k 1 > output/frequent_stripes.txt\n",
    "!wc -l output/frequent_stripes.txt\n",
    "!head -n 10 output/frequent_stripes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile dist_calc_hw54.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "import sys\n",
    "import ast\n",
    "from sets import Set\n",
    "import math\n",
    "\n",
    "\n",
    "class DistanceCalc(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper)\n",
    "        ]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        # Load the file into memory\n",
    "        self.stripes = {}\n",
    "        with open('frequent_stripes.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split('\\t')\n",
    "                self.stripes[tokens[0].replace(\"\\\"\",\"\")] = ast.literal_eval(tokens[1])\n",
    "        sys.stderr.write('### of stripes: {0}'.format(len(self.stripes)))\n",
    "\n",
    "    def mapper(self, _, line):        \n",
    "        tokens = line.strip().split('\\t')\n",
    "        key = tokens[0].replace(\"\\\"\",\"\")\n",
    "        dict_pairs = ast.literal_eval(tokens[1])\n",
    "        for n_key, n_dict_pairs in self.stripes.iteritems():\n",
    "            # Do distance calc for only (a,b) but not (b,a) --> Redundant\n",
    "            if key > n_key:\n",
    "                continue\n",
    "            \n",
    "            s1 = Set(dict_pairs.keys())\n",
    "            s2 = Set(n_dict_pairs.keys())\n",
    "            common_keys = s1.intersection(s2)\n",
    "            l_uniq = s1.difference(s2)\n",
    "            r_uniq = s2.difference(s1)\n",
    "            \n",
    "            euclidean_distance = 0\n",
    "            for k in common_keys:\n",
    "                euclidean_distance += (dict_pairs.get(k) - n_dict_pairs.get(k)) ** 2\n",
    "            for k in l_uniq:\n",
    "                euclidean_distance += dict_pairs.get(k) ** 2\n",
    "            for k in r_uniq:\n",
    "                euclidean_distance += n_dict_pairs.get(k) ** 2\n",
    "                \n",
    "            yield (key, n_key, 'E'), math.sqrt(euclidean_distance)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    DistanceCalc.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x dist_calc_hw54.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
