{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sayantan Satpati**\n",
    "* **sayantan.satpati@ischool.berkeley.edu**\n",
    "* **W261**\n",
    "* **Week-5**\n",
    "* **Assignment-5**\n",
    "* **Date of Submission: 07-OCT-2015**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  === Week 5: mrjob, aws, and n-grams ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.0\n",
    "---\n",
    "\n",
    "***What is a data warehouse? What is a Star schema? When is it used?***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.1\n",
    "---\n",
    "\n",
    "***In the database world What is 3NF? Does machine learning use data in 3NF? If so why? ***\n",
    "\n",
    "***In what form does ML consume data?***\n",
    "\n",
    "***Why would one use log files that are denormalized?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.2\n",
    "---\n",
    "\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, \n",
    "right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)\n",
    ":\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "```\n",
    "(1) Left joining Table Left with Table Right\n",
    "(2) Right joining Table Left with Table Right\n",
    "(3) Inner joining Table Left with Table Right\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "***File containing url(s) look like follows:***\n",
    "\n",
    "```\n",
    "[cloudera@localhost wk5]$ head -5 ../wk4/url \n",
    "A,1287,1,\"International AutoRoute\",\"/autoroute\"\n",
    "A,1288,1,\"library\",\"/library\"\n",
    "A,1289,1,\"Master Chef Product Information\",\"/masterchef\"\n",
    "A,1297,1,\"Central America\",\"/centroam\"\n",
    "A,1215,1,\"For Developers Only Info\",\"/developer\"\n",
    "```\n",
    "\n",
    "***File containing Page Visits by Customers look like follows:***\n",
    "\n",
    "```\n",
    "V,1000,1,C,10001\n",
    "V,1001,1,C,10001\n",
    "V,1002,1,C,10001\n",
    "V,1001,1,C,10002\n",
    "V,1003,1,C,10002\n",
    "```\n",
    "\n",
    "* For **INNER** and **LEFT** join, the url file is passed to the mrjob as a '--file' parameter; it is then loaded by the reducers in a dict for INNER and LEFT join. After determining the page Vists in descending order by customers in the first pass, the url details are added, using the url file, in the 2nd pass. In the case of a LEFT join, the page visits would output the URL as 'NA', if the 'url' file is missing any URL. In the case of an INNER join, the page visits would be output only if there is a macthing URL. Since no URL(s) are missing from the 'url' file, the result from LEFT & INNER joins are identical having 98654 rows. The type of join is passed to mrjob as a parameter.\n",
    "* For **RIGHT** join, we are supposed to show the page visits for all URLS present in the 'url' file, even though they have not been visited. For this one, we pass the 'pages visited file' as a '--file' parameter, and the url file as an input to the mrjob. Since there are urls which haven't been visited, the output 98663 rows, which is more than the previous 2 outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_join_hw52.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_join_hw52.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.compat import get_jobconf_value\n",
    "import sys\n",
    "\n",
    "\n",
    "class MRFrequentVisitor(MRJob):\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer),\n",
    "             MRStep(mapper=self.mapper_frequent_visitor,\n",
    "                   reducer_init=self.reducer_frequent_visitor_init,\n",
    "                   reducer=self.reducer_frequent_visitor)\n",
    "        ]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        tokens = line.strip().split(\",\")\n",
    "        key = \"{0},{1}\".format(tokens[1],tokens[4])\n",
    "        yield key, 1\n",
    "\n",
    "    def combiner(self, key, counts):\n",
    "        yield key, sum(counts)\n",
    "\n",
    "    def reducer(self, key, counts):\n",
    "        yield key, sum(counts)\n",
    "        \n",
    "    # 2nd Pass\n",
    "    \n",
    "    def mapper_frequent_visitor(self, key, value):\n",
    "        tokens = key.strip().split(\",\")\n",
    "        modified_key = \"{0},{1}\".format(tokens[0],value)\n",
    "        yield modified_key, tokens[1]\n",
    "     \n",
    "    \n",
    "    def reducer_frequent_visitor_init(self):\n",
    "        # Reads the 'url' file into a Dict for displaying additional information\n",
    "        self.last_page = None\n",
    "        self.pageDict = {}\n",
    "        with open('url','r') as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split(\",\")\n",
    "                self.pageDict[tokens[1]] = tokens[4]\n",
    "                \n",
    "    def reducer_frequent_visitor(self, key, values):\n",
    "        join_type = get_jobconf_value('join_type')\n",
    "        sys.stderr.write('### JOIN TYPE: {0}'.format(join_type))\n",
    "        tokens = key.strip().split(\",\")\n",
    "        page = tokens[0]\n",
    "        visits = int(tokens[1])\n",
    "        \n",
    "        if self.last_page != page:\n",
    "            self.last_page = page\n",
    "            # values might be a list, if there is a tie for same key => (p1, 1000), [v1,v2,v3..]\n",
    "            \n",
    "            # Emit even if page url is not present\n",
    "            if join_type == 'left' or join_type == 'right':\n",
    "                page_url = self.pageDict.get(page, 'NA').replace(\"\\\"\",\"\")\n",
    "                for value in values:\n",
    "                    k = '{0},{1}'.format(page, \n",
    "                                         page_url)\n",
    "                    v = '{0},{1}'.format(visits,\n",
    "                                        value)\n",
    "                    yield k,v\n",
    "                    \n",
    "            # Emit only if page url is present\n",
    "            if join_type == 'inner':\n",
    "                page_url = self.pageDict.get(page, 'NA').replace(\"\\\"\",\"\")\n",
    "                if page_url != 'NA':\n",
    "                    for value in values:\n",
    "                        k = '{0},{1}'.format(page, \n",
    "                                             page_url)\n",
    "                        v = '{0},{1}'.format(visits,\n",
    "                                            value)\n",
    "                        yield k,v\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRFrequentVisitor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mrjob_join_hw52.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_join_right_hw52.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_join_right_hw52.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.compat import get_jobconf_value\n",
    "import sys\n",
    "\n",
    "\n",
    "class MRFrequentVisitor1(MRJob):\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper)\n",
    "        ]\n",
    "\n",
    "    def mapper_init(self):\n",
    "        self.pageVisitsDict = {}\n",
    "        with open('anonymous-msweb.data.pp','r') as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split(\",\")\n",
    "                page = tokens[1]\n",
    "                visitor = tokens[4]\n",
    "                if page not in self.pageVisitsDict.keys():\n",
    "                    self.pageVisitsDict[page] = {}\n",
    "                if visitor not in self.pageVisitsDict[page].keys():\n",
    "                    self.pageVisitsDict[page][visitor] = 0\n",
    "                self.pageVisitsDict[page][visitor] = self.pageVisitsDict[page][visitor] + 1\n",
    "            \n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        tokens = line.strip().split(\",\")\n",
    "        page = tokens[1]\n",
    "        page_url = tokens[4].replace(\"\\\"\",\"\")\n",
    "        if page in self.pageVisitsDict.keys():\n",
    "            for k,v in self.pageVisitsDict[page].iteritems():\n",
    "                \n",
    "                key = '{0},{1}'.format(page, \n",
    "                                    page_url)\n",
    "                value = '{0},{1}'.format(v,\n",
    "                                     k)\n",
    "                yield (key,value)\n",
    "        else:\n",
    "            k = '{0},{1}'.format(page, \n",
    "                                page_url)\n",
    "            v = '{0},{1}'.format(0,\n",
    "                                 'NA')\n",
    "            yield (k,v)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRFrequentVisitor1.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mrjob_join_right_hw52.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.0.0:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "map.output.key.field.separator: mapreduce.map.output.key.field.separator\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.0.0:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "map.output.key.field.separator: mapreduce.map.output.key.field.separator\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### JOIN TYPE: left\n",
      "\"1000,/regwiz\"\t\"1,42411\"\n",
      "\"1000,/regwiz\"\t\"1,42381\"\n",
      "\"1000,/regwiz\"\t\"1,42320\"\n",
      "\"1000,/regwiz\"\t\"1,42291\"\n",
      "\"1000,/regwiz\"\t\"1,42285\"\n",
      "\"1000,/regwiz\"\t\"1,42260\"\n",
      "\"1000,/regwiz\"\t\"1,42213\"\n",
      "\"1000,/regwiz\"\t\"1,42198\"\n",
      "\"1000,/regwiz\"\t\"1,42176\"\n",
      "\"1000,/regwiz\"\t\"1,42160\"\n",
      "98654 output_hw52.txt\n",
      "### JOIN TYPE: right\n",
      "\"1287,/autoroute\"\t\"0,NA\"\n",
      "\"1288,/library\"\t\"0,NA\"\n",
      "\"1289,/masterchef\"\t\"0,NA\"\n",
      "\"1297,/centroam\"\t\"0,NA\"\n",
      "\"1215,/developer\"\t\"1,40224\"\n",
      "\"1215,/developer\"\t\"1,28813\"\n",
      "\"1215,/developer\"\t\"1,35353\"\n",
      "\"1215,/developer\"\t\"1,28058\"\n",
      "\"1215,/developer\"\t\"1,27881\"\n",
      "\"1215,/developer\"\t\"1,42447\"\n",
      "98663 output_hw52.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.0.0:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "map.output.key.field.separator: mapreduce.map.output.key.field.separator\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.0.0:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "map.output.key.field.separator: mapreduce.map.output.key.field.separator\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### JOIN TYPE: inner\n",
      "\"1000,/regwiz\"\t\"1,42411\"\n",
      "\"1000,/regwiz\"\t\"1,42381\"\n",
      "\"1000,/regwiz\"\t\"1,42320\"\n",
      "\"1000,/regwiz\"\t\"1,42291\"\n",
      "\"1000,/regwiz\"\t\"1,42285\"\n",
      "\"1000,/regwiz\"\t\"1,42260\"\n",
      "\"1000,/regwiz\"\t\"1,42213\"\n",
      "\"1000,/regwiz\"\t\"1,42198\"\n",
      "\"1000,/regwiz\"\t\"1,42176\"\n",
      "\"1000,/regwiz\"\t\"1,42160\"\n",
      "98654 output_hw52.txt\n"
     ]
    }
   ],
   "source": [
    "# Running mrjob using a Hadoop Runner in local cluster\n",
    "from mrjob_join_hw52 import MRFrequentVisitor\n",
    "from mrjob_join_right_hw52 import MRFrequentVisitor1\n",
    "import os\n",
    "\n",
    "# Passing Hadoop Streaming parameters to:\n",
    "# partition by leftmost part of composite key\n",
    "# secodary sort by rightmost part of the same composite key\n",
    "\n",
    "join_types = ['left', 'right', 'inner']\n",
    "\n",
    "# Run for each Join Type\n",
    "for jt in join_types:\n",
    "    mr_job = None\n",
    "    if jt == 'left' or jt == 'inner':\n",
    "        mr_job = MRFrequentVisitor(args=['-r', 'hadoop', \n",
    "                                         '--hadoop-home', '/usr/lib/hadoop-0.20-mapreduce',\n",
    "                                         '--hadoop-bin', '/usr/bin/hadoop',\n",
    "                                         '--file', '../wk4/url',\n",
    "                                         '--jobconf', 'join_type={0}'.format(jt),\n",
    "                                         '--jobconf', 'stream.num.map.output.key.fields=2',\n",
    "                                         '--jobconf', 'map.output.key.field.separator=,',\n",
    "                                         '--jobconf', 'mapred.text.key.partitioner.options=-k1,1',\n",
    "                                         '--jobconf', 'mapred.text.key.comparator.options=-k1,1 -k2,2nr',\n",
    "                                         '--jobconf', 'mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                                         '--partitioner', 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner',\n",
    "                                         '../wk4/anonymous-msweb.data.pp', '-v', '--no-strict-protocol'])\n",
    "    else:\n",
    "        mr_job = MRFrequentVisitor1(args=['-r', 'hadoop', \n",
    "                                         '--hadoop-home', '/usr/lib/hadoop-0.20-mapreduce',\n",
    "                                         '--hadoop-bin', '/usr/bin/hadoop',\n",
    "                                         '--file', '../wk4/anonymous-msweb.data.pp',\n",
    "                                         '../wk4/url', '-v', '--no-strict-protocol'])\n",
    "\n",
    "\n",
    "    output_file = \"output_hw52.txt\"\n",
    "    try:\n",
    "        os.remove(output_file)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    with mr_job.make_runner() as runner, open(output_file, 'a') as f: \n",
    "        runner.run()\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            #print mr_job.parse_output_line(line)\n",
    "            f.write(line)\n",
    "\n",
    "    print '### JOIN TYPE: {0}'.format(jt)\n",
    "    !head -n 10 output_hw52.txt\n",
    "    !wc -l output_hw52.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.3\n",
    "---\n",
    "\n",
    "For the remainder of this assignment you will work with a large subset \n",
    "of the Google n-grams dataset,\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket on s3:\n",
    "\n",
    "s3://filtered-5grams/\n",
    "\n",
    "In particular, this bucket contains (~200) files in the format:\n",
    "\n",
    "(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (count), i.e., unigrams\n",
    "- Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency (Hint: save to PART-000* and take the head -n 1000)\n",
    "- Distribution of 5-gram sizes (counts) sorted in decreasing order of relative frequency. (Hint: save to PART-000* and take the head -n 1000)\n",
    "OPTIONAL Question:\n",
    "- Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see:\n",
    "https://en.wikipedia.org/wiki/Log%E2%80%93log_plot\n",
    "https://en.wikipedia.org/wiki/Power_law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longest 5-gram (number of characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_hw53_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_hw53_1.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "\n",
    "class LongestNgram(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_ngrams_len,\n",
    "                   reducer=self.reducer_ngrams_len),\n",
    "            MRStep(reducer=self.reducer_find_max_ngram)\n",
    "        ]\n",
    "\n",
    "    def mapper_ngrams_len(self, _, line):\n",
    "        tokens = line.strip().split('\\t')\n",
    "        yield (tokens[0], len(tokens[0]))\n",
    "\n",
    "  \n",
    "    def reducer_ngrams_len(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    # discard the key; it is just None\n",
    "    def reducer_find_max_ngram(self, _, word_count_pairs):\n",
    "        # each item of word_count_pairs is (count, word),\n",
    "        # so yielding one results in key=counts, value=word\n",
    "        yield max(word_count_pairs)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    LongestNgram.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x mrjob_hw53_1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "(159, 'ROPLEZIMPREDASTRODONBRASLPKLSON YHROACLMPARCHEYXMMIOUDAVESAURUS PIOFPILOCOWERSURUASOGETSESNEGCP TYRAVOPSIFENGOQUAPIALLOBOSKENUO OWINFUYAIOKENECKSASXHYILPOYNUAT')\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from mrjob_hw53_1 import LongestNgram\n",
    "mr_job = LongestNgram(args=['s3://filtered-5grams',\n",
    "                            '-r', 'emr', '--no-strict-protocol'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_hw53_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_hw53_2.py\n",
    "#!/usr/bin/python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "from itertools import combinations\n",
    "\n",
    "class FrequentUnigrams(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer),\n",
    "             MRStep(mapper=self.mapper_frequent_unigrams,\n",
    "                   reducer=self.reducer_frequent_unigrams,\n",
    "                   jobconf={\n",
    "                            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'mapred.text.key.comparator.options': '-k1,1rn',\n",
    "                            }\n",
    "                   )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line.strip()\n",
    "        tokens = re.split(\"\\t\",line)\n",
    "        unigrams = tokens[0].split()\n",
    "        count = int(tokens[1])\n",
    "        for unigram in unigrams:\n",
    "            yield unigram, count\n",
    "    \n",
    "    def combiner(self, unigram, counts):\n",
    "        yield unigram, sum(counts)\n",
    "    \n",
    "    def reducer(self, unigram, counts):\n",
    "        yield unigram, sum(counts)\n",
    "        \n",
    "    def mapper_frequent_unigrams(self, unigram, count):\n",
    "        yield count, unigram\n",
    "        \n",
    "    def reducer_frequent_unigrams(self, count, unigrams):\n",
    "        for unigram in unigrams:\n",
    "            yield count, unigram\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    FrequentUnigrams.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mrjob_hw53_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /Users/ssatpati/.mrjob.conf\n",
      "using existing scratch bucket mrjob-d5ed1dc31babbe2c\n",
      "using s3://mrjob-d5ed1dc31babbe2c/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/mrjob_hw53_2.ssatpati.20151003.062810.595234\n",
      "writing master bootstrap script to /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/mrjob_hw53_2.ssatpati.20151003.062810.595234/b.py\n",
      "Copying non-input files into s3://mrjob-d5ed1dc31babbe2c/tmp/mrjob_hw53_2.ssatpati.20151003.062810.595234/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-QGLC3PJPOLBE\n",
      "Created new job flow j-QGLC3PJPOLBE\n",
      "Job launched 30.4s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 61.3s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 91.8s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 122.8s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 153.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 184.0s ago, status STARTING: Configuring cluster software\n",
      "Job launched 214.4s ago, status STARTING: Configuring cluster software\n",
      "Job launched 245.4s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 276.0s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 306.6s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 337.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 368.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 398.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 429.5s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 460.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 491.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 521.5s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 552.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 582.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 613.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 643.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 674.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 705.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 735.9s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 766.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 796.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 827.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 858.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 888.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 919.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 949.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 980.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1010.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1041.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1072.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1102.6s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1133.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1163.5s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1193.9s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1224.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1255.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1285.5s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1316.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1346.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1377.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1408.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1438.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1469.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1500.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1530.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1561.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1591.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1622.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1652.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1683.6s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1714.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1745.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1775.5s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1806.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1836.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1867.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1898.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1928.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1959.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1989.9s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2020.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2051.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2081.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2112.6s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2143.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2173.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2204.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2234.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2265.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2296.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2326.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2357.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2387.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2418.6s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2449.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2479.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2510.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2541.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2571.5s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2602.9s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2633.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2663.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2694.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2725.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2755.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2785.9s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2816.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2847.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2877.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2908.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2939.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2970.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3000.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3031.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3061.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3092.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3123.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3153.6s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3184.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3214.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3245.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3275.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3306.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 2 of 2)\n",
      "Job launched 3336.9s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 2 of 2)\n",
      "Job launched 3367.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 2 of 2)\n",
      "Job launched 3398.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 2 of 2)\n",
      "Job completed.\n",
      "Running time was 3103.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 2156069116\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 5251252\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 292640853\n",
      "    FILE_BYTES_WRITTEN: 323462370\n",
      "    HDFS_BYTES_READ: 23640\n",
      "    HDFS_BYTES_WRITTEN: 5251252\n",
      "    S3_BYTES_READ: 2156069116\n",
      "  Job Counters :\n",
      "    Launched map tasks: 193\n",
      "    Launched reduce tasks: 8\n",
      "    Rack-local map tasks: 191\n",
      "    SLOTS_MILLIS_MAPS: 22513524\n",
      "    SLOTS_MILLIS_REDUCES: 10666442\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 7828220\n",
      "    Combine input records: 307583853\n",
      "    Combine output records: 22094566\n",
      "    Map input bytes: 2156069116\n",
      "    Map input records: 58682266\n",
      "    Map output bytes: 3136729760\n",
      "    Map output materialized bytes: 98214630\n",
      "    Map output records: 293411330\n",
      "    Physical memory (bytes) snapshot: 79829819392\n",
      "    Reduce input groups: 343019\n",
      "    Reduce input records: 7922043\n",
      "    Reduce output records: 343019\n",
      "    Reduce shuffle bytes: 98214630\n",
      "    SPLIT_RAW_BYTES: 23640\n",
      "    Spilled Records: 30016609\n",
      "    Total committed heap usage (bytes): 86414835712\n",
      "    Virtual memory (bytes) snapshot: 188305924096\n",
      "Counters from step 2:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 6161441\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 5251252\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 4010303\n",
      "    FILE_BYTES_WRITTEN: 9264521\n",
      "    HDFS_BYTES_READ: 6164738\n",
      "    S3_BYTES_WRITTEN: 5251252\n",
      "  Job Counters :\n",
      "    Data-local map tasks: 21\n",
      "    Launched map tasks: 21\n",
      "    Launched reduce tasks: 8\n",
      "    SLOTS_MILLIS_MAPS: 239490\n",
      "    SLOTS_MILLIS_REDUCES: 174447\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 46410\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 5251252\n",
      "    Map input records: 343019\n",
      "    Map output bytes: 5251252\n",
      "    Map output materialized bytes: 4500335\n",
      "    Map output records: 343019\n",
      "    Physical memory (bytes) snapshot: 7540547584\n",
      "    Reduce input groups: 41362\n",
      "    Reduce input records: 343019\n",
      "    Reduce output records: 343019\n",
      "    Reduce shuffle bytes: 4500335\n",
      "    SPLIT_RAW_BYTES: 3297\n",
      "    Spilled Records: 686038\n",
      "    Total committed heap usage (bytes): 7062859776\n",
      "    Virtual memory (bytes) snapshot: 26687610880\n",
      "removing tmp directory /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/mrjob_hw53_2.ssatpati.20151003.062810.595234\n",
      "Removing all files in s3://mrjob-d5ed1dc31babbe2c/tmp/mrjob_hw53_2.ssatpati.20151003.062810.595234/\n",
      "Removing all files in s3://mrjob-d5ed1dc31babbe2c/tmp/logs/j-QGLC3PJPOLBE/\n",
      "Terminating job flow: j-QGLC3PJPOLBE\n"
     ]
    }
   ],
   "source": [
    "!python mrjob_hw53_2.py -r emr \\\n",
    " s3://filtered-5grams \\\n",
    " --output-dir=s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words \\\n",
    " --no-output \\\n",
    " --no-strict-protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/_SUCCESS to ./_SUCCESS\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00003 to ./part-00003\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00000 to ./part-00000\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00002 to ./part-00002\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00004 to ./part-00004\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00006 to ./part-00006\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00001 to ./part-00001\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00005 to ./part-00005\n",
      "5375699242\t\"the\"\n",
      "3691308874\t\"of\"\n",
      "2221164346\t\"to\"\n",
      "1387638591\t\"in\"\n",
      "1342195425\t\"a\"\n",
      "1135779433\t\"and\"\n",
      "798553959\t\"that\"\n",
      "756296656\t\"is\"\n",
      "688053106\t\"be\"\n",
      "481373389\t\"as\"\n",
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output;mkdir output;cd output;aws s3 cp --recursive s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words .;\n",
    "!cat output/part-0000* | sort -nrk 1 | head -n 10\n",
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Frequent Words are:\n",
    "\n",
    "```\n",
    "5375699242\t\"the\"\n",
    "3691308874\t\"of\"\n",
    "2221164346\t\"to\"\n",
    "1387638591\t\"in\"\n",
    "1342195425\t\"a\"\n",
    "1135779433\t\"and\"\n",
    "798553959\t\"that\"\n",
    "756296656\t\"is\"\n",
    "688053106\t\"be\"\n",
    "481373389\t\"as\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_hw53_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_hw53_3.py\n",
    "#!/usr/bin/python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "from itertools import combinations\n",
    "\n",
    "class DenseWords(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer,\n",
    "                   ),\n",
    "            MRStep(mapper=self.mapper_max_min,\n",
    "                   reducer=self.reducer_max_min,\n",
    "                   jobconf={\n",
    "                            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'mapred.text.key.comparator.options': '-k1,1rn',\n",
    "                            }\n",
    "                   )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        tokens = line.strip().split('\\t')\n",
    "        unigrams = tokens[0].split()\n",
    "        density = round((int(tokens[1]) * 1.0 / int(tokens[2])), 3)\n",
    "        for unigram in unigrams:\n",
    "            yield unigram, density\n",
    "            \n",
    "    def combiner(self, unigram, densities):\n",
    "        densities = [d for d in densities]\n",
    "        yield unigram, min(densities) \n",
    "        yield unigram, max(densities)\n",
    "        \n",
    "    def reducer(self, unigram, densities):\n",
    "        densities = [d for d in densities]\n",
    "        yield unigram, min(densities)\n",
    "        yield unigram, max(densities)\n",
    "        \n",
    "    def mapper_max_min(self, unigram, density):\n",
    "        yield density, unigram\n",
    "        \n",
    "    def reducer_max_min(self, density, unigrams):\n",
    "        for unigram in unigrams:\n",
    "            yield density, unigram\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    DenseWords.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mrjob_hw53_3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/density/_SUCCESS\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00001\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00000\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00002\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00004\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00003\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00005\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00006\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rm --recursive s3://ucb-mids-mls-sayantan-satpati/hw53/density/\n",
    "!python mrjob_hw53_3.py -q -r emr \\\n",
    " s3://filtered-5grams \\\n",
    " --output-dir=s3://ucb-mids-mls-sayantan-satpati/hw53/density \\\n",
    " --no-output \\\n",
    " --no-strict-protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/_SUCCESS to ./_SUCCESS\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00003 to ./part-00003\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00004 to ./part-00004\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00005 to ./part-00005\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00006 to ./part-00006\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00000 to ./part-00000\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00001 to ./part-00001\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00007 to ./part-00007\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00002 to ./part-00002\n",
      "29.704999999999998\t\"Death\"\n",
      "14.912000000000001\t\"write\"\n",
      "14.912000000000001\t\"and\"\n",
      "12.380000000000001\t\"NA\"\n",
      "11.557\t\"xxxx\"\n",
      "11.557\t\"xxxx\"\n",
      "10.882\t\"Sc\"\n",
      "10.6\t\"beep\"\n",
      "10.022\t\"blah\"\n",
      "9.8040000000000003\t\"whole\"\n",
      "1.0\t\"AAAS\"\n",
      "1.0\t\"AAAI\"\n",
      "1.0\t\"AAAE\"\n",
      "1.0\t\"AAAE\"\n",
      "1.0\t\"AAAA\"\n",
      "1.0\t\"AAAA\"\n",
      "1.0\t\"AAA\"\n",
      "1.0\t\"AA\"\n",
      "1.0\t\"A's\"\n",
      "1.0\t\"A\"\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output;mkdir output;cd output;aws s3 cp --recursive s3://ucb-mids-mls-sayantan-satpati/hw53/density/ .;\n",
    "!cat output/part-0000* | sort -nrk 1 > merged\n",
    "!head -n 10 merged\n",
    "!tail -n 10 merged\n",
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Most Dense Words\n",
    "\n",
    "```\n",
    "29.704999999999998\t\"Death\"\n",
    "14.912000000000001\t\"write\"\n",
    "14.912000000000001\t\"and\"\n",
    "12.380000000000001\t\"NA\"\n",
    "11.557\t\"xxxx\"\n",
    "11.557\t\"xxxx\"\n",
    "10.882\t\"Sc\"\n",
    "10.6\t\"beep\"\n",
    "10.022\t\"blah\"\n",
    "9.8040000000000003\t\"whole\"\n",
    "```\n",
    "\n",
    "### Top 10 Least Dense Words\n",
    "\n",
    "```\n",
    "1.0\t\"AAAS\"\n",
    "1.0\t\"AAAI\"\n",
    "1.0\t\"AAAE\"\n",
    "1.0\t\"AAAE\"\n",
    "1.0\t\"AAAA\"\n",
    "1.0\t\"AAAA\"\n",
    "1.0\t\"AAA\"\n",
    "1.0\t\"AA\"\n",
    "1.0\t\"A's\"\n",
    "1.0\t\"A\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (D)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_hw53_4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_hw53_4.py\n",
    "#!/usr/bin/python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "from itertools import combinations\n",
    "\n",
    "class DistributionNgram(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   reducer=self.reducer,\n",
    "                   jobconf={\n",
    "                            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'mapred.text.key.comparator.options': '-k1,1rn',\n",
    "                            }\n",
    "                   )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line.strip()\n",
    "        tokens = re.split(\"\\t\",line)\n",
    "        yield int(tokens[1]), tokens[0]\n",
    "    \n",
    "    def reducer(self, count, ngrams):\n",
    "        for ngram in ngrams:\n",
    "            yield count, ngram\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    DistributionNgram.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mrjob_hw53_4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/_SUCCESS\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00001\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00006\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00000\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00003\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00002\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00004\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00005\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rm --recursive s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/\n",
    "!python mrjob_hw53_4.py -q -r emr \\\n",
    " s3://filtered-5grams/ \\\n",
    " --output-dir=s3://ucb-mids-mls-sayantan-satpati/hw53/distribution \\\n",
    " --no-output \\\n",
    " --no-strict-protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/_SUCCESS to ./_SUCCESS\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00000 to ./part-00000\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00002 to ./part-00002\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00001 to ./part-00001\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00004 to ./part-00004\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00003 to ./part-00003\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00005 to ./part-00005\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00006 to ./part-00006\n",
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n",
      "cat: stdout: Broken pipe\n",
      "2223343\t\"on the basis of the\"\n",
      "1462063\t\"at the head of the\"\n",
      "1316960\t\"as well as in the\"\n",
      "871158\t\"at the same time the\"\n",
      "833764\t\"as a matter of fact\"\n",
      "590885\t\"as a part of the\"\n",
      "544783\t\"as one of the most\"\n",
      "533464\t\"and at the end of\"\n",
      "447364\t\"I do not think that\"\n",
      "441568\t\"from the beginning of the\"\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output;mkdir output;cd output;aws s3 cp --recursive s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/ .;\n",
    "!cat output/part-0000* | sort -nrk 1 > output_hw53_4.txt\n",
    "!rm -rf ./output\n",
    "!head -n 10 output_hw53_4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 NGram Distribution\n",
    "\n",
    "```\n",
    "2223343\t\"on the basis of the\"\n",
    "1462063\t\"at the head of the\"\n",
    "1316960\t\"as well as in the\"\n",
    "871158\t\"at the same time the\"\n",
    "833764\t\"as a matter of fact\"\n",
    "590885\t\"as a part of the\"\n",
    "544783\t\"as one of the most\"\n",
    "533464\t\"and at the end of\"\n",
    "447364\t\"I do not think that\"\n",
    "441568\t\"from the beginning of the\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.4\n",
    "---\n",
    "\n",
    "In this part of the assignment we will focus on developing methods\n",
    "for detecting synonyms, using the Google 5-grams dataset. To accomplish\n",
    "this you must script two main tasks using MRJob:\n",
    "\n",
    "(1) Build stripes of word co-ocurrence for the top 10,000\n",
    "most frequently appearing words across the entire set of 5-grams,\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "(2) Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "==Design notes for (1)==\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2).\n",
    "\n",
    "==Design notes for (2)==\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Cosine similarity\n",
    "- Kendall correlation\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 5.4: Part 1 > Step 1:\n",
    "\n",
    "***Find the frequent unigrams for the top 10,000 most frequently appearing words across the entire set of 5-grams***\n",
    "\n",
    "* Use the freqency count output from earlier step to get the top 10K most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/_SUCCESS to ./_SUCCESS\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00006 to ./part-00006\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00002 to ./part-00002\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00000 to ./part-00000\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00005 to ./part-00005\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00003 to ./part-00003\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00004 to ./part-00004\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00001 to ./part-00001\n",
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n",
      "5375699242\t\"the\"\n",
      "3691308874\t\"of\"\n",
      "2221164346\t\"to\"\n",
      "1387638591\t\"in\"\n",
      "1342195425\t\"a\"\n",
      "1135779433\t\"and\"\n",
      "798553959\t\"that\"\n",
      "756296656\t\"is\"\n",
      "688053106\t\"be\"\n",
      "481373389\t\"as\"\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output;mkdir output;cd output;aws s3 cp --recursive s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words .;\n",
    "!cat output/part-0000* | sort -nrk 1 | head -n 10000 > output/frequent_unigrams.txt\n",
    "!head -n 10 output/frequent_unigrams.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 5.4: Part 1 > Step 2:\n",
    "\n",
    "***Build stripes of word co-ocurrence for the top 10,000\n",
    "most frequently appearing words across the entire set of 5-grams***\n",
    "\n",
    "* Pass the 1 item frequent set generated from earlier step to the mapper(s) for filtering\n",
    "* The reducers would sum and emit Word Co-Occurrence if support count of 10,000 is met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting word_cooccurrences_set_hw54.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile word_cooccurrences_set_hw54.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "class WordCoOccurrenceFrequentSet(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer)\n",
    "        ]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        # Load the file into memory\n",
    "        self.unigrams = {}\n",
    "        with open('frequent_unigrams.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split('\\t')\n",
    "                self.unigrams[tokens[1].replace(\"\\\"\",\"\")] = int(tokens[0])\n",
    "        sys.stderr.write('### of unigrams: {0}'.format(self.unigrams))\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        tokens = line.strip().split('\\t')\n",
    "        # List of 5-grams\n",
    "        words = tokens[0].split()\n",
    "        # Filter 5-grams to only those in list\n",
    "        words = [w for w in words if w in self.unigrams.keys()]\n",
    "        l = len(words)\n",
    "        for i in xrange(l):\n",
    "            d = {}\n",
    "            for j in xrange(l):\n",
    "                if i != j:\n",
    "                    d[words[j]] = d.get(words[j], 0) + 1\n",
    "            # Emit word, stripe\n",
    "            yield words[i],d\n",
    "\n",
    "    def combiner(self, word, stripes):\n",
    "        d = {}\n",
    "        # Aggregate stripes\n",
    "        for s in stripes:\n",
    "            for k, v in s.iteritems():\n",
    "                d[k] = d.get(k, 0) + v\n",
    "        yield word,d\n",
    "        \n",
    "    def reducer(self, word, stripes):\n",
    "        d = {}\n",
    "        # Aggregate stripes\n",
    "        for s in stripes:\n",
    "            for k, v in s.iteritems():\n",
    "                d[k] = d.get(k, 0) + v\n",
    "        '''\n",
    "        # Filter based on support count (Not a requirement!)\n",
    "        d_final = {}\n",
    "        for k,v in d.iteritems():\n",
    "            if v >= 10000:\n",
    "                d_final[k] = v\n",
    "        '''\n",
    "        # Combine stripes\n",
    "        yield word,d\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    WordCoOccurrenceFrequentSet.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x word_cooccurrences_set_hw54.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!aws s3 rm --recursive s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/\n",
    "!python word_cooccurrences_set_hw54.py -q -r emr \\\n",
    " s3://filtered-5grams/ \\\n",
    " --file output/frequent_unigrams.txt \\\n",
    " --output-dir=s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur \\\n",
    " --no-output \\\n",
    " --no-strict-protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 5.4 Part 2 > Step 1\n",
    "\n",
    "***From the output of Part 1 > Step 2, create a single file with all the stripes - frequent itemsets of size 2***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/_SUCCESS to ./_SUCCESS\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00000 to ./part-00000\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00004 to ./part-00004\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00002 to ./part-00002\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00001 to ./part-00001\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00006 to ./part-00006\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00005 to ./part-00005\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00007 to ./part-00007\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00008 to ./part-00008\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00009 to ./part-00009\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00010 to ./part-00010\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00011 to ./part-00011\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00012 to ./part-00012\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00013 to ./part-00013\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00014 to ./part-00014\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00015 to ./part-00015\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00016 to ./part-00016\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00017 to ./part-00017\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00018 to ./part-00018\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00019 to ./part-00019\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00003 to ./part-00003\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00020 to ./part-00020\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00021 to ./part-00021\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00024 to ./part-00024\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00022 to ./part-00022\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00025 to ./part-00025\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00023 to ./part-00023\n",
      "   10000 output/frequent_stripes.txt\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output;mkdir output;cd output;aws s3 cp --recursive s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/ .;\n",
    "!cat output/part-000* | sort -k 1 > output/frequent_stripes.txt\n",
    "!wc -l output/frequent_stripes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dist_calc_hw54.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dist_calc_hw54.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.compat import get_jobconf_value\n",
    "import re\n",
    "import sys\n",
    "import ast\n",
    "import urllib2\n",
    "import math\n",
    "\n",
    "\n",
    "class DistanceCalc(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper,\n",
    "                  jobconf={\n",
    "                            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'mapred.text.key.comparator.options': '-k1,1n',\n",
    "                            }\n",
    "                   )\n",
    "        ]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        # Load the file into memory\n",
    "        self.counter = 0\n",
    "        self.stripes = {}\n",
    "        '''\n",
    "        f = urllib2.urlopen(\"https://s3-us-west-2.amazonaws.com/ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/frequent_stripes.txt\")\n",
    "        for line in f.readlines():\n",
    "            tokens = line.strip().split('\\t')\n",
    "            self.stripes[tokens[0].replace(\"\\\"\",\"\")] = ast.literal_eval(tokens[1])\n",
    "            self.increment_counter('distance', 'num_stripes_loaded', amount=1)\n",
    "        '''\n",
    "        with open('frequent_stripes.txt','r') as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split('\\t')\n",
    "                self.stripes[tokens[0].replace(\"\\\"\",\"\")] = ast.literal_eval(tokens[1])\n",
    "                self.increment_counter('distance', 'num_stripes_loaded', amount=1)\n",
    "      \n",
    "        sys.stderr.write('### of stripes: {0}\\n'.format(len(self.stripes)))\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        dist_type = get_jobconf_value('dist_type')\n",
    "        tokens = line.strip().split('\\t')\n",
    "        key = tokens[0].replace(\"\\\"\",\"\")\n",
    "        dict_pairs = ast.literal_eval(tokens[1])\n",
    "        s1 = set(dict_pairs.keys())\n",
    "        for n_key, n_dict_pairs in self.stripes.iteritems():\n",
    "            # TODO distance calc for only (a,b) but not (b,a) --> Redundant\n",
    "            if key > n_key:\n",
    "                continue\n",
    "            \n",
    "            self.counter += 1   \n",
    "            if self.counter % 1000 == 0:\n",
    "                self.set_status('# of Distances Calculated: {0}'.format(self.counter))\n",
    "                \n",
    "            s2 = set(n_dict_pairs.keys())\n",
    "            distance = None\n",
    "            \n",
    "            if dist_type == 'euclid':\n",
    "\n",
    "                # Calculate Euclidean Distance\n",
    "                # Get the union of keys from both stripes\n",
    "                union_keys = s1.union(s2)\n",
    "\n",
    "                squared_distance = 0\n",
    "                for k in union_keys:\n",
    "                    squared_distance += (dict_pairs.get(k, 0) - n_dict_pairs.get(k, 0)) ** 2\n",
    "                    \n",
    "                distance = math.sqrt(squared_distance)\n",
    "                \n",
    "            if dist_type == 'cosine':\n",
    "           \n",
    "                # Calculate Cosine Distance\n",
    "                # Get the intersection of keys from both stripes\n",
    "                intersection_keys = s1.intersection(s2)\n",
    "\n",
    "                dot_x_y = 0\n",
    "                for k in intersection_keys:\n",
    "                    dot_x_y += dict_pairs[k] * n_dict_pairs[k]\n",
    "\n",
    "                norm_x = 0\n",
    "                for k in s1:\n",
    "                    norm_x += dict_pairs[k] * dict_pairs[k]\n",
    "\n",
    "                norm_y = 0\n",
    "                for k in s2:\n",
    "                    norm_y += n_dict_pairs[k] * n_dict_pairs[k]\n",
    "\n",
    "                distance = float(dot_x_y) / (math.sqrt(norm_x) * math.sqrt(norm_y))\n",
    "                    \n",
    "          \n",
    "            self.increment_counter('distance', 'num_{0}_distances'.format(dist_type), amount=1)\n",
    "            yield (distance), (key, n_key)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    DistanceCalc.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x dist_calc_hw54.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Calculation\n",
    "\n",
    "***Distance has been calculated in local m/c using a smaller dataset. Tried multiple times in EMR, but everytime the job failed with errors that were not very helpful in debuggin the problem. Most likely the jobs failed due to Out of Memory Errors. Based on my online research, looks like there are better ways to parallelize the distance calculations (Cosine, Jaccard, Euclidean, Manhattan etc), but was unable to explore those due to time constraints.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /Users/ssatpati/.mrjob.conf\n",
      "creating tmp directory /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.042953.238385\n",
      "writing wrapper script to /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.042953.238385/setup-wrapper.sh\n",
      "writing to /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.042953.238385/step-0-mapper_part-00000\n",
      "> sh -ex setup-wrapper.sh /Users/ssatpati/anaconda/bin/python dist_calc_hw54.py --step-num=0 --mapper --no-strict-protocols /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.042953.238385/input_part-00000 > /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.042953.238385/step-0-mapper_part-00000\n",
      "STDERR: + __mrjob_PWD=/private/var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.042953.238385/job_local_dir/0/mapper/0\n",
      "STDERR: + exec\n",
      "STDERR: + /Users/ssatpati/anaconda/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "STDERR: + export PYTHONPATH=/private/var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.042953.238385/job_local_dir/0/mapper/0/mrjob.tar.gz:\n",
      "STDERR: + PYTHONPATH=/private/var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.042953.238385/job_local_dir/0/mapper/0/mrjob.tar.gz:\n",
      "STDERR: + exec\n",
      "STDERR: + cd /private/var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.042953.238385/job_local_dir/0/mapper/0\n",
      "STDERR: + /Users/ssatpati/anaconda/bin/python dist_calc_hw54.py --step-num=0 --mapper --no-strict-protocols /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.042953.238385/input_part-00000\n",
      "STDERR: ### of stripes: 10000\n",
      "STDERR: No handlers could be found for logger \"mrjob.compat\"\n",
      "status: # of Distances Calculated: 1000\n",
      "status: # of Distances Calculated: 2000\n",
      "status: # of Distances Calculated: 3000\n",
      "status: # of Distances Calculated: 4000\n",
      "Counters from step 1:\n",
      "  distance:\n",
      "    num_euclid_distances: 4043\n",
      "    num_stripes_loaded: 10000\n",
      "Moving /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.042953.238385/step-0-mapper_part-00000 -> /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.042953.238385/output/part-00000\n",
      "Streaming final output from /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.042953.238385/output\n",
      "removing tmp directory /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.042953.238385\n",
      "\n",
      "real\t1m59.408s\n",
      "user\t1m50.587s\n",
      "sys\t0m8.591s\n"
     ]
    }
   ],
   "source": [
    "!rm output/euclid_dist.txt\n",
    "!time python dist_calc_hw54.py -r local \\\n",
    "--file output/frequent_stripes.txt \\\n",
    "--jobconf 'dist_type=euclid' \\\n",
    "--no-strict-protocol \\\n",
    "output/test_stripes.txt > output/euclid_dist.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162651.75911437295\t[\"life\", \"writings\"]\r\n",
      "170813.8704028452\t[\"life\", \"stain\"]\r\n",
      "166962.7704968985\t[\"life\", \"lord\"]\r\n",
      "169735.15824071335\t[\"life\", \"sinking\"]\r\n",
      "167200.72430166084\t[\"life\", \"regional\"]\r\n",
      "167569.63904001226\t[\"life\", \"yellow\"]\r\n",
      "169539.30474966564\t[\"life\", \"uncertain\"]\r\n",
      "169395.3866668157\t[\"life\", \"prize\"]\r\n",
      "169005.3579387352\t[\"life\", \"wooden\"]\r\n",
      "170999.37683512183\t[\"life\", \"persisted\"]\r\n"
     ]
    }
   ],
   "source": [
    "!head -10 output/euclid_dist.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /Users/ssatpati/.mrjob.conf\n",
      "creating tmp directory /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.043153.014211\n",
      "writing wrapper script to /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.043153.014211/setup-wrapper.sh\n",
      "writing to /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.043153.014211/step-0-mapper_part-00000\n",
      "> sh -ex setup-wrapper.sh /Users/ssatpati/anaconda/bin/python dist_calc_hw54.py --step-num=0 --mapper --no-strict-protocols /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.043153.014211/input_part-00000 > /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.043153.014211/step-0-mapper_part-00000\n",
      "STDERR: + __mrjob_PWD=/private/var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.043153.014211/job_local_dir/0/mapper/0\n",
      "STDERR: + exec\n",
      "STDERR: + /Users/ssatpati/anaconda/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "STDERR: + export PYTHONPATH=/private/var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.043153.014211/job_local_dir/0/mapper/0/mrjob.tar.gz:\n",
      "STDERR: + PYTHONPATH=/private/var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.043153.014211/job_local_dir/0/mapper/0/mrjob.tar.gz:\n",
      "STDERR: + exec\n",
      "STDERR: + cd /private/var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.043153.014211/job_local_dir/0/mapper/0\n",
      "STDERR: + /Users/ssatpati/anaconda/bin/python dist_calc_hw54.py --step-num=0 --mapper --no-strict-protocols /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.043153.014211/input_part-00000\n",
      "STDERR: ### of stripes: 10000\n",
      "STDERR: No handlers could be found for logger \"mrjob.compat\"\n",
      "status: # of Distances Calculated: 1000\n",
      "status: # of Distances Calculated: 2000\n",
      "status: # of Distances Calculated: 3000\n",
      "status: # of Distances Calculated: 4000\n",
      "Counters from step 1:\n",
      "  distance:\n",
      "    num_cosine_distances: 4043\n",
      "    num_stripes_loaded: 10000\n",
      "Moving /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.043153.014211/step-0-mapper_part-00000 -> /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.043153.014211/output/part-00000\n",
      "Streaming final output from /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.043153.014211/output\n",
      "removing tmp directory /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/dist_calc_hw54.ssatpati.20151006.043153.014211\n",
      "\n",
      "real\t1m49.110s\n",
      "user\t1m40.577s\n",
      "sys\t0m8.381s\n"
     ]
    }
   ],
   "source": [
    "!rm output/cosine_dist.txt\n",
    "!time python dist_calc_hw54.py -r local \\\n",
    "--file output/frequent_stripes.txt \\\n",
    "--jobconf 'dist_type=cosine' \\\n",
    "--no-strict-protocol \\\n",
    "output/test_stripes.txt > output/cosine_dist.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9421374042513717\t[\"life\", \"writings\"]\r\n",
      "0.8313871899681543\t[\"life\", \"stain\"]\r\n",
      "0.91983668656369\t[\"life\", \"lord\"]\r\n",
      "0.8690261634238905\t[\"life\", \"sinking\"]\r\n",
      "0.914862119190327\t[\"life\", \"regional\"]\r\n",
      "0.8862305158599959\t[\"life\", \"yellow\"]\r\n",
      "0.8287440348705982\t[\"life\", \"uncertain\"]\r\n",
      "0.8154294013145477\t[\"life\", \"prize\"]\r\n",
      "0.7597525475632223\t[\"life\", \"wooden\"]\r\n",
      "0.6218562101579173\t[\"life\", \"persisted\"]\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 output/cosine_dist.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.5\n",
    "---\n",
    "\n",
    "In this part of the assignment you will evaluate the success of you synonym detector.\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined\n",
    "by your measure in (2), and use the synonyms function in the accompanying\n",
    "python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0636379482385911\t[\"life\", \"sur\"]\r\n",
      "0.07529117883951839\t[\"life\", \"los\"]\r\n",
      "0.10126889679347573\t[\"life\", \"que\"]\r\n",
      "0.13826129238715115\t[\"life\", \"und\"]\r\n",
      "0.2207153823724399\t[\"life\", \"wilt\"]\r\n",
      "0.22714412269531603\t[\"life\", \"singled\"]\r\n",
      "0.2581920664977755\t[\"life\", \"subdivided\"]\r\n",
      "0.26553826954428433\t[\"life\", \"wasn\"]\r\n",
      "0.29670000814305053\t[\"life\", \"shalt\"]\r\n",
      "0.31510580174581476\t[\"life\", \"refrain\"]\r\n"
     ]
    }
   ],
   "source": [
    "!cat output/cosine_dist.txt | sort -k 1 > output/cosine_dist_sorted.txt\n",
    "!head -n 1000 output/cosine_dist_sorted.txt > output/cosine_dist_top_1000.txt\n",
    "!head -n 10 output/cosine_dist_top_1000.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfortunately, none of top 1000 similar words from my list matched the synonyms from NLTK wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "living\n",
      "life\n",
      "sprightliness\n",
      "lifespan\n",
      "spirit\n",
      "liveliness\n",
      "animation\n",
      "life_story\n",
      "life_sentence\n",
      "lifetime\n",
      "aliveness\n",
      "life-time\n",
      "biography\n",
      "life_history\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "import ast\n",
    "#print all the synset element of an element\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()\n",
    "\n",
    "# Find out synonyms for life\n",
    "syn = synonyms(\"life\")\n",
    "for s in syn:\n",
    "    print s\n",
    "    \n",
    "# Check if any of the top 1000 matches the synonym list\n",
    "with open('output/cosine_dist_top_1000.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.strip().split('\\t')\n",
    "        pair = ast.literal_eval(t[1])\n",
    "        if pair[1] in syn:\n",
    "            print 'Synonym Matched: {0}'.format(pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
