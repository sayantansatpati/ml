{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sayantan Satpati**\n",
    "* **sayantan.satpati@ischool.berkeley.edu**\n",
    "* **W261**\n",
    "* **Week-5**\n",
    "* **Assignment-5**\n",
    "* **Date of Submission: 13-OCT-2015**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Since merging notebooks turned out to be a difficult and messy task, we are submitting 3 different notebooks each containing only the parts contributed by a single person towards HW5. However, 1 consolidated PDF would be submitted***\n",
    "\n",
    "***This notebook contains solutions contributed by Sayantan***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  === Week 5: mrjob, aws, and n-grams ===\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.3\n",
    "---\n",
    "\n",
    "For the remainder of this assignment you will work with a large subset \n",
    "of the Google n-grams dataset,\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket on s3:\n",
    "\n",
    "s3://filtered-5grams/\n",
    "\n",
    "In particular, this bucket contains (~200) files in the format:\n",
    "\n",
    "(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (count), i.e., unigrams\n",
    "- Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency (Hint: save to PART-000* and take the head -n 1000)\n",
    "- Distribution of 5-gram sizes (counts) sorted in decreasing order of relative frequency. (Hint: save to PART-000* and take the head -n 1000)\n",
    "OPTIONAL Question:\n",
    "- Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see:\n",
    "https://en.wikipedia.org/wiki/Log%E2%80%93log_plot\n",
    "https://en.wikipedia.org/wiki/Power_law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longest 5-gram (number of characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A)\n",
    "---\n",
    "\n",
    "***Refer to Chris's notebook***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_hw53_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_hw53_2.py\n",
    "#!/usr/bin/python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "from itertools import combinations\n",
    "\n",
    "class FrequentUnigrams(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer),\n",
    "             MRStep(mapper=self.mapper_frequent_unigrams,\n",
    "                   reducer=self.reducer_frequent_unigrams,\n",
    "                   jobconf={\n",
    "                            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'mapred.text.key.comparator.options': '-k1,1rn',\n",
    "                            }\n",
    "                   )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line.strip()\n",
    "        tokens = re.split(\"\\t\",line)\n",
    "        unigrams = tokens[0].split()\n",
    "        count = int(tokens[1])\n",
    "        for unigram in unigrams:\n",
    "            yield unigram, count\n",
    "    \n",
    "    def combiner(self, unigram, counts):\n",
    "        yield unigram, sum(counts)\n",
    "    \n",
    "    def reducer(self, unigram, counts):\n",
    "        yield unigram, sum(counts)\n",
    "        \n",
    "    def mapper_frequent_unigrams(self, unigram, count):\n",
    "        yield count, unigram\n",
    "        \n",
    "    def reducer_frequent_unigrams(self, count, unigrams):\n",
    "        for unigram in unigrams:\n",
    "            yield count, unigram\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    FrequentUnigrams.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mrjob_hw53_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /Users/ssatpati/.mrjob.conf\n",
      "using existing scratch bucket mrjob-d5ed1dc31babbe2c\n",
      "using s3://mrjob-d5ed1dc31babbe2c/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/mrjob_hw53_2.ssatpati.20151003.062810.595234\n",
      "writing master bootstrap script to /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/mrjob_hw53_2.ssatpati.20151003.062810.595234/b.py\n",
      "Copying non-input files into s3://mrjob-d5ed1dc31babbe2c/tmp/mrjob_hw53_2.ssatpati.20151003.062810.595234/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-QGLC3PJPOLBE\n",
      "Created new job flow j-QGLC3PJPOLBE\n",
      "Job launched 30.4s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 61.3s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 91.8s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 122.8s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 153.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 184.0s ago, status STARTING: Configuring cluster software\n",
      "Job launched 214.4s ago, status STARTING: Configuring cluster software\n",
      "Job launched 245.4s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 276.0s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 306.6s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 337.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 368.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 398.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 429.5s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 460.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 491.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 521.5s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 552.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 582.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 613.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 643.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 674.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 705.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 735.9s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 766.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 796.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 827.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 858.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 888.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 919.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 949.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 980.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1010.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1041.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1072.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1102.6s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1133.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1163.5s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1193.9s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1224.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1255.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1285.5s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1316.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1346.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1377.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1408.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1438.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1469.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1500.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1530.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1561.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1591.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1622.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1652.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1683.6s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1714.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1745.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1775.5s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1806.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1836.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1867.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1898.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1928.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1959.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 1989.9s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2020.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2051.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2081.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2112.6s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2143.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2173.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2204.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2234.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2265.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2296.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2326.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2357.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2387.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2418.6s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2449.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2479.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2510.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2541.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2571.5s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2602.9s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2633.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2663.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2694.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2725.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2755.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2785.9s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2816.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2847.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2877.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2908.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2939.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 2970.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3000.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3031.4s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3061.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3092.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3123.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3153.6s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3184.0s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3214.8s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3245.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3275.7s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 1 of 2)\n",
      "Job launched 3306.1s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 2 of 2)\n",
      "Job launched 3336.9s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 2 of 2)\n",
      "Job launched 3367.3s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 2 of 2)\n",
      "Job launched 3398.2s ago, status RUNNING: Running step (mrjob_hw53_2.ssatpati.20151003.062810.595234: Step 2 of 2)\n",
      "Job completed.\n",
      "Running time was 3103.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 2156069116\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 5251252\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 292640853\n",
      "    FILE_BYTES_WRITTEN: 323462370\n",
      "    HDFS_BYTES_READ: 23640\n",
      "    HDFS_BYTES_WRITTEN: 5251252\n",
      "    S3_BYTES_READ: 2156069116\n",
      "  Job Counters :\n",
      "    Launched map tasks: 193\n",
      "    Launched reduce tasks: 8\n",
      "    Rack-local map tasks: 191\n",
      "    SLOTS_MILLIS_MAPS: 22513524\n",
      "    SLOTS_MILLIS_REDUCES: 10666442\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 7828220\n",
      "    Combine input records: 307583853\n",
      "    Combine output records: 22094566\n",
      "    Map input bytes: 2156069116\n",
      "    Map input records: 58682266\n",
      "    Map output bytes: 3136729760\n",
      "    Map output materialized bytes: 98214630\n",
      "    Map output records: 293411330\n",
      "    Physical memory (bytes) snapshot: 79829819392\n",
      "    Reduce input groups: 343019\n",
      "    Reduce input records: 7922043\n",
      "    Reduce output records: 343019\n",
      "    Reduce shuffle bytes: 98214630\n",
      "    SPLIT_RAW_BYTES: 23640\n",
      "    Spilled Records: 30016609\n",
      "    Total committed heap usage (bytes): 86414835712\n",
      "    Virtual memory (bytes) snapshot: 188305924096\n",
      "Counters from step 2:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 6161441\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 5251252\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 4010303\n",
      "    FILE_BYTES_WRITTEN: 9264521\n",
      "    HDFS_BYTES_READ: 6164738\n",
      "    S3_BYTES_WRITTEN: 5251252\n",
      "  Job Counters :\n",
      "    Data-local map tasks: 21\n",
      "    Launched map tasks: 21\n",
      "    Launched reduce tasks: 8\n",
      "    SLOTS_MILLIS_MAPS: 239490\n",
      "    SLOTS_MILLIS_REDUCES: 174447\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 46410\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 5251252\n",
      "    Map input records: 343019\n",
      "    Map output bytes: 5251252\n",
      "    Map output materialized bytes: 4500335\n",
      "    Map output records: 343019\n",
      "    Physical memory (bytes) snapshot: 7540547584\n",
      "    Reduce input groups: 41362\n",
      "    Reduce input records: 343019\n",
      "    Reduce output records: 343019\n",
      "    Reduce shuffle bytes: 4500335\n",
      "    SPLIT_RAW_BYTES: 3297\n",
      "    Spilled Records: 686038\n",
      "    Total committed heap usage (bytes): 7062859776\n",
      "    Virtual memory (bytes) snapshot: 26687610880\n",
      "removing tmp directory /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/mrjob_hw53_2.ssatpati.20151003.062810.595234\n",
      "Removing all files in s3://mrjob-d5ed1dc31babbe2c/tmp/mrjob_hw53_2.ssatpati.20151003.062810.595234/\n",
      "Removing all files in s3://mrjob-d5ed1dc31babbe2c/tmp/logs/j-QGLC3PJPOLBE/\n",
      "Terminating job flow: j-QGLC3PJPOLBE\n"
     ]
    }
   ],
   "source": [
    "!python mrjob_hw53_2.py -r emr \\\n",
    " s3://filtered-5grams \\\n",
    " --output-dir=s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words \\\n",
    " --no-output \\\n",
    " --no-strict-protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/_SUCCESS to ./_SUCCESS\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00003 to ./part-00003\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00000 to ./part-00000\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00002 to ./part-00002\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00004 to ./part-00004\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00006 to ./part-00006\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00001 to ./part-00001\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00005 to ./part-00005\n",
      "5375699242\t\"the\"\n",
      "3691308874\t\"of\"\n",
      "2221164346\t\"to\"\n",
      "1387638591\t\"in\"\n",
      "1342195425\t\"a\"\n",
      "1135779433\t\"and\"\n",
      "798553959\t\"that\"\n",
      "756296656\t\"is\"\n",
      "688053106\t\"be\"\n",
      "481373389\t\"as\"\n",
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output;mkdir output;cd output;aws s3 cp --recursive s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words .;\n",
    "!cat output/part-0000* | sort -nrk 1 | head -n 10\n",
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Frequent Words are:\n",
    "\n",
    "```\n",
    "5375699242\t\"the\"\n",
    "3691308874\t\"of\"\n",
    "2221164346\t\"to\"\n",
    "1387638591\t\"in\"\n",
    "1342195425\t\"a\"\n",
    "1135779433\t\"and\"\n",
    "798553959\t\"that\"\n",
    "756296656\t\"is\"\n",
    "688053106\t\"be\"\n",
    "481373389\t\"as\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_hw53_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_hw53_3.py\n",
    "#!/usr/bin/python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "from itertools import combinations\n",
    "\n",
    "class DenseWords(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer,\n",
    "                   ),\n",
    "            MRStep(mapper=self.mapper_max_min,\n",
    "                   reducer=self.reducer_max_min,\n",
    "                   jobconf={\n",
    "                            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'mapred.text.key.comparator.options': '-k1,1rn',\n",
    "                            }\n",
    "                   )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        tokens = line.strip().split('\\t')\n",
    "        unigrams = tokens[0].split()\n",
    "        density = round((int(tokens[1]) * 1.0 / int(tokens[2])), 3)\n",
    "        for unigram in unigrams:\n",
    "            yield unigram, density\n",
    "            \n",
    "    def combiner(self, unigram, densities):\n",
    "        densities = [d for d in densities]\n",
    "        yield unigram, min(densities) \n",
    "        yield unigram, max(densities)\n",
    "        \n",
    "    def reducer(self, unigram, densities):\n",
    "        densities = [d for d in densities]\n",
    "        yield unigram, min(densities)\n",
    "        yield unigram, max(densities)\n",
    "        \n",
    "    def mapper_max_min(self, unigram, density):\n",
    "        yield density, unigram\n",
    "        \n",
    "    def reducer_max_min(self, density, unigrams):\n",
    "        for unigram in unigrams:\n",
    "            yield density, unigram\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    DenseWords.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mrjob_hw53_3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/density/_SUCCESS\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00001\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00000\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00002\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00004\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00003\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00005\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00006\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rm --recursive s3://ucb-mids-mls-sayantan-satpati/hw53/density/\n",
    "!python mrjob_hw53_3.py -q -r emr \\\n",
    " s3://filtered-5grams \\\n",
    " --output-dir=s3://ucb-mids-mls-sayantan-satpati/hw53/density \\\n",
    " --no-output \\\n",
    " --no-strict-protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/_SUCCESS to ./_SUCCESS\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00003 to ./part-00003\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00004 to ./part-00004\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00005 to ./part-00005\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00006 to ./part-00006\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00000 to ./part-00000\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00001 to ./part-00001\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00007 to ./part-00007\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/density/part-00002 to ./part-00002\n",
      "29.704999999999998\t\"Death\"\n",
      "14.912000000000001\t\"write\"\n",
      "14.912000000000001\t\"and\"\n",
      "12.380000000000001\t\"NA\"\n",
      "11.557\t\"xxxx\"\n",
      "11.557\t\"xxxx\"\n",
      "10.882\t\"Sc\"\n",
      "10.6\t\"beep\"\n",
      "10.022\t\"blah\"\n",
      "9.8040000000000003\t\"whole\"\n",
      "1.0\t\"AAAS\"\n",
      "1.0\t\"AAAI\"\n",
      "1.0\t\"AAAE\"\n",
      "1.0\t\"AAAE\"\n",
      "1.0\t\"AAAA\"\n",
      "1.0\t\"AAAA\"\n",
      "1.0\t\"AAA\"\n",
      "1.0\t\"AA\"\n",
      "1.0\t\"A's\"\n",
      "1.0\t\"A\"\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output;mkdir output;cd output;aws s3 cp --recursive s3://ucb-mids-mls-sayantan-satpati/hw53/density/ .;\n",
    "!cat output/part-0000* | sort -nrk 1 > merged\n",
    "!head -n 10 merged\n",
    "!tail -n 10 merged\n",
    "!rm -rf ./output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Most Dense Words\n",
    "\n",
    "```\n",
    "29.704999999999998\t\"Death\"\n",
    "14.912000000000001\t\"write\"\n",
    "14.912000000000001\t\"and\"\n",
    "12.380000000000001\t\"NA\"\n",
    "11.557\t\"xxxx\"\n",
    "11.557\t\"xxxx\"\n",
    "10.882\t\"Sc\"\n",
    "10.6\t\"beep\"\n",
    "10.022\t\"blah\"\n",
    "9.8040000000000003\t\"whole\"\n",
    "```\n",
    "\n",
    "### Top 10 Least Dense Words\n",
    "\n",
    "```\n",
    "1.0\t\"AAAS\"\n",
    "1.0\t\"AAAI\"\n",
    "1.0\t\"AAAE\"\n",
    "1.0\t\"AAAE\"\n",
    "1.0\t\"AAAA\"\n",
    "1.0\t\"AAAA\"\n",
    "1.0\t\"AAA\"\n",
    "1.0\t\"AA\"\n",
    "1.0\t\"A's\"\n",
    "1.0\t\"A\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (D)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_hw53_4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_hw53_4.py\n",
    "#!/usr/bin/python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "from itertools import combinations\n",
    "\n",
    "class DistributionNgram(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   reducer=self.reducer,\n",
    "                   jobconf={\n",
    "                            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'mapred.text.key.comparator.options': '-k1,1rn',\n",
    "                            }\n",
    "                   )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line.strip()\n",
    "        tokens = re.split(\"\\t\",line)\n",
    "        yield int(tokens[1]), tokens[0]\n",
    "    \n",
    "    def reducer(self, count, ngrams):\n",
    "        for ngram in ngrams:\n",
    "            yield count, ngram\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    DistributionNgram.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mrjob_hw53_4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/_SUCCESS\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00001\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00006\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00000\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00003\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00002\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00004\n",
      "delete: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00005\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rm --recursive s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/\n",
    "!python mrjob_hw53_4.py -q -r emr \\\n",
    " s3://filtered-5grams/ \\\n",
    " --output-dir=s3://ucb-mids-mls-sayantan-satpati/hw53/distribution \\\n",
    " --no-output \\\n",
    " --no-strict-protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/_SUCCESS to ./_SUCCESS\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00000 to ./part-00000\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00002 to ./part-00002\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00001 to ./part-00001\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00004 to ./part-00004\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00003 to ./part-00003\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00005 to ./part-00005\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/part-00006 to ./part-00006\n",
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n",
      "cat: stdout: Broken pipe\n",
      "2223343\t\"on the basis of the\"\n",
      "1462063\t\"at the head of the\"\n",
      "1316960\t\"as well as in the\"\n",
      "871158\t\"at the same time the\"\n",
      "833764\t\"as a matter of fact\"\n",
      "590885\t\"as a part of the\"\n",
      "544783\t\"as one of the most\"\n",
      "533464\t\"and at the end of\"\n",
      "447364\t\"I do not think that\"\n",
      "441568\t\"from the beginning of the\"\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output;mkdir output;cd output;aws s3 cp --recursive s3://ucb-mids-mls-sayantan-satpati/hw53/distribution/ .;\n",
    "!cat output/part-0000* | sort -nrk 1 > output_hw53_4.txt\n",
    "!rm -rf ./output\n",
    "!head -n 10 output_hw53_4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 NGram Distribution\n",
    "\n",
    "```\n",
    "2223343\t\"on the basis of the\"\n",
    "1462063\t\"at the head of the\"\n",
    "1316960\t\"as well as in the\"\n",
    "871158\t\"at the same time the\"\n",
    "833764\t\"as a matter of fact\"\n",
    "590885\t\"as a part of the\"\n",
    "544783\t\"as one of the most\"\n",
    "533464\t\"and at the end of\"\n",
    "447364\t\"I do not think that\"\n",
    "441568\t\"from the beginning of the\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.4\n",
    "---\n",
    "\n",
    "In this part of the assignment we will focus on developing methods\n",
    "for detecting synonyms, using the Google 5-grams dataset. To accomplish\n",
    "this you must script two main tasks using MRJob:\n",
    "\n",
    "(1) Build stripes of word co-ocurrence for the top 10,000\n",
    "most frequently appearing words across the entire set of 5-grams,\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "(2) Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "==Design notes for (1)==\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2).\n",
    "\n",
    "==Design notes for (2)==\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Cosine similarity\n",
    "- Kendall correlation\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 5.4: Part 1 > Step 1:\n",
    "\n",
    "***Find the frequent unigrams for the top 10,000 most frequently appearing words across the entire set of 5-grams***\n",
    "\n",
    "* Use the freqency count output from earlier step to get the top 10K most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/_SUCCESS to ./_SUCCESS\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00006 to ./part-00006\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00002 to ./part-00002\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00000 to ./part-00000\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00005 to ./part-00005\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00003 to ./part-00003\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00004 to ./part-00004\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words/part-00001 to ./part-00001\n",
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n",
      "5375699242\t\"the\"\n",
      "3691308874\t\"of\"\n",
      "2221164346\t\"to\"\n",
      "1387638591\t\"in\"\n",
      "1342195425\t\"a\"\n",
      "1135779433\t\"and\"\n",
      "798553959\t\"that\"\n",
      "756296656\t\"is\"\n",
      "688053106\t\"be\"\n",
      "481373389\t\"as\"\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output;mkdir output;cd output;aws s3 cp --recursive s3://ucb-mids-mls-sayantan-satpati/hw53/frequent_words .;\n",
    "!cat output/part-0000* | sort -nrk 1 | head -n 10000 > output/frequent_unigrams.txt\n",
    "!head -n 10 output/frequent_unigrams.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 5.4: Part 1 > Step 2:\n",
    "\n",
    "***Build stripes of word co-ocurrence for the top 10,000\n",
    "most frequently appearing words across the entire set of 5-grams***\n",
    "\n",
    "* Pass the 1 item frequent set generated from earlier step to the mapper(s) for filtering\n",
    "* The reducers would sum and emit Word Co-Occurrence if support count of 10,000 is met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting word_cooccurrences_set_hw54.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile word_cooccurrences_set_hw54.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "class WordCoOccurrenceFrequentSet(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer)\n",
    "        ]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        # Load the file into memory\n",
    "        self.unigrams = {}\n",
    "        with open('frequent_unigrams.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split('\\t')\n",
    "                self.unigrams[tokens[1].replace(\"\\\"\",\"\")] = int(tokens[0])\n",
    "        sys.stderr.write('### of unigrams: {0}'.format(self.unigrams))\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        tokens = line.strip().split('\\t')\n",
    "        # List of 5-grams\n",
    "        words = tokens[0].split()\n",
    "        # Filter 5-grams to only those in list\n",
    "        words = [w for w in words if w in self.unigrams.keys()]\n",
    "        l = len(words)\n",
    "        for i in xrange(l):\n",
    "            d = {}\n",
    "            for j in xrange(l):\n",
    "                if i != j:\n",
    "                    d[words[j]] = d.get(words[j], 0) + 1\n",
    "            # Emit word, stripe\n",
    "            yield words[i],d\n",
    "\n",
    "    def combiner(self, word, stripes):\n",
    "        d = {}\n",
    "        # Aggregate stripes\n",
    "        for s in stripes:\n",
    "            for k, v in s.iteritems():\n",
    "                d[k] = d.get(k, 0) + v\n",
    "        yield word,d\n",
    "        \n",
    "    def reducer(self, word, stripes):\n",
    "        d = {}\n",
    "        # Aggregate stripes\n",
    "        for s in stripes:\n",
    "            for k, v in s.iteritems():\n",
    "                d[k] = d.get(k, 0) + v\n",
    "        '''\n",
    "        # Filter based on support count (Not a requirement!)\n",
    "        d_final = {}\n",
    "        for k,v in d.iteritems():\n",
    "            if v >= 10000:\n",
    "                d_final[k] = v\n",
    "        '''\n",
    "        # Combine stripes\n",
    "        yield word,d\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    WordCoOccurrenceFrequentSet.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x word_cooccurrences_set_hw54.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!aws s3 rm --recursive s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/\n",
    "!python word_cooccurrences_set_hw54.py -q -r emr \\\n",
    " s3://filtered-5grams/ \\\n",
    " --file output/frequent_unigrams.txt \\\n",
    " --output-dir=s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur \\\n",
    " --no-output \\\n",
    " --no-strict-protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 5.4 Part 2 > Step 1\n",
    "\n",
    "***From the output of Part 1 > Step 2, create a single file with all the stripes - frequent itemsets of size 2***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/_SUCCESS to ./_SUCCESS\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00000 to ./part-00000\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00004 to ./part-00004\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00002 to ./part-00002\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00001 to ./part-00001\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00006 to ./part-00006\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00005 to ./part-00005\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00007 to ./part-00007\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00008 to ./part-00008\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00009 to ./part-00009\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00010 to ./part-00010\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00011 to ./part-00011\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00012 to ./part-00012\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00013 to ./part-00013\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00014 to ./part-00014\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00015 to ./part-00015\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00016 to ./part-00016\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00017 to ./part-00017\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00018 to ./part-00018\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00019 to ./part-00019\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00003 to ./part-00003\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00020 to ./part-00020\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00021 to ./part-00021\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00024 to ./part-00024\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00022 to ./part-00022\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00025 to ./part-00025\n",
      "download: s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/part-00023 to ./part-00023\n",
      "   10000 output/frequent_stripes.txt\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./output;mkdir output;cd output;aws s3 cp --recursive s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/ .;\n",
    "!cat output/part-000* | sort -k 1 > output/frequent_stripes.txt\n",
    "!wc -l output/frequent_stripes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Take on Distance Calculation using Mapper Side Hash-Join (Brute Force)\n",
    "---\n",
    "\n",
    "***Approach***\n",
    "\n",
    "* Implement a Hash Join using a Mapper only Job. The frequent 10K word pairs (stripes) are fed to each mapper as a distributed cache.\n",
    "* **With 11 m1.large instances, the job completed in 15 hrs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dist_calc_hw54.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dist_calc_hw54.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.compat import get_jobconf_value\n",
    "import re\n",
    "import sys\n",
    "import ast\n",
    "import urllib2\n",
    "import math\n",
    "\n",
    "\n",
    "class DistanceCalc(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper)\n",
    "        ]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        # Load the file into memory\n",
    "        self.counter = 0\n",
    "        self.stripes = {}\n",
    "        '''\n",
    "        f = urllib2.urlopen(\"https://s3-us-west-2.amazonaws.com/ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/frequent_stripes.txt\")\n",
    "        for line in f.readlines():\n",
    "            tokens = line.strip().split('\\t')\n",
    "            self.stripes[tokens[0].replace(\"\\\"\",\"\")] = ast.literal_eval(tokens[1])\n",
    "            self.increment_counter('distance', 'num_stripes_loaded', amount=1)\n",
    "        '''\n",
    "        # Load File\n",
    "        with open('frequent_stripes.txt','r') as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split('\\t')\n",
    "                self.stripes[tokens[0].replace(\"\\\"\",\"\")] = ast.literal_eval(tokens[1])\n",
    "                self.increment_counter('distance', 'num_stripes_loaded', amount=1)\n",
    "        \n",
    "        sys.stderr.write('### of stripes Loaded: {0}\\n'.format(len(self.stripes)))\n",
    "        \n",
    "        ''' (Not Required By Cosine)\n",
    "        #Pre-Process File (Add missing keys to make it a 10K by 10K matrix)\n",
    "        s1 = set(self.stripes.keys()) # Full Set\n",
    "        for key,value_dict in self.stripes.iteritems():\n",
    "            s2 = set(value_dict.keys())\n",
    "            missing_keys = s1.difference(s2)\n",
    "            for mk in missing_keys:\n",
    "                self.stripes[key][mk] = 0\n",
    "        sys.stderr.write('### of stripes Pre-Processed: {0}\\n'.format(len(self.stripes)))\n",
    "        '''\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        dist_type = get_jobconf_value('dist_type')\n",
    "        tokens = line.strip().split('\\t')\n",
    "        \n",
    "        key = tokens[0].replace(\"\\\"\",\"\")\n",
    "        dict_pairs = ast.literal_eval(tokens[1])\n",
    "        \n",
    "        for n_key, n_dict_pairs in self.stripes.iteritems():\n",
    "            # TODO distance calc for only (a,b) but not (b,a) --> Redundant\n",
    "            if key > n_key:\n",
    "                continue\n",
    "            \n",
    "            self.counter += 1   \n",
    "            if self.counter % 1000 == 0:\n",
    "                self.set_status('# of Distances Calculated: {0}'.format(self.counter))\n",
    "                \n",
    "            distance = None\n",
    "            \n",
    "            if dist_type == 'euclid':\n",
    "\n",
    "                # Calculate Euclidean Distance\n",
    "                squared_distance = 0\n",
    "                for k in n_dict_pairs.keys():\n",
    "                    squared_distance += (dict_pairs.get(k, 0) - n_dict_pairs.get(k, 0)) ** 2\n",
    "                    \n",
    "                distance = math.sqrt(squared_distance)\n",
    "                \n",
    "            if dist_type == 'cosine':\n",
    "                \n",
    "                # Calculate Cosine Distance\n",
    "                # Get the intersection of keys from both stripes\n",
    "                norm_x = 0\n",
    "                norm_y = 0\n",
    "                dot_x_y = 0\n",
    "                for k in self.stripes.keys(): # Iterate through entire key range once\n",
    "                    norm_x += dict_pairs.get(k,0) * dict_pairs.get(k,0)\n",
    "                    norm_y += n_dict_pairs.get(k,0) * n_dict_pairs.get(k,0)\n",
    "                    dot_x_y += dict_pairs.get(k,0) * n_dict_pairs.get(k,0)\n",
    "                    \n",
    "                distance = float(dot_x_y) / (math.sqrt(norm_x) * math.sqrt(norm_y))\n",
    "          \n",
    "            self.increment_counter('distance', 'num_{0}_distances'.format(dist_type), amount=1)\n",
    "            yield (distance), (key, n_key)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    DistanceCalc.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x dist_calc_hw54.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!aws s3 rm --recursive s3://ucb-mids-mls-sayantan-satpati/hw54/distance/output/\n",
    "!python dist_calc_hw54.py -q -r emr \\\n",
    " --file s3://ucb-mids-mls-sayantan-satpati/hw54/distance/input/frequent_stripes.txt \\\n",
    " --jobconf 'dist_type=cosine' \\\n",
    " s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/ \\\n",
    " --output-dir=s3://ucb-mids-mls-sayantan-satpati/hw54/distance/output/ \\\n",
    " --bootstrap-action=\"s3://elasticmapreduce/bootstrap-actions/configure-hadoop \\\n",
    "        -m mapred.tasktracker.map.tasks.maximum=2 \\\n",
    "        -m mapreduce.map.memory.mb=3000\" \\\n",
    " --no-strict-protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Top 1K Word Pairs with minimum distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top_1k_syn_hw54.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top_1k_syn_hw54.py\n",
    "#!/usr/bin/python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "from itertools import combinations\n",
    "\n",
    "class Top1KSynonyms(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   reducer=self.reducer,\n",
    "                   jobconf={\n",
    "                            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'mapred.text.key.comparator.options': '-k1,1rn',\n",
    "                            'mapred.reduce.tasks': 1\n",
    "                            }\n",
    "                   )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        tokens = line.strip().split('\\t')\n",
    "        [w1,w2] = ast.literal_eval(tokens[1])\n",
    "        if w1 != w2:\n",
    "            yield float(tokens[0]),tokens[1]\n",
    "    \n",
    "    def reducer(self, key, values):\n",
    "        for value in values:\n",
    "            yield key, value\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    Top1KSynonyms.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x top_1k_syn_hw54.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!aws s3 rm --recursive s3://ucb-mids-mls-sayantan-satpati/hw54/distance/synonyms/\n",
    "!python top_1k_syn_hw54.py -q -r emr \\\n",
    " s3://ucb-mids-mls-sayantan-satpati/hw54/distance/output/ \\\n",
    " --output-dir=s3://ucb-mids-mls-sayantan-satpati/hw54/distance/synonyms/ \\\n",
    " --no-strict-protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf ./output1;mkdir output1;cd output1;aws s3 cp --recursive s3://ucb-mids-mls-sayantan-satpati/hw54/distance/synonyms/ .;\n",
    "!head -n 1000 output1/part-00000 > output1/top1k.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1000 output1/top1k.txt\n",
      "0.9989871786920097\t\"[\\\"center\\\", \\\"centre\\\"]\"\n",
      "0.9983273473595391\t\"[\\\"aegis\\\", \\\"auspices\\\"]\"\n",
      "0.99819561795478362\t\"[\\\"seventeenth\\\", \\\"sixteenth\\\"]\"\n",
      "0.99808891300127756\t\"[\\\"eighteenth\\\", \\\"nineteenth\\\"]\"\n",
      "0.99803557390404385\t\"[\\\"Germans\\\", \\\"Russians\\\"]\"\n",
      "0.99793556998358346\t\"[\\\"ascribed\\\", \\\"attributed\\\"]\"\n",
      "0.99793255052813279\t\"[\\\"authenticity\\\", \\\"validity\\\"]\"\n",
      "0.99782320499493526\t\"[\\\"favor\\\", \\\"favour\\\"]\"\n",
      "0.99776231214994249\t\"[\\\"concerning\\\", \\\"respecting\\\"]\"\n",
      "0.99775180932774554\t\"[\\\"Danube\\\", \\\"Rhine\\\"]\"\n"
     ]
    }
   ],
   "source": [
    "!wc -l output1/top1k.txt\n",
    "!head -n 10 output1/top1k.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2nd Take on Distance Calculation\n",
    "---\n",
    "\n",
    "***Approach taken from the paper: https://terpconnect.umd.edu/~oard/pdf/acl08elsayed2.pdf***\n",
    "\n",
    "**3 step map/reduce job:**\n",
    "\n",
    "**Step:1**\n",
    "\n",
    "* Create an inverted index with postings of all terms (Term as key, and Stripes/Docs are Values)\n",
    "\n",
    "* Mapper: Emits (key, stripe/doc & count) after normalizing & filtering for stopwords\n",
    "* Reducer: Creates the postings for each key\n",
    "\n",
    "**Step:2**\n",
    "\n",
    "* Calculate Cosine Distance\n",
    "\n",
    "* Mapper: Emits docId pair as key, product of normalized counts\n",
    "* Combiner: Sums Up normalized counts to reduce IO between mappers and reducers\n",
    "* Reducer: Calculates the cosine distance\n",
    "\n",
    "**Step:3**\n",
    "\n",
    "* Sort and take top 1000 words which are similar\n",
    "\n",
    "#### The job is running in AWS for the last 1 Day & 9 hrs. Cluster size: 15 m1.large slaves\n",
    "\n",
    "**As of writing this notebook, the job was done till this point**\n",
    "```\n",
    "2015-10-13 00:11:53,485 INFO org.apache.hadoop.streaming.StreamJob (main):  map 100%  reduce 16%\n",
    "2015-10-13 00:40:20,736 INFO org.apache.hadoop.streaming.StreamJob (main):  map 100%  reduce 17%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords list using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dist_calc_inverted_index_hw54.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dist_calc_inverted_index_hw54.py\n",
    "#!/usr/bin/python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re, sys\n",
    "import ast\n",
    "import math\n",
    "from mrjob.protocol import RawValueProtocol,JSONProtocol\n",
    "from itertools import combinations\n",
    "\n",
    "class DistanceCalcInvertedIndex(MRJob):\n",
    "    \n",
    "    STOP_WORDS = [u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now']\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   reducer=self.reducer\n",
    "                  ),\n",
    "            MRStep(mapper=self.mapper_dist,\n",
    "                   reducer=self.reducer_dist\n",
    "                  ),\n",
    "            MRStep(mapper_init=self.mapper_top1k_init,\n",
    "                   mapper=self.mapper_top1k,\n",
    "                   mapper_final=self.mapper_top1k_final,\n",
    "                   reducer_init=self.reducer_top1k_init,\n",
    "                   reducer=self.reducer_top1k,\n",
    "                   reducer_final=self.reducer_top1k_final,\n",
    "                   jobconf={\n",
    "                            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                            'mapred.text.key.comparator.options': '-k1,1rn',\n",
    "                            'mapred.reduce.tasks': 1\n",
    "                            }\n",
    "                  )\n",
    "        ]\n",
    "        \n",
    "\n",
    "    # Step:1\n",
    "    def mapper(self, _, line):\n",
    "        tokens = line.strip().split('\\t')\n",
    "        \n",
    "        docId = tokens[0].replace(\"\\\"\",\"\")\n",
    "        stripe = ast.literal_eval(tokens[1])\n",
    "        \n",
    "        # Normalize & Round to 3\n",
    "        if docId.lower() not in self.STOP_WORDS:\n",
    "            total_cnt = sum(stripe.values())\n",
    "            for word,cnt in stripe.iteritems():\n",
    "                if word.lower() not in self.STOP_WORDS:\n",
    "                    yield word, (docId, round(float(cnt)/total_cnt,5))\n",
    "    \n",
    "    def reducer(self, word, docId_count):\n",
    "        docId_counts = [i for i in docId_count]\n",
    "        yield word, docId_counts\n",
    "        \n",
    "    # Step: 2\n",
    "        \n",
    "    def mapper_dist(self, key, docId_counts):\n",
    "        #sys.stderr.write('{0} # {1}\\n'.format(key, docId_counts))\n",
    "        docId_counts = sorted(docId_counts)\n",
    "        l = len(docId_counts)\n",
    "        if l < 7000:\n",
    "            for i in xrange(l):\n",
    "                for j in xrange(l):\n",
    "                    if j > i:\n",
    "                        x = docId_counts[i][1]\n",
    "                        y = docId_counts[j][1]\n",
    "                        yield (docId_counts[i][0], docId_counts[j][0]), round((docId_counts[i][1] * docId_counts[j][1]),5)\n",
    "    \n",
    "    \n",
    "    def combiner_dist(self, docId_pair, values):\n",
    "        yield docId_pair, round(sum(values),5)\n",
    "        \n",
    "        \n",
    "    def reducer_dist(self, docId_pair, values):\n",
    "        yield docId_pair, round(sum(values),5)\n",
    "        \n",
    "        \n",
    "    # Step: 3\n",
    "    \n",
    "    def mapper_top1k_init(self):\n",
    "        self.TOP_N = 1000\n",
    "        self.top_1k_pairs = []\n",
    "\n",
    "    def mapper_top1k(self, docId_pair, distance):\n",
    "        self.top_1k_pairs.append((distance, docId_pair))\n",
    "        if len(self.top_1k_pairs) > self.TOP_N:\n",
    "            self.top_1k_pairs.sort(key=lambda x: -x[0])\n",
    "            self.top_1k_pairs = self.top_1k_pairs[:self.TOP_N]\n",
    "            \n",
    "    def mapper_top1k_final(self):\n",
    "        sys.stderr.write('##### [Mapper_Final]: {0}\\n'.format(len(self.top_1k_pairs)))\n",
    "        for e in self.top_1k_pairs:\n",
    "            yield e[0], e[1]\n",
    "            \n",
    "    def reducer_top1k_init(self):\n",
    "        self.TOP_N = 1000\n",
    "        self.top_1k_pairs = []\n",
    "            \n",
    "    def reducer_top1k(self, distance, docId_pairs):\n",
    "        for docId_pair in docId_pairs:\n",
    "            self.top_1k_pairs.append((distance, docId_pair))\n",
    "        if len(self.top_1k_pairs) > self.TOP_N:\n",
    "            self.top_1k_pairs.sort(key=lambda x: -x[0])\n",
    "            self.top_1k_pairs = self.top_1k_pairs[:self.TOP_N]\n",
    "        \n",
    "    def reducer_top1k_final(self):\n",
    "        sys.stderr.write('##### [Reducer_Final]: {0}\\n'.format(len(self.top_1k_pairs)))\n",
    "        for e in self.top_1k_pairs:\n",
    "            yield e[0], e[1]\n",
    "        \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    DistanceCalcInvertedIndex.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x dist_calc_inverted_index_hw54.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!aws s3 rm --recursive s3://ucb-mids-mls-sayantan-satpati/hw54/distance/output1/\n",
    "!python dist_calc_inverted_index_hw54.py -q -r emr \\\n",
    " s3://ucb-mids-mls-sayantan-satpati/hw54/word_cooccur/ \\\n",
    " --output-dir=s3://ucb-mids-mls-sayantan-satpati/hw54/distance/output1/ \\\n",
    " --no-strict-protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"dist_calc_inverted_index_hw54.py\", line 122, in <module>\r\n",
      "    DistanceCalcInvertedIndex.run()\r\n",
      "  File \"/Users/ssatpati/anaconda/lib/python2.7/site-packages/mrjob/job.py\", line 461, in run\r\n",
      "    mr_job.execute()\r\n",
      "  File \"/Users/ssatpati/anaconda/lib/python2.7/site-packages/mrjob/job.py\", line 479, in execute\r\n",
      "    super(MRJob, self).execute()\r\n",
      "  File \"/Users/ssatpati/anaconda/lib/python2.7/site-packages/mrjob/launch.py\", line 153, in execute\r\n",
      "    self.run_job()\r\n",
      "  File \"/Users/ssatpati/anaconda/lib/python2.7/site-packages/mrjob/launch.py\", line 216, in run_job\r\n",
      "    runner.run()\r\n",
      "  File \"/Users/ssatpati/anaconda/lib/python2.7/site-packages/mrjob/runner.py\", line 470, in run\r\n",
      "    self._run()\r\n",
      "  File \"/Users/ssatpati/anaconda/lib/python2.7/site-packages/mrjob/emr.py\", line 882, in _run\r\n",
      "    self._wait_for_job_to_complete()\r\n",
      "  File \"/Users/ssatpati/anaconda/lib/python2.7/site-packages/mrjob/emr.py\", line 1767, in _wait_for_job_to_complete\r\n",
      "    raise Exception(msg)\r\n",
      "Exception: Job on job flow j-24306CFK4EVT failed with status TERMINATED_WITH_ERRORS: The requested number of instances exceeds your EC2 quota\r\n"
     ]
    }
   ],
   "source": [
    "''' For Running under Sayantan's personal A/C\n",
    "!aws s3 rm --recursive s3://w261/distance/output/\n",
    "!python dist_calc_inverted_index_hw54.py -q -r emr \\\n",
    " s3://w261/word_cooccur/ \\\n",
    " --output-dir=s3://w261/distance/output/ \\\n",
    " --no-strict-protocol\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.5\n",
    "---\n",
    "\n",
    "In this part of the assignment you will evaluate the success of you synonym detector.\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined\n",
    "by your measure in (2), and use the synonyms function in the accompanying\n",
    "python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall, and F1 Score\n",
    "___\n",
    "\n",
    "*** The following exercise has been done using the distance output from the brute force implementation (Cosine Distance) ***\n",
    "\n",
    "#### PRECISION: 0.055\n",
    "\n",
    "#### RECALL: 0.057\n",
    "\n",
    "#### F1 Score: 0.056"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Synonym Dict: 7315\n",
      "MATCH: ['center', 'centre']\n",
      "MATCH: ['aegis', 'auspices']\n",
      "MATCH: ['favor', 'favour']\n",
      "MATCH: ['Earl', 'earl']\n",
      "MATCH: ['color', 'colour']\n",
      "MATCH: ['neighbourhood', 'vicinity']\n",
      "MATCH: ['Fig', 'Figure']\n",
      "MATCH: ['irrespective', 'regardless']\n",
      "MATCH: ['fulfillment', 'fulfilment']\n",
      "MATCH: ['honor', 'honour']\n",
      "MATCH: ['authenticity', 'legitimacy']\n",
      "MATCH: ['analyse', 'analyze']\n",
      "MATCH: ['Emperor', 'emperor']\n",
      "MATCH: ['assess', 'evaluate']\n",
      "MATCH: ['vigor', 'vigour']\n",
      "MATCH: ['colors', 'colours']\n",
      "MATCH: ['calculate', 'compute']\n",
      "MATCH: ['calculation', 'computation']\n",
      "MATCH: ['Gospel', 'gospel']\n",
      "MATCH: ['establishment', 'formation']\n",
      "MATCH: ['castle', 'palace']\n",
      "MATCH: ['recognise', 'recognize']\n",
      "MATCH: ['embodiment', 'incarnation']\n",
      "MATCH: ['labor', 'labour']\n",
      "MATCH: ['brink', 'verge']\n",
      "MATCH: ['chiefly', 'principally']\n",
      "MATCH: ['chief', 'principal']\n",
      "MATCH: ['harbor', 'harbour']\n",
      "MATCH: ['theater', 'theatre']\n",
      "MATCH: ['neighborhood', 'neighbourhood']\n",
      "MATCH: ['Duke', 'duke']\n",
      "MATCH: ['advancement', 'promotion']\n",
      "MATCH: ['Origin', 'Origins']\n",
      "MATCH: ['recognised', 'recognized']\n",
      "MATCH: ['adhere', 'cling']\n",
      "MATCH: ['disintegration', 'dissolution']\n",
      "MATCH: ['realise', 'realize']\n",
      "MATCH: ['realisation', 'realization']\n",
      "MATCH: ['elimination', 'evacuation']\n",
      "MATCH: ['defence', 'defense']\n",
      "MATCH: ['fulfil', 'fulfill']\n",
      "MATCH: ['Chairman', 'chairman']\n",
      "MATCH: ['gray', 'grey']\n",
      "MATCH: ['railroad', 'railway']\n",
      "MATCH: ['initiation', 'installation']\n",
      "MATCH: ['accomplishment', 'attainment']\n",
      "MATCH: ['Governor', 'governor']\n",
      "MATCH: ['deity', 'god']\n",
      "MATCH: ['magnificent', 'splendid']\n",
      "MATCH: ['Parliament', 'parliament']\n",
      "MATCH: ['Universe', 'universe']\n",
      "MATCH: ['founder', 'founders']\n",
      "MATCH: ['Presidency', 'presidency']\n",
      "MATCH: ['organisation', 'organization']\n",
      "MATCH: ['origin', 'origins']\n",
      "\n",
      "Total Count: 1000, TP: 55, FP: 945, FN: 913\n",
      "\n",
      "### PRECISION: 0.055\n",
      "\n",
      "### RECALL: 0.057\n",
      "\n",
      "### F1 Score: 0.056\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "import ast\n",
    "#print all the synset element of an element\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()\n",
    "\n",
    "cnt_t = 0\n",
    "cnt_m = 0\n",
    "cnt_fn = 0\n",
    "\n",
    "# Load synomyn file\n",
    "dict_syn = {}\n",
    "with open('synonyms.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.strip().split(',')\n",
    "        w1 = t[0].lower()\n",
    "        w2 = t[1].lower()\n",
    "        if w1 in dict_syn.keys():\n",
    "            dict_syn[w1].append(w2)\n",
    "        else:\n",
    "            dict_syn[w1] = [w2]\n",
    "        if w2 in dict_syn.keys():\n",
    "            dict_syn[w2].append(w1)\n",
    "        else:\n",
    "            dict_syn[w2] = [w1]\n",
    "\n",
    "print \"Length of Synonym Dict: {0}\".format(len(dict_syn))\n",
    "            \n",
    "# Check if any of the top 1000 matches the synonym list\n",
    "with open('output1/top1k.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        cnt_t += 1\n",
    "        t = line.strip().split('\\t')\n",
    "        t[1] = t[1].replace(\"\\\\\",\"\")[1:-1]\n",
    "        pair = ast.literal_eval(t[1])\n",
    "        syn0 = synonyms(pair[0].lower())\n",
    "        syn1 = synonyms(pair[1].lower())\n",
    "\n",
    "        # Precision\n",
    "        if pair[1].lower() in syn0 or pair[0].lower() in syn1:\n",
    "            print \"MATCH: {0}\".format(pair)\n",
    "            cnt_m += 1\n",
    "        \n",
    "        # Recall\n",
    "        if pair[0].lower() in dict_syn.keys() or pair[1].lower() in dict_syn.keys():\n",
    "            cnt_fn += 1\n",
    "        \n",
    "            \n",
    "print \"\\nTotal Count: {0}, TP: {1}, FP: {2}, FN: {3}\".format(cnt_t, cnt_m, cnt_t - cnt_m, cnt_fn)\n",
    "\n",
    "p = round(float(cnt_m) / cnt_t, 3)\n",
    "r = round(float(cnt_m) / (cnt_m + cnt_fn), 3)\n",
    "f1 = round(2 * p * r / (p + r), 3)\n",
    "\n",
    "print \"\\n### PRECISION: {0}\".format(p)\n",
    "print \"\\n### RECALL: {0}\".format(r)\n",
    "print \"\\n### F1 Score: {0}\".format(f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
