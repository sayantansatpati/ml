{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write some words to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!echo foo foo quux labs foo bar quux > WordCount.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MrJob class for wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing WordCount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRJobStep\n",
    "import re\n",
    " \n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    " \n",
    "class MRWordFreqCount(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFreqCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Run the code in command line locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/WordCount.ssatpati.20151001.154753.264949\n",
      "writing to /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/WordCount.ssatpati.20151001.154753.264949/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/WordCount.ssatpati.20151001.154753.264949/step-0-mapper-sorted\n",
      "> sort /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/WordCount.ssatpati.20151001.154753.264949/step-0-mapper_part-00000\n",
      "writing to /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/WordCount.ssatpati.20151001.154753.264949/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/WordCount.ssatpati.20151001.154753.264949/step-0-reducer_part-00000 -> /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/WordCount.ssatpati.20151001.154753.264949/output/part-00000\n",
      "Streaming final output from /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/WordCount.ssatpati.20151001.154753.264949/output\n",
      "\"bar\"\t1\n",
      "\"foo\"\t3\n",
      "\"labs\"\t1\n",
      "\"quux\"\t2\n",
      "removing tmp directory /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/WordCount.ssatpati.20151001.154753.264949\n"
     ]
    }
   ],
   "source": [
    "!python WordCount.py WordCount.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is straightforward. Mapper outputs (word, 1) key value pairs, and then conbiner combines the sum locally. At last, Reducer sums them up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the code through python driver locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Reminder: You cannot use the programmatic runner functionality in the same file as your job class. That is because the file with the job class is sent to Hadoop to be run. Therefore, the job file cannot attempt to start the Hadoop job, or you would be recursively creating Hadoop jobs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use make_runner() to run an MRJob\n",
    "1. seperate driver from mapreduce jobs\n",
    "2. now we can run it within python notebook \n",
    "3. In python, typically one class is in each file. Each mrjob job is a seperate class, should be in a seperate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bar', 1)\n",
      "('foo', 3)\n",
      "('labs', 1)\n",
      "('quux', 2)\n"
     ]
    }
   ],
   "source": [
    "from WordCount import MRWordFreqCount\n",
    "mr_job = MRWordFreqCount(args=['WordCount.txt'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Put your access key info in configuration  file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Check you configration file path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the config file path is empty, you need to set environment varible MRJOB_CONF to be '~/.mrjob.conf'\n",
    "\n",
    "Please have a look of the find_mrjob_conf function in the following link to understand how mrjob find the config file\n",
    "http://pydoc.net/Python/mrjob/0.4.0/mrjob.conf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ssatpati/.mrjob.conf'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mrjob import conf\n",
    "conf.find_mrjob_conf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Create or replace .mrjob.conf file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "runners:\n",
    "    emr:\n",
    "        aws_access_key_id: your_access_key_id\n",
    "        aws_secret_access_key: your_secret_access_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the code in command line in AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /Users/ssatpati/.mrjob.conf\n",
      "using existing scratch bucket mrjob-d5ed1dc31babbe2c\n",
      "using s3://mrjob-d5ed1dc31babbe2c/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/WordCount.ssatpati.20151003.032533.193129\n",
      "writing master bootstrap script to /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/WordCount.ssatpati.20151003.032533.193129/b.py\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "Copying non-input files into s3://mrjob-d5ed1dc31babbe2c/tmp/WordCount.ssatpati.20151003.032533.193129/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-20024AKKNNBRA\n",
      "Created new job flow j-20024AKKNNBRA\n",
      "Job launched 30.4s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 61.9s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 92.3s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 123.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 153.5s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 184.5s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 215.0s ago, status STARTING: Configuring cluster software\n",
      "Job launched 246.0s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 276.3s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 306.8s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 337.2s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 368.4s ago, status RUNNING: Running step (WordCount.ssatpati.20151003.032533.193129: Step 1 of 1)\n",
      "Job launched 398.8s ago, status RUNNING: Running step (WordCount.ssatpati.20151003.032533.193129: Step 1 of 1)\n",
      "Job launched 429.5s ago, status RUNNING: Running step (WordCount.ssatpati.20151003.032533.193129: Step 1 of 1)\n",
      "Job launched 459.9s ago, status RUNNING: Running step (WordCount.ssatpati.20151003.032533.193129: Step 1 of 1)\n",
      "Job completed.\n",
      "Running time was 111.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 89\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 34\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 61\n",
      "    FILE_BYTES_WRITTEN: 161137\n",
      "    HDFS_BYTES_READ: 730\n",
      "    S3_BYTES_READ: 89\n",
      "    S3_BYTES_WRITTEN: 34\n",
      "  Job Counters :\n",
      "    Launched map tasks: 5\n",
      "    Launched reduce tasks: 1\n",
      "    Rack-local map tasks: 5\n",
      "    SLOTS_MILLIS_MAPS: 82101\n",
      "    SLOTS_MILLIS_REDUCES: 35802\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 5020\n",
      "    Combine input records: 7\n",
      "    Combine output records: 4\n",
      "    Map input bytes: 31\n",
      "    Map input records: 1\n",
      "    Map output bytes: 59\n",
      "    Map output materialized bytes: 121\n",
      "    Map output records: 7\n",
      "    Physical memory (bytes) snapshot: 1075793920\n",
      "    Reduce input groups: 4\n",
      "    Reduce input records: 4\n",
      "    Reduce output records: 4\n",
      "    Reduce shuffle bytes: 121\n",
      "    SPLIT_RAW_BYTES: 730\n",
      "    Spilled Records: 8\n",
      "    Total committed heap usage (bytes): 812883968\n",
      "    Virtual memory (bytes) snapshot: 4106801152\n",
      "Streaming final output from s3://mrjob-d5ed1dc31babbe2c/tmp/WordCount.ssatpati.20151003.032533.193129/output/\n",
      "\"bar\"\t1\n",
      "\"foo\"\t3\n",
      "\"labs\"\t1\n",
      "\"quux\"\t2\n",
      "removing tmp directory /var/folders/h5/1q71m1c54cn07f16c232pqgm38ynd8/T/WordCount.ssatpati.20151003.032533.193129\n",
      "Removing all files in s3://mrjob-d5ed1dc31babbe2c/tmp/WordCount.ssatpati.20151003.032533.193129/\n",
      "Removing all files in s3://mrjob-d5ed1dc31babbe2c/tmp/logs/j-20024AKKNNBRA/\n",
      "Terminating job flow: j-20024AKKNNBRA\n"
     ]
    }
   ],
   "source": [
    "!python WordCount.py WordCount.txt -r emr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the code through driver in AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bar', 1)\n",
      "('foo', 3)\n",
      "('labs', 1)\n",
      "('quux', 2)\n"
     ]
    }
   ],
   "source": [
    "from WordCount import MRWordFreqCount\n",
    "mr_job = MRWordFreqCount(args=['WordCount.txt','-r', 'emr'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
