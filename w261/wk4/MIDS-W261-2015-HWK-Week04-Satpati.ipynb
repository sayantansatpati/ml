{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sayantan Satpati**\n",
    "* **sayantan.satpati@ischool.berkeley.edu**\n",
    "* **W261**\n",
    "* **Week-4**\n",
    "* **Assignment-4**\n",
    "* **Date of Submission: 29-SEP-2015**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  === Week 4: mrjob & k-means ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.0\n",
    "---\n",
    "\n",
    "***What is MrJob? How is it different to Hadoop MapReduce?***\n",
    "\n",
    "***What are the mapper_final(), combiner_final(), reducer_final() methods? When are they called?***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.1\n",
    "---\n",
    "\n",
    "***What is serialization in the context of MrJob or Hadoop?***\n",
    "\n",
    "***When it used in these frameworks?***\n",
    "\n",
    "***What is the default serialization mode for input and outputs for MrJob?***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.2\n",
    "---\n",
    "\n",
    "Recall the Microsoft logfiles data from the async lecture. The logfiles are described are located at:\n",
    "\n",
    "https://kdd.ics.uci.edu/databases/msweb/msweb.html\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/\n",
    "\n",
    "This dataset records which areas (Vroots) of www.microsoft.com each user visited in a one-week timeframe in Feburary 1998.\n",
    "\n",
    " Here, you must preprocess the data on a single node (i.e., not on a cluster of nodes) from the format:\n",
    "\n",
    "C,\"10001\",10001   #Visitor id 10001\n",
    "V,1000,1          #Visit by Visitor 10001 to page id 1000\n",
    "V,1001,1          #Visit by Visitor 10001 to page id 1001\n",
    "V,1002,1          #Visit by Visitor 10001 to page id 1002\n",
    "C,\"10002\",10002   #Visitor id 10001\n",
    "V\n",
    "Note: #denotes comments\n",
    "to the format:\n",
    "\n",
    "V,1000,1,C, 10001\n",
    "V,1001,1,C, 10001\n",
    "V,1002,1,C, 10001\n",
    "\n",
    "Write the python code to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess_hw42.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess_hw42.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if len(sys.argv) < 2:\n",
    "    print \"No input file is passed, Aborting!!!\"\n",
    "    sys.exit(1)\n",
    "\n",
    "input_file = sys.argv[1]\n",
    "output_file = input_file + '.pp'\n",
    "\n",
    "try:\n",
    "    os.remove(output_file)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "last_visitor = None\n",
    "with open(input_file, 'r') as f1:\n",
    "    with open(output_file, 'a') as f2:\n",
    "        for line in f1:\n",
    "            line = line.strip()\n",
    "            tokens = line.split(\",\")\n",
    "            if len(tokens) == 3 and tokens[0] == 'C':\n",
    "                last_visitor = tokens[2]\n",
    "\n",
    "            if len(tokens) == 3 and tokens[0] == 'V':\n",
    "                out_line = '{0},C,{1}\\n'.format(line,last_visitor)\n",
    "                f2.write(out_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x preprocess_hw42.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V,1000,1,C,10001\r\n",
      "V,1001,1,C,10001\r\n",
      "V,1002,1,C,10001\r\n",
      "V,1001,1,C,10002\r\n",
      "V,1003,1,C,10002\r\n",
      "V,1001,1,C,10003\r\n",
      "V,1003,1,C,10003\r\n",
      "V,1004,1,C,10003\r\n",
      "V,1005,1,C,10004\r\n",
      "V,1006,1,C,10005\r\n"
     ]
    }
   ],
   "source": [
    "!python preprocess_hw42.py anonymous-msweb.data\n",
    "!head -n 10 anonymous-msweb.data.pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW 4.3\n",
    "---\n",
    "\n",
    "**Find the 5 most frequently visited pages using mrjob from the output of 4.2 (i.e., transfromed log file).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_hw43.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_hw43.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "\n",
    "class MRVistedPagesCount(MRJob):\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer),\n",
    "            MRStep(reducer=self.reducer_find_top_5)\n",
    "        ]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        tokens = line.strip().split(\",\")\n",
    "        yield tokens[1], 1\n",
    "\n",
    "    def combiner(self, page_visted, counts):\n",
    "        yield page_visted, sum(counts)\n",
    "\n",
    "    def reducer(self, page_visted, counts):\n",
    "        yield None, (sum(counts), page_visted)\n",
    "\n",
    "    # discard the key; it is just None\n",
    "    def reducer_find_top_5(self, _, page_visted_pairs):\n",
    "        # Store all the keys into memory (Assumption: Can be loaded into memory)\n",
    "        pairs = []\n",
    "        for p in page_visted_pairs:\n",
    "            pairs.append(p)\n",
    "        pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "            \n",
    "        for p in pairs[:5]:\n",
    "            yield p[1],p[0]\n",
    "            \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRVistedPagesCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mrjob_hw43.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1008\"\t10836\r\n",
      "\"1034\"\t9383\r\n",
      "\"1004\"\t8463\r\n",
      "\"1018\"\t5330\r\n",
      "\"1017\"\t5108\r\n"
     ]
    }
   ],
   "source": [
    "# Running mrjob using command line\n",
    "!python mrjob_hw43.py -r local anonymous-msweb.data.pp -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/0\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/0/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/0/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/0\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python mrjob_hw43.py --step-num=0 --mapper /tmp/mrjob_hw43.cloudera.20150926.200443.882049/input_part-00000\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/0\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/0/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/0/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/0\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python mrjob_hw43.py --step-num=0 --combiner\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/1\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/1/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/1/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/1\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python mrjob_hw43.py --step-num=0 --mapper /tmp/mrjob_hw43.cloudera.20150926.200443.882049/input_part-00001\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/1\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/1/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/1/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/1\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python mrjob_hw43.py --step-num=0 --combiner\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/reducer/0\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/reducer/0/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/reducer/0/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/reducer/0\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python mrjob_hw43.py --step-num=0 --reducer /tmp/mrjob_hw43.cloudera.20150926.200443.882049/input_part-00000\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/reducer/1\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/reducer/1/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/reducer/1/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/reducer/1\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python mrjob_hw43.py --step-num=0 --reducer /tmp/mrjob_hw43.cloudera.20150926.200443.882049/input_part-00001\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/1/reducer/0\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/1/reducer/0/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/1/reducer/0/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/1/reducer/0\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python mrjob_hw43.py --step-num=1 --reducer /tmp/mrjob_hw43.cloudera.20150926.200443.882049/input_part-00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1008', 10836)\n",
      "('1034', 9383)\n",
      "('1004', 8463)\n",
      "('1018', 5330)\n",
      "('1017', 5108)\n"
     ]
    }
   ],
   "source": [
    "# Running mrjob using a driver\n",
    "from mrjob_hw43 import MRVistedPagesCount\n",
    "mr_job = MRVistedPagesCount(args=['-r', 'local', 'anonymous-msweb.data.pp', 'q'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW 4.4 \n",
    "---\n",
    "\n",
    "**Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "1. Create a separate file with URLS, i.e with records that start with A. This will be passed to the mrjob as an additional file for joining datasets\n",
    "2. Following happens in the first pass of mrjob:\n",
    "    * Mapper: Emits (pageId,visitorId), 1\n",
    "    * Combiner: Combines the counts\n",
    "    * Reducer: Combines the counts and emits records like (p1,v1) 100 | (p1, v2), 101 | (p1,v3), 202\n",
    "    * Partitioner is used to that all keys with the same pageId goes to the same reducer\n",
    "3. Following happens in the second pass of mrjob:\n",
    "    * Mapper: Emits (pageId,visit_count), visitorId\n",
    "    * Reducer_Init: Loads the url file into a dict\n",
    "    * Reducer: Emits (pageId, pageURL),(vists, vistorId)\n",
    "    * Partitioner is used to that all keys with the same pageId goes to the same reducer\n",
    "    * Secondary sorting (Descending Order) is done on the visit_count. This ensures that pages with highest page visits come first, before other records which has a lower page visit for the same page.\n",
    "   \n",
    "**Final Output Format(output_hw44.txt): (pageId, pageURL),(vists, vistorId)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed `url'\r\n"
     ]
    }
   ],
   "source": [
    "# Create a file with only URL(s), i.e. records starting with 'A'\n",
    "!rm -v url\n",
    "!grep ^A anonymous-msweb.data > url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_hw44.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_hw44.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "\n",
    "class MRFrequentVisitor(MRJob):\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer),\n",
    "             MRStep(mapper=self.mapper_frequent_visitor,\n",
    "                   reducer_init=self.reducer_frequent_visitor_init,\n",
    "                   reducer=self.reducer_frequent_visitor)\n",
    "        ]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        tokens = line.strip().split(\",\")\n",
    "        key = \"{0},{1}\".format(tokens[1],tokens[4])\n",
    "        yield key, 1\n",
    "\n",
    "    def combiner(self, key, counts):\n",
    "        yield key, sum(counts)\n",
    "\n",
    "    def reducer(self, key, counts):\n",
    "        yield key, sum(counts)\n",
    "        \n",
    "    # 2nd Pass\n",
    "    \n",
    "    def mapper_frequent_visitor(self, key, value):\n",
    "        tokens = key.strip().split(\",\")\n",
    "        modified_key = \"{0},{1}\".format(tokens[0],value)\n",
    "        yield modified_key, tokens[1]\n",
    "     \n",
    "    \n",
    "    def reducer_frequent_visitor_init(self):\n",
    "        # Reads the 'url' file into a Dict for displaying additional information\n",
    "        self.last_page = None\n",
    "        self.pageDict = {}\n",
    "        with open('url','r') as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split(\",\")\n",
    "                self.pageDict[tokens[1]] = tokens[4]\n",
    "                \n",
    "    def reducer_frequent_visitor(self, key, values):\n",
    "        tokens = key.strip().split(\",\")\n",
    "        page = tokens[0]\n",
    "        visits = int(tokens[1])\n",
    "        \n",
    "        if self.last_page != page:\n",
    "            self.last_page = page\n",
    "            # values might be a list, if there is a tie for same key => (p1, 1000), [v1,v2,v3..]\n",
    "            for value in values:\n",
    "                k = '{0},{1}'.format(page, \n",
    "                                    self.pageDict.get(page, 'NA').replace(\"\\\"\",\"\"))\n",
    "                v = '{0},{1}'.format(visits,\n",
    "                                    value)\n",
    "                yield k,v\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRFrequentVisitor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mrjob_hw44.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.0.0:\n",
      "The have been translated as follows\n",
      " mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "map.output.key.field.separator: mapreduce.map.output.key.field.separator\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "WARNING:mrjob.compat:Detected hadoop configuration property names that do not match hadoop version 2.0.0:\n",
      "The have been translated as follows\n",
      " mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "map.output.key.field.separator: mapreduce.map.output.key.field.separator\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n"
     ]
    }
   ],
   "source": [
    "# Running mrjob using a Hadoop Runner in local cluster\n",
    "from mrjob_hw44 import MRFrequentVisitor\n",
    "import os\n",
    "\n",
    "mr_job = MRFrequentVisitor(args=['-r', 'hadoop', \n",
    "                                 '--hadoop-home', '/usr/lib/hadoop-0.20-mapreduce',\n",
    "                                 '--hadoop-bin', '/usr/bin/hadoop',\n",
    "                                 '--file', 'url',\n",
    "                                 '--jobconf', 'stream.num.map.output.key.fields=2',\n",
    "                                 '--jobconf', 'map.output.key.field.separator=,',\n",
    "                                 '--jobconf', 'mapred.text.key.partitioner.options=-k1,1',\n",
    "                                 '--jobconf', 'mapred.text.key.comparator.options=-k1,1 -k2,2nr',\n",
    "                                 '--jobconf', 'mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                                 '--partitioner', 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner',\n",
    "                                 'anonymous-msweb.data.pp', '-v'])\n",
    "\n",
    "\n",
    "output_file = \"output_hw44.txt\"\n",
    "try:\n",
    "    os.remove(output_file)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "with mr_job.make_runner() as runner, open(output_file, 'a') as f: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        #print mr_job.parse_output_line(line)\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98654 output_hw44.txt\n",
      "\"1000,/regwiz\"\t\"1,42411\"\n",
      "\"1000,/regwiz\"\t\"1,42381\"\n",
      "\"1000,/regwiz\"\t\"1,42320\"\n",
      "\"1000,/regwiz\"\t\"1,42291\"\n",
      "\"1000,/regwiz\"\t\"1,42285\"\n",
      "\"1000,/regwiz\"\t\"1,42260\"\n",
      "\"1000,/regwiz\"\t\"1,42213\"\n",
      "\"1000,/regwiz\"\t\"1,42198\"\n",
      "\"1000,/regwiz\"\t\"1,42176\"\n",
      "\"1000,/regwiz\"\t\"1,42160\"\n"
     ]
    }
   ],
   "source": [
    "# Output (Max page visit for all pages is 1; All ties have been reported)\n",
    "!wc -l output_hw44.txt \n",
    "!head -n 10 output_hw44.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW 4.5 \n",
    "---\n",
    "\n",
    "Here you will use a different dataset consisting of word-frequency distributions \n",
    "for 1,000 Twitter users. These Twitter users use language in very different ways,\n",
    "and were classified by hand according to the criteria:\n",
    "\n",
    "0: Human, where only basic human-human communication is observed.\n",
    "\n",
    "1: Cyborg, where language is primarily borrowed from other sources\n",
    "(e.g., jobs listings, classifieds postings, advertisements, etc...).\n",
    "\n",
    "2: Robot, where language is formulaically derived from unrelated sources\n",
    "(e.g., weather/seismology, police/fire event logs, etc...).\n",
    "\n",
    "3: Spammer, where language is replicated to high multiplicity\n",
    "(e.g., celebrity obsessions, personal promotion, etc... )\n",
    "\n",
    "Check out the preprints of our recent research,\n",
    "which spawned this dataset:\n",
    "\n",
    "http://arxiv.org/abs/1505.04342\n",
    "http://arxiv.org/abs/1508.01843\n",
    "\n",
    "The main data lie in the accompanying file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words.txt\n",
    "\n",
    "and are of the form:\n",
    "\n",
    "USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...\n",
    ".\n",
    ".\n",
    "\n",
    "where\n",
    "\n",
    "USERID = unique user identifier\n",
    "CODE = 0/1/2/3 class code\n",
    "TOTAL = sum of the word counts\n",
    "\n",
    "Using this data, you will implement a 1000-dimensional K-means algorithm on the users\n",
    "by their 1000-dimensional word stripes/vectors using several \n",
    "centroid initializations and values of K.\n",
    "\n",
    "Note that each \"point\" is a user as represented by 1000 words, and that\n",
    "word-frequency distributions are generally heavy-tailed power-laws\n",
    "(often called Zipf distributions), and are very rare in the larger class\n",
    "of discrete, random distributions. For each user you will have to normalize\n",
    "by its \"TOTAL\" column. Try several parameterizations and initializations:\n",
    "\n",
    "(A) K=4 uniform random centroid-distributions over the 1000 words\n",
    "(B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "(C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "(D) K=4 \"trained\" centroids, determined by the sums across the classes.\n",
    "\n",
    "and iterate until a threshold (try 0.001) is reached.\n",
    "After convergence, print out a summary of the classes present in each cluster.\n",
    "In particular, report the composition as measured by the total\n",
    "portion of each class type (0-3) contained in each cluster,\n",
    "and discuss your findings and any differences in outcomes across parts A-D.\n",
    "\n",
    "Note that you do not have to compute the aggregated distribution or the \n",
    "class-aggregated distributions, which are rows in the auxiliary file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words_summaries.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process File By Normalizing the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.0\n",
      "Max: 40.990967\n"
     ]
    }
   ],
   "source": [
    "# Pre-Process the input by normalizing the values\n",
    "from sets import Set\n",
    "\n",
    "input_file = 'topUsers_Apr-Jul_2014_1000-words.txt'\n",
    "output_file = input_file + '.pp'\n",
    "\n",
    "try:\n",
    "    os.remove(output_file)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "values = Set()\n",
    "\n",
    "with open(input_file, 'r') as f1, open(output_file, 'w') as f2:\n",
    "    for line in f1:\n",
    "        count = 0\n",
    "        elements = line.strip().split(\",\")\n",
    "        total = int(elements[2])\n",
    "        for i in xrange(3, len(elements)):\n",
    "            value = round(int(elements[i]) * 100.0 / total, 6)\n",
    "            elements[i] = str(value)\n",
    "            values.add(value)\n",
    "        f2.write(\",\".join(elements) + \"\\n\")\n",
    "        \n",
    "print 'Min: {0}'.format(min(values))\n",
    "print 'Max: {0}'.format(max(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Initial Centroids for different use cases\n",
    "* (A) K=4 uniform random centroid-distributions over the 1000 words\n",
    "* (B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "* (C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "* (D) K=4 \"trained\" centroids, determined by the sums across the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_hw45.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_hw45.py\n",
    "from numpy import argmin, array, random\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.compat import get_jobconf_value \n",
    "from itertools import chain\n",
    "import sys\n",
    "\n",
    "#Calculate find the nearest centroid for data point \n",
    "def MinDist(datapoint, centroid_points):\n",
    "    datapoint = array(datapoint)\n",
    "    centroid_points = array(centroid_points)\n",
    "    diff = datapoint - centroid_points \n",
    "    diffsq = diff*diff\n",
    "    # Get the nearest centroid for each instance\n",
    "    minidx = argmin(list(diffsq.sum(axis = 1)))\n",
    "    return minidx\n",
    "\n",
    "#Check whether centroids converge\n",
    "def stop_criterion(centroid_points_old, centroid_points_new,T):\n",
    "    oldvalue = list(chain(*centroid_points_old))\n",
    "    newvalue = list(chain(*centroid_points_new))\n",
    "    Diff = [abs(x-y) for x, y in zip(oldvalue, newvalue)]\n",
    "    Flag = True\n",
    "    for i in Diff:\n",
    "        if(i>T):\n",
    "            Flag = False\n",
    "            break\n",
    "    return Flag\n",
    "\n",
    "class MRKmeans(MRJob):\n",
    "    centroid_points=[]\n",
    "    #k=0    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init = self.mapper_init, mapper=self.mapper,combiner = self.combiner,reducer=self.reducer)\n",
    "               ]\n",
    "    #load centroids info from file\n",
    "    def mapper_init(self):\n",
    "        self.centroid_points = [map(float,s.split('\\n')[0].split(',')) for s in open(\"Centroids.txt\").readlines()]\n",
    "        sys.stderr.write('[ERR] k(m_i) {0}\\n'.format(get_jobconf_value('k')))\n",
    "        open('Centroids.txt', 'w').close()\n",
    "    #load data and output the nearest centroid index and data point \n",
    "    def mapper(self, _, line):\n",
    "        D = (map(float,line.split(',')))\n",
    "        #yield int(MinDist(D,self.centroid_points)), (D[0],D[1],1)\n",
    "        yield int(MinDist(D[3:],self.centroid_points)), (D[3:],1)\n",
    "    #Combine sum of data points locally\n",
    "    def combiner(self, idx, inputdata):\n",
    "        sys.stderr.write('[ERR] k(c) {0}\\n'.format(get_jobconf_value('k')))\n",
    "        '''\n",
    "        sumx = sumy = num = 0\n",
    "        for x,y,n in inputdata:\n",
    "            num = num + n\n",
    "            sumx = sumx + x\n",
    "            sumy = sumy + y\n",
    "        yield idx,(sumx,sumy,num)\n",
    "        '''\n",
    "        num = 0\n",
    "        sum_n = [0 for i in xrange(1000)]\n",
    "        for d, n in inputdata:\n",
    "            num = num + n\n",
    "            sum_n = [x + y for x,y in zip(d,sum_n)]\n",
    "        yield idx,(sum_n,num) \n",
    "    #Aggregate sum for each cluster and then calculate the new centroids\n",
    "    def reducer(self, idx, inputdata): \n",
    "        centroids = []\n",
    "        k = int(get_jobconf_value('k'))\n",
    "        num = [0] * k\n",
    "        sys.stderr.write('[ERR] idx: {0}\\n'.format(str(idx)))\n",
    "        sys.stderr.write('[ERR] k(r): {0}\\n'.format(get_jobconf_value('k')))\n",
    "        sys.stderr.write('[ERR] num: {0}\\n'.format(str(num)))\n",
    "        for i in range(k):\n",
    "            #centroids.append([0,0])\n",
    "            centroids.append([0 for i in xrange(1000)])\n",
    "        '''\n",
    "        for x, y, n in inputdata:\n",
    "            num[idx] = num[idx] + n\n",
    "            centroids[idx][0] = centroids[idx][0] + x\n",
    "            centroids[idx][1] = centroids[idx][1] + y\n",
    "        centroids[idx][0] = centroids[idx][0]/num[idx]\n",
    "        centroids[idx][1] = centroids[idx][1]/num[idx]\n",
    "        with open('Centroids.txt', 'a') as f:\n",
    "            f.writelines(str(centroids[idx][0]) + ',' + str(centroids[idx][1]) + '\\n')\n",
    "        yield idx,(centroids[idx][0],centroids[idx][1])\n",
    "        '''\n",
    "        for d, n in inputdata:\n",
    "            num[idx] = num[idx] + n\n",
    "            for i in xrange(1000):\n",
    "                centroids[idx][i] = centroids[idx][i] + d[i]\n",
    "        for i in xrange(1000):\n",
    "            centroids[idx][i] = centroids[idx][i]/num[idx]\n",
    "            \n",
    "        with open('Centroids.txt', 'a') as f:\n",
    "            f.writelines(\",\".join(centroids[idx]) + '\\n')\n",
    "        yield idx,(centroids[idx])\n",
    "      \n",
    "if __name__ == '__main__':\n",
    "    MRKmeans.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x mrjob_hw45.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "[ERR] k(m_i) 4\n",
      "[ERR] k(c) 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "iteration 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERR] idx: 0\n",
      "[ERR] k(r): 4\n",
      "[ERR] num: [0, 0, 0, 0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected string, float found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-143-5c1c18bfae71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"iteration \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\":\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mmr_job\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_runner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[1;31m# stream_output: get access of the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/anaconda/lib/python2.7/site-packages/mrjob/runner.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    468\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Job already ran!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 470\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    471\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ran_job\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/anaconda/lib/python2.7/site-packages/mrjob/sim.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m                 \u001b[1;31m# run the reducer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invoke_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reducer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;31m# move final output to output directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/anaconda/lib/python2.7/site-packages/mrjob/sim.pyc\u001b[0m in \u001b[0;36m_invoke_step\u001b[1;34m(self, step_num, step_type)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m             self._run_step(step_num, step_type, input_path, output_path,\n\u001b[1;32m--> 260\u001b[1;33m                            working_dir, env)\n\u001b[0m\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prev_outfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/anaconda/lib/python2.7/site-packages/mrjob/inline.pyc\u001b[0m in \u001b[0;36m_run_step\u001b[1;34m(self, step_num, step_type, input_path, output_path, working_dir, env, child_stdin)\u001b[0m\n\u001b[0;32m    158\u001b[0m                 \u001b[0mchild_instance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mrjob_cls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchild_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[0mchild_instance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msandbox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstdin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchild_stdin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchild_stdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m                 \u001b[0mchild_instance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhas_combiner\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/anaconda/lib/python2.7/site-packages/mrjob/job.pyc\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_reducer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_reducer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/anaconda/lib/python2.7/site-packages/mrjob/job.pyc\u001b[0m in \u001b[0;36mrun_reducer\u001b[1;34m(self, step_num)\u001b[0m\n\u001b[0;32m    578\u001b[0m                                                key=lambda(k, v): k):\n\u001b[0;32m    579\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkv_pairs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mout_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_value\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreducer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m                 \u001b[0mwrite_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/dev/ml/w261/wk4/mrjob_hw45.py\u001b[0m in \u001b[0;36mreducer\u001b[1;34m(self, idx, inputdata)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Centroids.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwritelines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected string, float found"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from mrjob_hw45 import MRKmeans, stop_criterion\n",
    "\n",
    "# Case: (A) K=4 uniform random centroid-distributions over the 1000 words\n",
    "K = 4\n",
    "output_file = 'Centroids.txt'\n",
    "mr_job = MRKmeans(args=['--file', output_file,\n",
    "                        '--jobconf', 'k=4',\n",
    "                        'topUsers_Apr-Jul_2014_1000-words.txt.pp', '-v'])\n",
    "\n",
    "#Geneate initial centroids\n",
    "\n",
    "try:\n",
    "    os.remove(output_file)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    for k in xrange(K):\n",
    "        uniform_random_arr = np.random.uniform(0.000001, 40.990967, 1000)\n",
    "        uniform_random_arr = uniform_random_arr.tolist()\n",
    "        uniform_random_arr = [str(round(i,6)) for i in uniform_random_arr]\n",
    "        f.write(\",\".join(uniform_random_arr) + \"\\n\")\n",
    "        #print uniform_random_arr\n",
    "\n",
    "# Update centroids iteratively\n",
    "i = 0\n",
    "while(1):\n",
    "    # save previous centoids to check convergency\n",
    "    centroid_points_old = centroid_points[:]\n",
    "    print \"iteration \" + str(i) + \":\"\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            print key, value\n",
    "            centroid_points[key] = value\n",
    "    print \"\\n\"\n",
    "    i = i + 1\n",
    "    if(stop_criterion(centroid_points_old,centroid_points,0.01)):\n",
    "        break\n",
    "print \"Centroids\\n\"\n",
    "print centroid_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<type 'list'> 1003 1000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1000,) (0,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-982c5fa9e7c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[1;32mprint\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMinDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcentroid_points\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mKmeans.py\u001b[0m in \u001b[0;36mMinDist\u001b[1;34m(datapoint, centroid_points)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1000,) (0,) "
     ]
    }
   ],
   "source": [
    "# Debug/Test\n",
    "from Kmeans import MRKmeans, stop_criterion, MinDist\n",
    "\n",
    "centroid_points = [map(float,s.split('\\n')[0].split(',')) for s in open(\"Centroids.txt\").readlines()]\n",
    "open('Centroids.txt', 'w').close()\n",
    "print len(centroid_points)\n",
    "\n",
    "with open('topUsers_Apr-Jul_2014_1000-words.txt.pp','r') as f:\n",
    "    for line in f:\n",
    "        print type(D), len(D), len(D[3:])\n",
    "        D = (map(float,line.split(',')))\n",
    "        print int(MinDist(D[3:],centroid_points))\n",
    "        break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
