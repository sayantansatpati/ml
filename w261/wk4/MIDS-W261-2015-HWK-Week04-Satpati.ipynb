{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sayantan Satpati**\n",
    "* **sayantan.satpati@ischool.berkeley.edu**\n",
    "* **W261**\n",
    "* **Week-4**\n",
    "* **Assignment-4**\n",
    "* **Date of Submission: 29-SEP-2015**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  === Week 4: mrjob & k-means ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.0\n",
    "---\n",
    "\n",
    "***What is MrJob? How is it different to Hadoop MapReduce?***\n",
    "\n",
    "***What are the mapper_final(), combiner_final(), reducer_final() methods? When are they called?***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.1\n",
    "---\n",
    "\n",
    "***What is serialization in the context of MrJob or Hadoop?***\n",
    "\n",
    "***When it used in these frameworks?***\n",
    "\n",
    "***What is the default serialization mode for input and outputs for MrJob?***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.2\n",
    "---\n",
    "\n",
    "Recall the Microsoft logfiles data from the async lecture. The logfiles are described are located at:\n",
    "\n",
    "https://kdd.ics.uci.edu/databases/msweb/msweb.html\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/\n",
    "\n",
    "This dataset records which areas (Vroots) of www.microsoft.com each user visited in a one-week timeframe in Feburary 1998.\n",
    "\n",
    " Here, you must preprocess the data on a single node (i.e., not on a cluster of nodes) from the format:\n",
    "\n",
    "C,\"10001\",10001   #Visitor id 10001\n",
    "V,1000,1          #Visit by Visitor 10001 to page id 1000\n",
    "V,1001,1          #Visit by Visitor 10001 to page id 1001\n",
    "V,1002,1          #Visit by Visitor 10001 to page id 1002\n",
    "C,\"10002\",10002   #Visitor id 10001\n",
    "V\n",
    "Note: #denotes comments\n",
    "to the format:\n",
    "\n",
    "V,1000,1,C, 10001\n",
    "V,1001,1,C, 10001\n",
    "V,1002,1,C, 10001\n",
    "\n",
    "Write the python code to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess_hw42.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess_hw42.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if len(sys.argv) < 2:\n",
    "    print \"No input file is passed, Aborting!!!\"\n",
    "    sys.exit(1)\n",
    "\n",
    "input_file = sys.argv[1]\n",
    "output_file = input_file + '.pp'\n",
    "\n",
    "try:\n",
    "    os.remove(output_file)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "last_visitor = None\n",
    "with open(input_file, 'r') as f1:\n",
    "    with open(output_file, 'a') as f2:\n",
    "        for line in f1:\n",
    "            line = line.strip()\n",
    "            tokens = line.split(\",\")\n",
    "            if len(tokens) == 3 and tokens[0] == 'C':\n",
    "                last_visitor = tokens[2]\n",
    "\n",
    "            if len(tokens) == 3 and tokens[0] == 'V':\n",
    "                out_line = '{0},C,{1}\\n'.format(line,last_visitor)\n",
    "                f2.write(out_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x preprocess_hw42.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V,1000,1,C,10001\r\n",
      "V,1001,1,C,10001\r\n",
      "V,1002,1,C,10001\r\n",
      "V,1001,1,C,10002\r\n",
      "V,1003,1,C,10002\r\n",
      "V,1001,1,C,10003\r\n",
      "V,1003,1,C,10003\r\n",
      "V,1004,1,C,10003\r\n",
      "V,1005,1,C,10004\r\n",
      "V,1006,1,C,10005\r\n"
     ]
    }
   ],
   "source": [
    "!python preprocess_hw42.py anonymous-msweb.data\n",
    "!head -n 10 anonymous-msweb.data.pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW 4.3\n",
    "---\n",
    "\n",
    "**Find the 5 most frequently visited pages using mrjob from the output of 4.2 (i.e., transfromed log file).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_hw43.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_hw43.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "\n",
    "class MRVistedPagesCount(MRJob):\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer),\n",
    "            MRStep(reducer=self.reducer_find_top_5)\n",
    "        ]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        tokens = line.strip().split(\",\")\n",
    "        yield tokens[1], 1\n",
    "\n",
    "    def combiner(self, page_visted, counts):\n",
    "        yield page_visted, sum(counts)\n",
    "\n",
    "    def reducer(self, page_visted, counts):\n",
    "        yield None, (sum(counts), page_visted)\n",
    "\n",
    "    # discard the key; it is just None\n",
    "    def reducer_find_top_5(self, _, page_visted_pairs):\n",
    "        # Store all the keys into memory (Assumption: Can be loaded into memory)\n",
    "        pairs = []\n",
    "        for p in page_visted_pairs:\n",
    "            pairs.append(p)\n",
    "        pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "            \n",
    "        for p in pairs[:5]:\n",
    "            yield p[1],p[0]\n",
    "            \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRVistedPagesCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mrjob_hw43.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1008\"\t10836\r\n",
      "\"1034\"\t9383\r\n",
      "\"1004\"\t8463\r\n",
      "\"1018\"\t5330\r\n",
      "\"1017\"\t5108\r\n"
     ]
    }
   ],
   "source": [
    "# Running mrjob using command line\n",
    "!python mrjob_hw43.py -r local anonymous-msweb.data.pp -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/0\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/0/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/0/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/0\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python mrjob_hw43.py --step-num=0 --mapper /tmp/mrjob_hw43.cloudera.20150926.200443.882049/input_part-00000\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/0\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/0/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/0/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/0\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python mrjob_hw43.py --step-num=0 --combiner\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/1\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/1/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/1/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/1\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python mrjob_hw43.py --step-num=0 --mapper /tmp/mrjob_hw43.cloudera.20150926.200443.882049/input_part-00001\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/1\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/1/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/1/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/mapper/1\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python mrjob_hw43.py --step-num=0 --combiner\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/reducer/0\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/reducer/0/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/reducer/0/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/reducer/0\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python mrjob_hw43.py --step-num=0 --reducer /tmp/mrjob_hw43.cloudera.20150926.200443.882049/input_part-00000\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/reducer/1\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/reducer/1/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/reducer/1/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/0/reducer/1\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python mrjob_hw43.py --step-num=0 --reducer /tmp/mrjob_hw43.cloudera.20150926.200443.882049/input_part-00001\n",
      "ERROR:mrjob.local:STDERR: + __mrjob_PWD=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/1/reducer/0\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
      "ERROR:mrjob.local:STDERR: + export PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/1/reducer/0/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + PYTHONPATH=/tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/1/reducer/0/mrjob.tar.gz:\n",
      "ERROR:mrjob.local:STDERR: + exec\n",
      "ERROR:mrjob.local:STDERR: + cd /tmp/mrjob_hw43.cloudera.20150926.200443.882049/job_local_dir/1/reducer/0\n",
      "ERROR:mrjob.local:STDERR: + /home/cloudera/anaconda/bin/python mrjob_hw43.py --step-num=1 --reducer /tmp/mrjob_hw43.cloudera.20150926.200443.882049/input_part-00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1008', 10836)\n",
      "('1034', 9383)\n",
      "('1004', 8463)\n",
      "('1018', 5330)\n",
      "('1017', 5108)\n"
     ]
    }
   ],
   "source": [
    "# Running mrjob using a driver\n",
    "from mrjob_hw43 import MRVistedPagesCount\n",
    "mr_job = MRVistedPagesCount(args=['-r', 'local', 'anonymous-msweb.data.pp', 'q'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW 4.4 \n",
    "---\n",
    "\n",
    "**Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "1. Create a separate file with URLS, i.e with records that start with A. This will be passed to the mrjob as an additional file for joining datasets\n",
    "2. Following happens in the first pass of mrjob:\n",
    "    * Mapper: Emits (pageId,visitorId), 1\n",
    "    * Combiner: Combines the counts\n",
    "    * Reducer: Combines the counts and emits records like (p1,v1) 100 | (p1, v2), 101 | (p1,v3), 202\n",
    "    * Partitioner is used to that all keys with the same pageId goes to the same reducer\n",
    "3. Following happens in the second pass of mrjob:\n",
    "    * Mapper: Emits (pageId,visit_count), visitorId\n",
    "    * Reducer_Init: Loads the url file into a dict\n",
    "    * Reducer: Emits (pageId, pageURL),(vists, List of visitorId in case of a tie)\n",
    "    * Partitioner is used to that all keys with the same pageId goes to the same reducer\n",
    "    * Secondary sorting (Descending Order) is done on the visit_count. This ensures that pages with highest page visits come first, before other records which has a lower page visit for the same page.\n",
    "   \n",
    "**Final Output Format(output_hw44.txt): (pageId, pageURL),(vists, List of visitorId in case of a tie)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed `url'\r\n"
     ]
    }
   ],
   "source": [
    "# Create a file with only URL(s), i.e. records starting with 'A'\n",
    "!rm -v url\n",
    "!grep ^A anonymous-msweb.data > url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_hw44.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_hw44.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "\n",
    "class MRFrequentVisitor(MRJob):\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer),\n",
    "             MRStep(mapper=self.mapper_frequent_visitor,\n",
    "                   reducer_init=self.reducer_frequent_visitor_init,\n",
    "                   reducer=self.reducer_frequent_visitor)\n",
    "        ]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        tokens = line.strip().split(\",\")\n",
    "        key = \"{0},{1}\".format(tokens[1],tokens[4])\n",
    "        yield key, 1\n",
    "\n",
    "    def combiner(self, key, counts):\n",
    "        yield key, sum(counts)\n",
    "\n",
    "    def reducer(self, key, counts):\n",
    "        yield key, sum(counts)\n",
    "        \n",
    "    # 2nd Pass\n",
    "    \n",
    "    def mapper_frequent_visitor(self, key, value):\n",
    "        tokens = key.strip().split(\",\")\n",
    "        modified_key = \"{0},{1}\".format(tokens[0],value)\n",
    "        yield modified_key, tokens[1]\n",
    "     \n",
    "    \n",
    "    def reducer_frequent_visitor_init(self):\n",
    "        # Reads the 'url' file into a Dict for displaying additional information\n",
    "        self.last_page = None\n",
    "        self.pageDict = {}\n",
    "        with open('url','r') as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split(\",\")\n",
    "                self.pageDict[tokens[1]] = tokens[4]\n",
    "                \n",
    "    def reducer_frequent_visitor(self, key, values):\n",
    "        tokens = key.strip().split(\",\")\n",
    "        page = tokens[0]\n",
    "        visits = int(tokens[1])\n",
    "        \n",
    "        if self.last_page != page:\n",
    "            self.last_page = page\n",
    "            # values might be a list, if there is a tie for same key => (p1, 1000), [v1,v2,v3..]\n",
    "            for value in values:\n",
    "                k = '{0},{1}'.format(page, \n",
    "                                    self.pageDict.get(page, 'NA').replace(\"\\\"\",\"\"))\n",
    "                v = '{0},{1}'.format(visits,\n",
    "                                    value)\n",
    "                yield k,v\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRFrequentVisitor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mrjob_hw44.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Running mrjob using a Hadoop Runner in local cluster\n",
    "from mrjob_hw44 import MRFrequentVisitor\n",
    "import os\n",
    "\n",
    "mr_job = MRFrequentVisitor(args=['-r', 'hadoop', \n",
    "                                 '--hadoop-home', '/usr/lib/hadoop-0.20-mapreduce',\n",
    "                                 '--hadoop-bin', '/usr/bin/hadoop',\n",
    "                                 '--file', 'url',\n",
    "                                 '--jobconf', 'stream.num.map.output.key.fields=2',\n",
    "                                 '--jobconf', 'map.output.key.field.separator=,',\n",
    "                                 '--jobconf', 'mapred.text.key.partitioner.options=-k1,1',\n",
    "                                 '--jobconf', 'mapred.text.key.comparator.options=-k1,1 -k2,2nr',\n",
    "                                 '--jobconf', 'mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                                 '--partitioner', 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner',\n",
    "                                 'anonymous-msweb.data.pp', '-v'])\n",
    "\n",
    "\n",
    "output_file = \"output_hw44.txt\"\n",
    "try:\n",
    "    os.remove(output_file)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "with mr_job.make_runner() as runner, open(output_file, 'a') as f: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        count += 1\n",
    "        if count == 20:\n",
    "            break;\n",
    "        #print mr_job.parse_output_line(line)\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##HW 4.5 \n",
    "---\n",
    "\n",
    "Here you will use a different dataset consisting of word-frequency distributions \n",
    "for 1,000 Twitter users. These Twitter users use language in very different ways,\n",
    "and were classified by hand according to the criteria:\n",
    "\n",
    "0: Human, where only basic human-human communication is observed.\n",
    "\n",
    "1: Cyborg, where language is primarily borrowed from other sources\n",
    "(e.g., jobs listings, classifieds postings, advertisements, etc...).\n",
    "\n",
    "2: Robot, where language is formulaically derived from unrelated sources\n",
    "(e.g., weather/seismology, police/fire event logs, etc...).\n",
    "\n",
    "3: Spammer, where language is replicated to high multiplicity\n",
    "(e.g., celebrity obsessions, personal promotion, etc... )\n",
    "\n",
    "Check out the preprints of our recent research,\n",
    "which spawned this dataset:\n",
    "\n",
    "http://arxiv.org/abs/1505.04342\n",
    "http://arxiv.org/abs/1508.01843\n",
    "\n",
    "The main data lie in the accompanying file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words.txt\n",
    "\n",
    "and are of the form:\n",
    "\n",
    "USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...\n",
    ".\n",
    ".\n",
    "\n",
    "where\n",
    "\n",
    "USERID = unique user identifier\n",
    "CODE = 0/1/2/3 class code\n",
    "TOTAL = sum of the word counts\n",
    "\n",
    "Using this data, you will implement a 1000-dimensional K-means algorithm on the users\n",
    "by their 1000-dimensional word stripes/vectors using several \n",
    "centroid initializations and values of K.\n",
    "\n",
    "Note that each \"point\" is a user as represented by 1000 words, and that\n",
    "word-frequency distributions are generally heavy-tailed power-laws\n",
    "(often called Zipf distributions), and are very rare in the larger class\n",
    "of discrete, random distributions. For each user you will have to normalize\n",
    "by its \"TOTAL\" column. Try several parameterizations and initializations:\n",
    "\n",
    "(A) K=4 uniform random centroid-distributions over the 1000 words\n",
    "(B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "(C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "(D) K=4 \"trained\" centroids, determined by the sums across the classes.\n",
    "\n",
    "and iterate until a threshold (try 0.001) is reached.\n",
    "After convergence, print out a summary of the classes present in each cluster.\n",
    "In particular, report the composition as measured by the total\n",
    "portion of each class type (0-3) contained in each cluster,\n",
    "and discuss your findings and any differences in outcomes across parts A-D.\n",
    "\n",
    "Note that you do not have to compute the aggregated distribution or the \n",
    "class-aggregated distributions, which are rows in the auxiliary file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words_summaries.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Environment \n",
    "\n",
    "##### Hadoop\n",
    "\n",
    "* For the purpose of this assignment, Cloudera VM (in Mac) has been used which comes pre-packaged with CDH 4.7.\n",
    "* Folder structure for input and output for HW3.1 and HW3.2\n",
    "\n",
    "```\n",
    "[cloudera@localhost wk3]$ hadoop fs -ls /user/cloudera/w261/wk3/*/input\n",
    "Found 1 items\n",
    "-rw-r--r--   3 cloudera cloudera    3458517 2015-09-21 13:15 /user/cloudera/w261/wk3/hw31/input/ProductPurchaseData.txt\n",
    "Found 1 items\n",
    "-rw-r--r--   3 cloudera cloudera    3458517 2015-09-21 13:15 /user/cloudera/w261/wk3/hw32/input/ProductPurchaseData.txt\n",
    "[cloudera@localhost wk3]$ hadoop fs -ls /user/cloudera/w261/wk3/*/output\n",
    "Found 3 items\n",
    "-rw-r--r--   3 cloudera cloudera          0 2015-09-21 13:16 /user/cloudera/w261/wk3/hw31/output/_SUCCESS\n",
    "drwxr-xr-x   - cloudera cloudera          0 2015-09-21 13:15 /user/cloudera/w261/wk3/hw31/output/_logs\n",
    "-rw-r--r--   3 cloudera cloudera        116 2015-09-21 13:16 /user/cloudera/w261/wk3/hw31/output/part-00000\n",
    "Found 3 items\n",
    "-rw-r--r--   3 cloudera cloudera          0 2015-09-21 13:18 /user/cloudera/w261/wk3/hw32/output/_SUCCESS\n",
    "drwxr-xr-x   - cloudera cloudera          0 2015-09-21 13:16 /user/cloudera/w261/wk3/hw32/output/_logs\n",
    "-rw-r--r--   3 cloudera cloudera        161 2015-09-21 13:18 /user/cloudera/w261/wk3/hw32/output/part-00000\n",
    "```\n",
    "\n",
    "##### Amazon EC2\n",
    "\n",
    "* For HW3.3, Ubuntu 1404 (Micro) VM was spinned up in Amazon EC2 Cluster (More details in that section) in order to run PyFim on the Product Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_hw31.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_hw31.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        # Remove leading & trailing chars\n",
    "        line = line.strip()\n",
    "        # Split the line by <TAB> delimeter\n",
    "        items = re.split(r'\\s', line)\n",
    "               \n",
    "        for i in items:\n",
    "            print '%s\\t%d\\t%d' %(i, 1, len(items))\n",
    "        \n",
    "        ''' Commenting out\n",
    "        #Sort the list\n",
    "        items.sort()\n",
    "        \n",
    "        for c in itertools.combinations(items, 1):\n",
    "            print '%s,%s\\t%d\\t%d' %(c[0], '*', 1, len(items))\n",
    "            \n",
    "        for c in itertools.combinations(items, 2):\n",
    "            print '%s,%s\\t%d' %(c[0], c[1], 1)\n",
    "        \n",
    "        for c in itertools.combinations(items, 3):\n",
    "            print '%s,%s,%s\\t%d' %(c[0], c[1], c[2], 1)\n",
    "        '''\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper_hw31.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_hw31.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_hw31.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "from sets import Set\n",
    "\n",
    "'''\n",
    "a1 1 5\n",
    "a1 1 5\n",
    "a1 1 6\n",
    "a2 1 18\n",
    "'''\n",
    "\n",
    "itemset_1_cnt = 0\n",
    "item_last = None\n",
    "\n",
    "THRESHOLD = 100\n",
    "\n",
    "# Statistics\n",
    "# Unique Items\n",
    "uniq = Set()\n",
    "# Max Basket Length\n",
    "max_basket_len = 0\n",
    "# Total Itemset Counts for Sizes: 1\n",
    "total_itemset_1 = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Remove leading & trailing chars\n",
    "        line = line.strip()\n",
    "        # Split the line by <TAB> delimeter\n",
    "        tokens = re.split(r'\\s', line)\n",
    "    \n",
    "        item = tokens[0]\n",
    "        cnt = int(tokens[1])\n",
    "        basket_len = int(tokens[2])\n",
    "        \n",
    "        if not item_last:\n",
    "            item_last = item\n",
    "            \n",
    "        # Basket Length\n",
    "        if basket_len > max_basket_len:\n",
    "            max_basket_len = basket_len\n",
    "        \n",
    "        # Unique Items\n",
    "        uniq.add(item)\n",
    "        \n",
    "        if item_last != item:\n",
    "            # Check whether itemset 1 exceeds the support of 100\n",
    "            if itemset_1_cnt >= THRESHOLD:\n",
    "                total_itemset_1 += 1\n",
    "    \n",
    "            item_last = item\n",
    "            itemset_1_cnt = cnt\n",
    "        else:\n",
    "            itemset_1_cnt += cnt\n",
    "                    \n",
    "# Last Record\n",
    "if itemset_1_cnt >= THRESHOLD:\n",
    "    total_itemset_1 += 1\n",
    "\n",
    "print '=== Statistics ==='\n",
    "print 'Total Unique Items: %d' %(len(uniq))\n",
    "print 'Maximum Basket Length: %d' %(max_basket_len)\n",
    "print 'Total # frequent itemsets of size 1: %d' %(total_itemset_1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer_hw31.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/cloudera/w261/wk3/hw31/output\n",
      "packageJobJar: [./mapper_hw31.py, ./reducer_hw31.py, /tmp/hadoop-cloudera/hadoop-unjar1278596197593495009/] [] /tmp/streamjob4204464395455964255.jar tmpDir=null\n",
      "15/09/21 13:15:49 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n",
      "15/09/21 13:15:49 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/21 13:15:50 INFO streaming.StreamJob: getLocalDirs(): [/tmp/hadoop-cloudera/mapred/local]\n",
      "15/09/21 13:15:50 INFO streaming.StreamJob: Running job: job_201509211304_0004\n",
      "15/09/21 13:15:50 INFO streaming.StreamJob: To kill this job, run:\n",
      "15/09/21 13:15:50 INFO streaming.StreamJob: UNDEF/bin/hadoop job  -Dmapred.job.tracker=localhost.localdomain:8021 -kill job_201509211304_0004\n",
      "15/09/21 13:15:50 INFO streaming.StreamJob: Tracking URL: http://0.0.0.0:50030/jobdetails.jsp?jobid=job_201509211304_0004\n",
      "15/09/21 13:15:51 INFO streaming.StreamJob:  map 0%  reduce 0%\n",
      "15/09/21 13:16:10 INFO streaming.StreamJob:  map 100%  reduce 0%\n",
      "15/09/21 13:16:23 INFO streaming.StreamJob:  map 100%  reduce 100%\n",
      "15/09/21 13:16:26 INFO streaming.StreamJob: Job complete: job_201509211304_0004\n",
      "15/09/21 13:16:26 INFO streaming.StreamJob: Output: /user/cloudera/w261/wk3/hw31/output\n",
      "=== Statistics ===\t\n",
      "Total Unique Items: 12592\t\n",
      "Maximum Basket Length: 37\t\n",
      "Total # frequent itemsets of size 1: 647\t\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HW3.1. Product Recommendations\n",
    "'''\n",
    "\n",
    "# Delete existing Output Dirs if available\n",
    "!hadoop fs -rm -r -skipTrash /user/cloudera/w261/wk3/hw31/output\n",
    "\n",
    "# Run the Hadoop Streaming Command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.7.0.jar \\\n",
    "-input /user/cloudera/w261/wk3/hw31/input/ProductPurchaseData.txt \\\n",
    "-output /user/cloudera/w261/wk3/hw31/output \\\n",
    "-file ./mapper_hw31.py \\\n",
    "-mapper 'python mapper_hw31.py' \\\n",
    "-file ./reducer_hw31.py \\\n",
    "-reducer 'python reducer_hw31.py'\n",
    "\n",
    "# Show Output\n",
    "!hadoop fs -cat /user/cloudera/w261/wk3/hw31/output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.2. (Computationally prohibitive but then again Hadoop can handle this)\n",
    "---\n",
    "\n",
    "Note: for this part the writeup will require a specific rule ordering but the program need not sort the output.\n",
    "\n",
    "List the top 5 rules with corresponding confidence scores in decreasing order of confidence score \n",
    "for frequent (100>count) itemsets of size 2. \n",
    "A rule is of the form: \n",
    "\n",
    "(item1) ⇒ item2.\n",
    "\n",
    "Fix the ordering of the rule lexicographically (left to right), \n",
    "and break ties in confidence (between rules, if any exist) \n",
    "by taking the first ones in lexicographically increasing order. \n",
    "Use Hadoop MapReduce to complete this part of the assignment; \n",
    "use a single mapper and single reducer; use a combiner if you think it will help and justify. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_hw32.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_hw32.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        # Remove leading & trailing chars\n",
    "        line = line.strip()\n",
    "        # Split the line by <TAB> delimeter\n",
    "        items = re.split(r'\\s+', line)\n",
    "        #Sort the list\n",
    "        items.sort()\n",
    "        \n",
    "        l = len(items)\n",
    "        \n",
    "        for i in xrange(l):\n",
    "            print '%s,*\\t%d' %(items[i], 1)\n",
    "            for j in xrange(i+1, l):\n",
    "               print '%s,%s\\t%d' %(items[i], items[j], 1)\n",
    "               print '%s,%s\\t%d' %(items[j], items[i], 1)\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper_hw32.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_hw32.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_hw32.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import heapq\n",
    "\n",
    "itemset_1_cnt = 0\n",
    "itemset_2_cnt = 0\n",
    "\n",
    "itemset_1_last = None\n",
    "itemset_2_last = None\n",
    "\n",
    "'''\n",
    "a1,* 1\n",
    "a1,* 1\n",
    "a1,b1 1\n",
    "a1,b1 1\n",
    "a1,b2 1\n",
    "a1,b2 1\n",
    "a2,* 1\n",
    "'''\n",
    "\n",
    "THRESHOLD = 100\n",
    "# Store Itemsets 2\n",
    "dict = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Remove leading & trailing chars\n",
    "        line = line.strip()\n",
    "        # Split the line by <TAB> delimeter\n",
    "        tokens = re.split(r'\\s+', line)\n",
    "    \n",
    "        # Split the key by <COMMA> delimeter\n",
    "        items = tokens[0].split(\",\")\n",
    "        i1 = items[0]\n",
    "        i2 = items[1]\n",
    "        \n",
    "        if not itemset_1_last:\n",
    "            itemset_1_last = i1\n",
    "        \n",
    "        if itemset_1_last != i1:\n",
    "            '''\n",
    "            if itemset_1_cnt >= THRESHOLD:\n",
    "                confidence = (itemset_2_cnt * 1.0) / itemset_1_cnt\n",
    "                print '[%d,%d]%s\\t%f' %(itemset_1_cnt, itemset_2_cnt, tokens[0], confidence)\n",
    "                dict[tokens[0]] = confidence\n",
    "            '''\n",
    "                        \n",
    "            # Reset\n",
    "            itemset_1_last = i1\n",
    "            itemset_1_cnt = int(tokens[1])\n",
    "            itemset_2_last = None\n",
    "            itemset_2_cnt = 0\n",
    "        else:\n",
    "            if i2 == '*':\n",
    "                itemset_1_cnt += int(tokens[1])\n",
    "            else:\n",
    "                if itemset_2_last != tokens[0]:\n",
    "                    if itemset_1_cnt >= THRESHOLD and itemset_2_cnt >= THRESHOLD:\n",
    "                        confidence = (itemset_2_cnt * 1.0) / itemset_1_cnt\n",
    "                        #print '[%d,%d]%s\\t%f' %(itemset_1_cnt, itemset_2_cnt, itemset_2_last, confidence)\n",
    "                        dict[itemset_2_last] = confidence\n",
    "                    itemset_2_last = tokens[0]\n",
    "                    itemset_2_cnt = int(tokens[1]) \n",
    "                else:\n",
    "                    itemset_2_cnt += int(tokens[1])                    \n",
    "\n",
    "# Last Set of Counts\n",
    "if itemset_1_cnt >= THRESHOLD and itemset_2_cnt >= THRESHOLD:\n",
    "    confidence = (itemset_2_cnt * 1.0) / itemset_1_cnt\n",
    "    #print '[%d,%d]%s\\t%f' %(itemset_1_cnt, itemset_2_cnt, itemset_2_last, confidence)\n",
    "    dict[itemset_2_last] = confidence\n",
    "\n",
    "print '=== Top 5 Confidence ==='\n",
    "sorted_dict = sorted(dict.items(), key=lambda x:(-x[1], x[0]))\n",
    "for j,k in sorted_dict[:5]:\n",
    "    print '%s\\t%f' %(j,k)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer_hw32.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/cloudera/w261/wk3/hw32/output\n",
      "packageJobJar: [./mapper_hw32.py, ./reducer_hw32.py, /tmp/hadoop-cloudera/hadoop-unjar1867291528774777589/] [] /tmp/streamjob8875451858342305964.jar tmpDir=null\n",
      "15/09/21 13:16:53 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n",
      "15/09/21 13:16:53 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/21 13:16:54 INFO streaming.StreamJob: getLocalDirs(): [/tmp/hadoop-cloudera/mapred/local]\n",
      "15/09/21 13:16:54 INFO streaming.StreamJob: Running job: job_201509211304_0005\n",
      "15/09/21 13:16:54 INFO streaming.StreamJob: To kill this job, run:\n",
      "15/09/21 13:16:54 INFO streaming.StreamJob: UNDEF/bin/hadoop job  -Dmapred.job.tracker=localhost.localdomain:8021 -kill job_201509211304_0005\n",
      "15/09/21 13:16:54 INFO streaming.StreamJob: Tracking URL: http://0.0.0.0:50030/jobdetails.jsp?jobid=job_201509211304_0005\n",
      "15/09/21 13:16:55 INFO streaming.StreamJob:  map 0%  reduce 0%\n",
      "15/09/21 13:17:11 INFO streaming.StreamJob:  map 19%  reduce 0%\n",
      "15/09/21 13:17:12 INFO streaming.StreamJob:  map 34%  reduce 0%\n",
      "15/09/21 13:17:14 INFO streaming.StreamJob:  map 53%  reduce 0%\n",
      "15/09/21 13:17:18 INFO streaming.StreamJob:  map 72%  reduce 0%\n",
      "15/09/21 13:17:21 INFO streaming.StreamJob:  map 91%  reduce 0%\n",
      "15/09/21 13:17:23 INFO streaming.StreamJob:  map 95%  reduce 0%\n",
      "15/09/21 13:17:24 INFO streaming.StreamJob:  map 100%  reduce 0%\n",
      "15/09/21 13:17:37 INFO streaming.StreamJob:  map 100%  reduce 68%\n",
      "15/09/21 13:17:44 INFO streaming.StreamJob:  map 100%  reduce 69%\n",
      "15/09/21 13:17:50 INFO streaming.StreamJob:  map 100%  reduce 70%\n",
      "15/09/21 13:17:56 INFO streaming.StreamJob:  map 100%  reduce 71%\n",
      "15/09/21 13:18:02 INFO streaming.StreamJob:  map 100%  reduce 72%\n",
      "15/09/21 13:18:04 INFO streaming.StreamJob:  map 100%  reduce 100%\n",
      "15/09/21 13:18:07 INFO streaming.StreamJob: Job complete: job_201509211304_0005\n",
      "15/09/21 13:18:07 INFO streaming.StreamJob: Output: /user/cloudera/w261/wk3/hw32/output\n",
      "=== Top 5 Confidence ===\t\n",
      "DAI93865,FRO40251\t1.000000\n",
      "GRO85051,FRO40251\t0.999176\n",
      "GRO38636,FRO40251\t0.990654\n",
      "ELE12951,FRO40251\t0.990566\n",
      "DAI88079,FRO40251\t0.986726\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HW3.2. Confidence Caculations\n",
    "List the top 5 rules with corresponding confidence scores in decreasing order of confidence score \n",
    "for frequent (100>count) itemsets of size 2\n",
    "'''\n",
    "\n",
    "# Delete existing Output Dirs if available\n",
    "!hadoop fs -rm -r -skipTrash /user/cloudera/w261/wk3/hw32/output\n",
    "\n",
    "# Run the Hadoop Streaming Command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.7.0.jar \\\n",
    "-D map.output.key.field.separator=, \\\n",
    "-D mapred.text.key.partitioner.options=-k1,1 \\\n",
    "-input /user/cloudera/w261/wk3/hw32/input/ProductPurchaseData.txt \\\n",
    "-output /user/cloudera/w261/wk3/hw32/output \\\n",
    "-file ./mapper_hw32.py \\\n",
    "-mapper 'python mapper_hw32.py' \\\n",
    "-file ./reducer_hw32.py \\\n",
    "-reducer 'python reducer_hw32.py'\n",
    "\n",
    "# Show Output\n",
    "!hadoop fs -cat /user/cloudera/w261/wk3/hw32/output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.3\n",
    "---\n",
    "\n",
    "Benchmark your results using the pyFIM implementation of the Apriori algorithm\n",
    "(Apriori - Association Rule Induction / Frequent Item Set Mining implemented by Christian Borgelt). \n",
    "You can download pyFIM from here:***\n",
    "\n",
    "http://www.borgelt.net/pyfim.html\n",
    "\n",
    "Comment on the results from both implementations (your Hadoop MapReduce of apriori versus pyFIM) \n",
    "in terms of results and execution times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this part, the following steps were performed:\n",
    "\n",
    "1. Since I was using a Mac, I spinned up a Ubuntu 1404 (Micro) VM in Amazon EC2 Cluster\n",
    "2. Installed all required libraries: pip, git, ipython\n",
    "3. Downloaded the fim.so file from the link & set PYTHONPATH & LD_LIBRARY_PATH\n",
    "4. Ran the following command in the VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python top5pyfim.py | sort -n -r -k 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "FRO40251\t('DAI93865',)\t1.0\n",
    "FRO40251\t('GRO85051',)\t0.999176276771\n",
    "FRO40251\t('GRO38636',)\t0.990654205607\n",
    "FRO40251\t('ELE12951',)\t0.990566037736\n",
    "FRO40251\t('DAI88079',)\t0.986725663717\n",
    "FRO40251\t('FRO92469',)\t0.983510011779\n",
    "SNA82528\t('DAI43868',)\t0.972972972973\n",
    "DAI62779\t('DAI23334',)\t0.954545454545\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 5 Comparison from HW3.2 (Map/Reduce) and HW3.3 (PyFim Apriori)\n",
    "\n",
    "##### Map/Reduce\n",
    "```\n",
    "DAI93865,FRO40251\t1.000000\n",
    "GRO85051,FRO40251\t0.999176\n",
    "GRO38636,FRO40251\t0.990654\n",
    "ELE12951,FRO40251\t0.990566\n",
    "DAI88079,FRO40251\t0.986726\n",
    "```\n",
    "\n",
    "##### HW3.3 (PyFim Apriori)\n",
    "```\n",
    "FRO40251    ('DAI93865',)    1.0\n",
    "FRO40251    ('GRO85051',)    0.999176276771\n",
    "FRO40251    ('GRO38636',)    0.990654205607\n",
    "FRO40251    ('ELE12951',)    0.990566037736\n",
    "FRO40251    ('DAI88079',)    0.986725663717\n",
    "```\n",
    "**As it can be seen from above, both the results are identical with each other**\n",
    "\n",
    "* The Map/Reduce approach is much more scalable, even though it took longer to execute since there is a start up cost for Hadoop Map Reduce Jobs. As the file size gets bigger, the Map Reduce would perform much better than using PyFim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.4 (Conceptual Exercise)\n",
    "\n",
    "Suppose that you wished to perform the Apriori algorithm once again,\n",
    "though this time now with the goal of listing the top 5 rules with corresponding confidence scores \n",
    "in decreasing order of confidence score for itemsets of size 3 using Hadoop MapReduce.\n",
    "A rule is now of the form: \n",
    "\n",
    "(item1, item2) ⇒ item3 \n",
    "\n",
    "Recall that the Apriori algorithm is iterative for increasing itemset size,\n",
    "working off of the frequent itemsets of the previous size to explore \n",
    "ONLY the NECESSARY subset of a large combinatorial space. \n",
    "Describe how you might design a framework to perform this exercise.\n",
    "\n",
    "In particular, focus on the following:\n",
    "  — map-reduce steps required\n",
    "  - enumeration of item sets and filtering for frequent candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In order to solve this problem a chain of Hadoop Map Reduce Jobs need to be used. The design would be something as follows:\n",
    "\n",
    "**Input Document: D**\n",
    "```\n",
    "item1 item2 item3 item4 item5\n",
    "item1 item2 item4 item5\n",
    "item3 item4 item5\n",
    ".\n",
    ".\n",
    ".\n",
    "```\n",
    "\n",
    "#### 1st Pass:\n",
    "  * **Mapper**: Read Document line by line and emit items as (item, 1)\n",
    "  * **Reducer**: Sum the items and emit only those that are > support count (Ex: s=100). At the end of the first pass the reducer file (L1) would look like the following. Since the output from Reducer is not supposed to be a big file, the number of reducer would be passed as 1 to the Hadoop Job, so that only one output file is generated from this pass.\n",
    "  ```\n",
    "    item1, 102\n",
    "    item3, 105\n",
    "  ```\n",
    "  \n",
    "#### 2nd Pass:\n",
    "  * **Mapper**: The output file from 1st pass would be passed to the Mapper in the form of a Distributed Cache. Assuming this file is small (Typical Super Store doesn't have more than 10,000 or so frequently sold items) it is loaded into the memory of the m/c(s) runnning mappers. The mapper would load L1 into a Dictionary (Ex: key=item1, value=102 etc). It would also create a candidate generation C2 by taking all combination pairs from L1 and create another Dictionary where keys are candidate pairs;values can be anyting like '-'. C2 would look like the following:\n",
    "  ```\n",
    "  (item1, item2) -\n",
    "  (item1, item3) -\n",
    "  ```\n",
    "  The mapper would then read portion of the Document D passed to it, and emit only those pairs which are present in C2 (Candidates). Output of mapper would have key like \"item1,item3\" and value as \"1\".\n",
    "  * **Reducer**: Hadoop framework would make sure Reducer receives the values for the same keys. The job of the reducer would be to sum the values for same keys, and emit only those that are > support count (Ex: s=100), just like in the 1st pass. Since the output from Reducer is not supposed to be a very big file (Though it will be much bigger than the file from 1st pass), the number of reducer would be passed as 1 to the Hadoop Job, so that only one output file is generated from this pass. The output of the reducer (L2) would look like the following. \n",
    "  ```\n",
    "  item1,item2 150\n",
    "  item1,item3 240\n",
    "  ```\n",
    "  \n",
    "#### 3rd Pass:\n",
    "  * **Mapper**: The output file from 2nd pass would be passed to the Mapper in the form of a Distributed Cache. Assuming this file is small it is loaded into the memory, as a Dictionary, of the m/c(s) runnning mappers. The mapper would read portion of the Document D passed to it, read line by line, and emit the following as long as the line has the candidate pairs. The mapper would emit something like the following if a candidate pair is present in the Document D:\n",
    "  ```\n",
    "  item1,item2,* 150\n",
    "  item1,item2,item4 1\n",
    "  ```\n",
    "  * **Reducer**: Paritioning has to be done based on the leftmost item pair (item1,item2), which would ensure all records with the same item pair would be passed to the same reducer. And secondary sort would ensure that the reducer would get to read the total (item1,item2,* 150) before it starts calculating the confidence scores. Each reducer would need to store the confidence scores in memory to be able to spit out the top 5 once the processing is done. If the number of records exceeds the memory of the reducer, then top 5 has to be recomputed every time a confidence score is calculated by the reducer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
