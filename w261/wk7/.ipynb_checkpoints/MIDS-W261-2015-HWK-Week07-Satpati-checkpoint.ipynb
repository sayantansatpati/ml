{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sayantan Satpati**\n",
    "* **sayantan.satpati@ischool.berkeley.edu**\n",
    "* **W261**\n",
    "* **Week-7**\n",
    "* **Assignment-7**\n",
    "* **Date of Submission: 27-OCT-2015**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  === Week 7: Graph Processing ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 7.0\n",
    "---\n",
    "\n",
    "Shortest path graph distances (toy networks)===\n",
    "\n",
    "In this part of your assignment you will develop the base of your code for the week.\n",
    "\n",
    "Write MRJob classes to find shortest path graph distances, \n",
    "as described in the lectures. In addition to finding the distances, \n",
    "your code should also output a distance-minimizing path between the source and target.\n",
    "Work locally for this part of the assignment, and use \n",
    "both of the undirected and directed toy networks.\n",
    "\n",
    "To proof you code's function, run the following jobs\n",
    "\n",
    "- shortest path in the undirected network from node 1 to node 4\n",
    "Solution: 1,5,4 \n",
    "\n",
    "- shortest path in the directed network from node 1 to node 5\n",
    "Solution: 1,2,4,5\n",
    "\n",
    "and report your output---make sure it is correct!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!aws s3 cp s3://ucb-mids-mls-networks/undirected_toy.txt .\n",
    "!head undirected_toy.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!aws s3 cp s3://ucb-mids-mls-networks/directed_toy.txt .\n",
    "!head directed_toy.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob_sp_hw70.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob_sp_hw70.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.compat import get_jobconf_value\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "'''\n",
    "Record Emitted by Mapper/Reducer:\n",
    "Node <TAB> NULL|Neighbor Dict,Distance,Parent/Child,V|Q|U\n",
    "'''\n",
    "\n",
    "class ShortestPath(MRJob):\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper,\n",
    "                   reducer=self.reducer)\n",
    "        ]\n",
    "    \n",
    "    def val1(self,ngbr,dist,parent,child,state):\n",
    "        return '{0}|{1}|{2}/{3}|{4}'.format(ngbr,str(dist),parent,child,state)\n",
    "    \n",
    "    def val2(self,ngbr,dist,path,state):\n",
    "        return '{0}|{1}|{2}|{3}'.format(ngbr,str(dist),path,state)\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.frontier_node = get_jobconf_value('frontier_node')\n",
    "        #sys.stderr.write('### Frontier Node: {0}\\n'.format(self.frontier_node))\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        line = line.replace(\"\\\"\",\"\")\n",
    "        # Passed only to first iteration\n",
    "        t = line.strip().split('\\t')\n",
    "        node = t[0]\n",
    "        #sys.stderr.write('[M] {0}\\n'.format(line))\n",
    "        if self.frontier_node:\n",
    "            neighbors = ast.literal_eval(t[1])    \n",
    "            if node == self.frontier_node:\n",
    "                # Mark Node as Visited\n",
    "                yield node, self.val1(neighbors,0,node,node,'V')\n",
    "                \n",
    "                # Open/Emit Frontiers\n",
    "                for k,v in neighbors.iteritems():\n",
    "                    self.increment_counter('graph', 'frontiers', amount=1)\n",
    "                    yield k, self.val1('NULL',v,node,k,'Q')\n",
    "            else:\n",
    "                # Rest Passthrough\n",
    "                yield node, self.val1(neighbors,sys.maxint,'NULL','NULL','U')\n",
    "        else:\n",
    "            t1 = t[1].split(\"|\")\n",
    "            neighbors = {}\n",
    "            if t1[0] != 'NULL':\n",
    "                neighbors = ast.literal_eval(t1[0])\n",
    "            dist = t1[1]\n",
    "            path = t1[2]\n",
    "            status = t1[3]\n",
    "            if status == 'Q':\n",
    "                # Mark Node as Visited\n",
    "                yield node, self.val2(neighbors,dist,path,'V')\n",
    "                \n",
    "                # Open/Emit Frontiers\n",
    "                for k,v in neighbors.iteritems():\n",
    "                    self.increment_counter('graph', 'frontiers', amount=1)\n",
    "                    yield k, self.val1('NULL', int(dist) + int(v), path, k,'Q')\n",
    "            else:\n",
    "                yield t[0], t[1] # Passthrough (Rest)\n",
    "            \n",
    "    def combiner(self, key, counts):\n",
    "        pass\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        vList = [value for value in values]\n",
    "        \n",
    "        if len(vList) == 1:\n",
    "            yield key, vList[0]\n",
    "        else:\n",
    "            min_dist = sys.maxint\n",
    "            neighbors = None\n",
    "            dist = None\n",
    "            path = None\n",
    "            status = None\n",
    "            \n",
    "            for value in vList:\n",
    "                #sys.stderr.write('[R] {0} : {1}\\n'.format(key,value))\n",
    "                t = value.split(\"|\")\n",
    "\n",
    "                # Eliminate processing nodes which are already visited\n",
    "                if t[3] == 'V':\n",
    "                    neighbors = ast.literal_eval(t[0])\n",
    "                    dist = t[1]\n",
    "                    path = t[2]\n",
    "                    status = t[3]\n",
    "                \n",
    "                if t[3] == 'Q':\n",
    "                    # Take the shortest path\n",
    "                    if dist < min_dist:\n",
    "                        dist = t[1]\n",
    "                        path = t[2]\n",
    "                        status = t[3]\n",
    "                        min_dist = dist\n",
    "                        \n",
    "                if t[3] == 'U':\n",
    "                    neighbors = ast.literal_eval(t[0])\n",
    "                    \n",
    "        \n",
    "            yield key, self.val2(neighbors,dist,path,status)    \n",
    "            \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    ShortestPath.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x mrjob_sp_hw70.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "@@@@@ SHORTEST PATH ANALYSIS FOR: undirected_toy.txt\n",
      "\n",
      "iteration: 1:\n",
      "('1', \"{'2': 1, '5': 1}|0|1/1|V\")\n",
      "('2', \"{'1': 1, '3': 1, '5': 1, '4': 1}|1|1/2|Q\")\n",
      "('3', \"{'2': 1, '4': 1}|9223372036854775807|NULL/NULL|U\")\n",
      "('4', \"{'3': 1, '2': 1, '5': 1}|9223372036854775807|NULL/NULL|U\")\n",
      "('5', \"{'1': 1, '2': 1, '4': 1}|1|1/5|Q\")\n",
      "# Counters: [{'graph': {'frontiers': 2}}]\n",
      "iteration: 2:\n",
      "('1', \"{'2': 1, '5': 1}|0|1/1|V\")\n",
      "('2', \"{'1': 1, '3': 1, '5': 1, '4': 1}|1|1/2|V\")\n",
      "('3', \"{'2': 1, '4': 1}|2|1/2/3|Q\")\n",
      "('4', \"{'3': 1, '2': 1, '5': 1}|2|1/2/4|Q\")\n",
      "('5', \"{'1': 1, '2': 1, '4': 1}|1|1/5|V\")\n",
      "# Counters: [{'graph': {'frontiers': 7}}]\n",
      "iteration: 3:\n",
      "('1', \"{'2': 1, '5': 1}|0|1/1|V\")\n",
      "('2', \"{'1': 1, '3': 1, '5': 1, '4': 1}|1|1/2|V\")\n",
      "('3', \"{'2': 1, '4': 1}|2|1/2/3|V\")\n",
      "('4', \"{'3': 1, '2': 1, '5': 1}|2|1/2/4|V\")\n",
      "('5', \"{'1': 1, '2': 1, '4': 1}|1|1/5|V\")\n",
      "# Counters: [{'graph': {'frontiers': 5}}]\n",
      "iteration: 4:\n",
      "('1', \"{'2': 1, '5': 1}|0|1/1|V\")\n",
      "('2', \"{'1': 1, '3': 1, '5': 1, '4': 1}|1|1/2|V\")\n",
      "('3', \"{'2': 1, '4': 1}|2|1/2/3|V\")\n",
      "('4', \"{'3': 1, '2': 1, '5': 1}|2|1/2/4|V\")\n",
      "('5', \"{'1': 1, '2': 1, '4': 1}|1|1/5|V\")\n",
      "\n",
      "@@@@@ SHORTEST PATH ANALYSIS FOR: directed_toy.txt\n",
      "\n",
      "iteration: 1:\n",
      "('1', \"{'2': 1, '6': 1}|0|1/1|V\")\n",
      "('2', \"{'1': 1, '3': 1, '4': 1}|1|1/2|Q\")\n",
      "('3', \"{'2': 1, '4': 1}|9223372036854775807|NULL/NULL|U\")\n",
      "('4', \"{'2': 1, '5': 1}|9223372036854775807|NULL/NULL|U\")\n",
      "('5', \"{'1': 1, '2': 1, '4': 1}|9223372036854775807|NULL/NULL|U\")\n",
      "('6', 'NULL|1|1/6|Q')\n",
      "# Counters: [{'graph': {'frontiers': 2}}]\n",
      "iteration: 2:\n",
      "('1', \"{'2': 1, '6': 1}|0|1/1|V\")\n",
      "('2', \"{'1': 1, '3': 1, '4': 1}|1|1/2|V\")\n",
      "('3', \"{'2': 1, '4': 1}|2|1/2/3|Q\")\n",
      "('4', \"{'2': 1, '5': 1}|2|1/2/4|Q\")\n",
      "('5', \"{'1': 1, '2': 1, '4': 1}|9223372036854775807|NULL/NULL|U\")\n",
      "('6', '{}|1|1/6|V')\n",
      "# Counters: [{'graph': {'frontiers': 3}}]\n",
      "iteration: 3:\n",
      "('1', \"{'2': 1, '6': 1}|0|1/1|V\")\n",
      "('2', \"{'1': 1, '3': 1, '4': 1}|1|1/2|V\")\n",
      "('3', \"{'2': 1, '4': 1}|2|1/2/3|V\")\n",
      "('4', \"{'2': 1, '5': 1}|2|1/2/4|V\")\n",
      "('5', \"{'1': 1, '2': 1, '4': 1}|3|1/2/4/5|Q\")\n",
      "('6', '{}|1|1/6|V')\n",
      "# Counters: [{'graph': {'frontiers': 4}}]\n",
      "iteration: 4:\n",
      "('1', \"{'2': 1, '6': 1}|0|1/1|V\")\n",
      "('2', \"{'1': 1, '3': 1, '4': 1}|1|1/2|V\")\n",
      "('3', \"{'2': 1, '4': 1}|2|1/2/3|V\")\n",
      "('4', \"{'2': 1, '5': 1}|2|1/2/4|V\")\n",
      "('5', \"{'1': 1, '2': 1, '4': 1}|3|1/2/4/5|V\")\n",
      "('6', '{}|1|1/6|V')\n",
      "# Counters: [{'graph': {'frontiers': 3}}]\n",
      "iteration: 5:\n",
      "('1', \"{'2': 1, '6': 1}|0|1/1|V\")\n",
      "('2', \"{'1': 1, '3': 1, '4': 1}|1|1/2|V\")\n",
      "('3', \"{'2': 1, '4': 1}|2|1/2/3|V\")\n",
      "('4', \"{'2': 1, '5': 1}|2|1/2/4|V\")\n",
      "('5', \"{'1': 1, '2': 1, '4': 1}|3|1/2/4/5|V\")\n",
      "('6', '{}|1|1/6|V')\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from mrjob_sp_hw70 import ShortestPath\n",
    "\n",
    "files = ['undirected_toy.txt', 'directed_toy.txt']\n",
    "\n",
    "for input_file in files:\n",
    "    print '\\n@@@@@ SHORTEST PATH ANALYSIS FOR: {0}\\n'.format(input_file)\n",
    "    frontiers = 1\n",
    "    cnt = 0\n",
    "    while frontiers and frontiers > 0:\n",
    "        print \"iteration: \" + str(cnt+1) + \":\"\n",
    "\n",
    "        mr_job = None\n",
    "        if cnt == 0: # First Iteration\n",
    "            mr_job = ShortestPath(args=[input_file,\n",
    "                                '--jobconf', 'frontier_node=1',\n",
    "                                '--no-strict-protocol'])\n",
    "        else:\n",
    "            mr_job = ShortestPath(args=[input_file + '1',\n",
    "                                '--no-strict-protocol'])\n",
    "\n",
    "        with mr_job.make_runner() as runner: \n",
    "            runner.run()\n",
    "            # stream_output: get access of the output \n",
    "            with open(input_file + '1','w') as f:\n",
    "                for line in runner.stream_output():\n",
    "                    print mr_job.parse_output_line(line)\n",
    "                    f.write(line)\n",
    "\n",
    "            if 'graph' in runner.counters()[0]:\n",
    "                print \"# Counters: {0}\".format(runner.counters())\n",
    "                frontiers = runner.counters()[0]['graph']['frontiers']\n",
    "            else:\n",
    "                break;\n",
    "\n",
    "        cnt += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.1\n",
    "---\n",
    "\n",
    "***In the database world What is 3NF? Does machine learning use data in 3NF? If so why? ***\n",
    "\n",
    "* Third normal form is a normal form used in normalizing a database design to reduce the duplication of data and ensure referential integrity by ensuring that the entity is in second normal form and all the attributes in a table are determined only by the candidate keys of that table and not by any non-prime attributes.\n",
    "\n",
    "* ML requires data to be in one single tabular format. So, if a data set is in 3NF, it should be denormalized into 2NF.\n",
    "\n",
    "***In what form does ML consume data?***\n",
    "\n",
    "* Machine Learning algorithms require data to be into a single text file in tabular format, with each row representing a full instance of the input dataset and each column one of its features. Data is typically in normal form separated in tables: Ex: one for users, another for movies, and another for ratings. To get the date in machine-learning-ready format in this way we would need to join the tables and get it in a single tabular format (Ex: Join users with ratings): Second Normal Form.\n",
    "\n",
    "***Why would one use log files that are denormalized?***\n",
    "\n",
    "* Log files are denormalized for faster & easier processing of data. In absence of denormalized data, there can be severe challenges in finding out all the relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.2\n",
    "---\n",
    "\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, \n",
    "right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)\n",
    ":\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "```\n",
    "(1) Left joining Table Left with Table Right\n",
    "(2) Right joining Table Left with Table Right\n",
    "(3) Inner joining Table Left with Table Right\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "***File containing url(s) look like follows:***\n",
    "\n",
    "```\n",
    "[cloudera@localhost wk5]$ head -5 ../wk4/url \n",
    "A,1287,1,\"International AutoRoute\",\"/autoroute\"\n",
    "A,1288,1,\"library\",\"/library\"\n",
    "A,1289,1,\"Master Chef Product Information\",\"/masterchef\"\n",
    "A,1297,1,\"Central America\",\"/centroam\"\n",
    "A,1215,1,\"For Developers Only Info\",\"/developer\"\n",
    "```\n",
    "\n",
    "***File containing Page Visits by Customers look like follows:***\n",
    "\n",
    "```\n",
    "V,1000,1,C,10001\n",
    "V,1001,1,C,10001\n",
    "V,1002,1,C,10001\n",
    "V,1001,1,C,10002\n",
    "V,1003,1,C,10002\n",
    "```\n",
    "\n",
    "* For **INNER** and **LEFT** join, the url file is passed to the mrjob as a '--file' parameter; it is then loaded by the reducers in a dict for INNER and LEFT join. After determining the page Vists in descending order by customers in the first pass, the url details are added, using the url file, in the 2nd pass. In the case of a LEFT join, the page visits would output the URL as 'NA', if the 'url' file is missing any URL. In the case of an INNER join, the page visits would be output only if there is a macthing URL. Since no URL(s) are missing from the 'url' file, the result from LEFT & INNER joins are identical having 98654 rows. The type of join is passed to mrjob as a parameter.\n",
    "* For **RIGHT** join, we are supposed to show the page visits for all URLS present in the 'url' file, even though they have not been visited. For this one, we pass the 'pages visited file' as a '--file' parameter, and the url file as an input to the mrjob. Since there are urls which haven't been visited, the output 98663 rows, which is more than the previous 2 outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
