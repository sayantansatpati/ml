{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sayantan Satpati**\n",
    "* **sayantan.satpati@ischool.berkeley.edu**\n",
    "* **W261**\n",
    "* **Week-3**\n",
    "* **Assignment-4**\n",
    "* **Date of Submission: 22-SEP-2015**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  === Week 3 Hadoop & Apriori ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW3.0. What is a merge sort? Where is it used in Hadoop?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.1.\n",
    "---\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a program using the A-priori algorithm\n",
    "to find products which are frequently browsed together. Fix the support to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find itemsets of size 2 and 3.\n",
    "\n",
    "Use the online browsing behavior dataset at: \n",
    "\n",
    "https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Do some exploratory data analysis of this dataset. \n",
    "Report your findings such as number of unique products; largest basket, etc. using Hadoop Map-Reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_hw31.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_hw31.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        # Remove leading & trailing chars\n",
    "        line = line.strip()\n",
    "        # Split the line by <TAB> delimeter\n",
    "        items = re.split(r'\\s+', line)\n",
    "        #Sort the list\n",
    "        items.sort()\n",
    "        \n",
    "        for c in itertools.combinations(items, 1):\n",
    "            print '%s,%s\\t%d\\t%d' %(c[0], '*', 1, len(items))\n",
    "            \n",
    "        for c in itertools.combinations(items, 2):\n",
    "            print '%s,%s\\t%d' %(c[0], c[1], 1)\n",
    "        \n",
    "        ''' Commenting out itemset-3 for the moment\n",
    "        for c in itertools.combinations(items, 3):\n",
    "            print '%s,%s,%s\\t%d' %(c[0], c[1], c[2], 1)\n",
    "        '''\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper_hw31.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_hw31.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_hw31.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import heapq\n",
    "from sets import Set\n",
    "\n",
    "'''\n",
    "a1,* 1 4\n",
    "a1,* 1 5\n",
    "a1,b1 1\n",
    "a1,b1 1\n",
    "a1,b2 1\n",
    "a1,b2 1\n",
    "a2,* 1 6\n",
    "'''\n",
    "\n",
    "itemset_1_cnt = 0\n",
    "itemset_2_cnt = 0\n",
    "\n",
    "itemset_1_last = None\n",
    "itemset_2_last = None\n",
    "\n",
    "THRESHOLD = 100\n",
    "\n",
    "# Statistics\n",
    "# Unique Items\n",
    "uniq = Set()\n",
    "# Max Basket Length\n",
    "max_basket_len = 0\n",
    "# Total Itemset Counts for Sizes: 1 & 2\n",
    "total_itemset_1 = 0\n",
    "total_itemset_2 = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Remove leading & trailing chars\n",
    "        line = line.strip()\n",
    "        # Split the line by <TAB> delimeter\n",
    "        tokens = re.split(r'\\s+', line)\n",
    "    \n",
    "        # Split the key by <COMMA> delimeter\n",
    "        items = tokens[0].split(\",\")\n",
    "        i1 = items[0]\n",
    "        i2 = items[1]\n",
    "        \n",
    "        # Count\n",
    "        count = int(tokens[1])\n",
    "        \n",
    "        if itemset_1_last != i1:\n",
    "            uniq.add(i1)\n",
    "            \n",
    "            basket_len = int(tokens[2])\n",
    "            if basket_len >= max_basket_len:\n",
    "                max_basket_len = basket_len\n",
    "                \n",
    "            if itemset_1_cnt >= THRESHOLD:\n",
    "                total_itemset_1 += 1\n",
    "                \n",
    "            # Reset\n",
    "            itemset_1_last = i1\n",
    "            itemset_1_cnt = count\n",
    "            itemset_2_last = None\n",
    "            itemset_2_cnt = 0\n",
    "        else:\n",
    "            if i2 == '*':\n",
    "                itemset_1_cnt += count\n",
    "                \n",
    "                basket_len = int(tokens[2])\n",
    "                if basket_len > max_basket_len:\n",
    "                    max_basket_len = basket_len\n",
    "            else:\n",
    "                if itemset_2_last != tokens[0]:\n",
    "                    if itemset_1_cnt >= THRESHOLD and itemset_2_cnt >= THRESHOLD:\n",
    "                        total_itemset_2 += 1\n",
    "                        \n",
    "                    itemset_2_last = tokens[0]\n",
    "                    itemset_2_cnt = count\n",
    "                else:\n",
    "                    itemset_2_cnt += count\n",
    "                    \n",
    "# Last Set of Counts\n",
    "if itemset_1_cnt >= THRESHOLD:\n",
    "    total_itemset_1 += 1\n",
    "if itemset_1_cnt >= THRESHOLD and itemset_2_cnt >= THRESHOLD:\n",
    "    total_itemset_2 += 1\n",
    "\n",
    "print '=== Statistics ==='\n",
    "print 'Total Unique Items: %d' %(len(uniq))\n",
    "print 'Maximum Basket Length: %d' %(max_basket_len)\n",
    "print 'Total # itemsets of size 1: %d' %(total_itemset_1)\n",
    "print 'Total # itemsets of size 2: %d' %(total_itemset_2)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_hw31.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_hw31.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import heapq\n",
    "from sets import Set\n",
    "\n",
    "'''\n",
    "a1,* 1 4\n",
    "a1,* 1 5\n",
    "a1,b1 1\n",
    "a1,b1 1\n",
    "a1,b2 1\n",
    "a1,b2 1\n",
    "a2,* 1 6\n",
    "'''\n",
    "\n",
    "itemset_1_cnt = 0\n",
    "itemset_2_cnt = 0\n",
    "\n",
    "itemset_1_last = None\n",
    "\n",
    "THRESHOLD = 100\n",
    "\n",
    "# Statistics\n",
    "# Unique Items\n",
    "uniq = Set()\n",
    "# Max Basket Length\n",
    "max_basket_len = 0\n",
    "# Total Itemset Counts for Sizes: 1 & 2\n",
    "total_itemset_1 = 0\n",
    "total_itemset_2 = 0\n",
    "\n",
    "d_counts = {}\n",
    "\n",
    "def update_counts():\n",
    "    global itemset_1_last\n",
    "    global d_counts\n",
    "    global total_itemset_1\n",
    "    global total_itemset_2\n",
    "    key = '{0},{1}'.format(itemset_1_last, '*')\n",
    "    if d_counts.get(key, 0) >= THRESHOLD:\n",
    "        total_itemset_1 += 1\n",
    "        for k,v in d_counts.iteritems():\n",
    "            if k != key:\n",
    "                if v >= THRESHOLD:\n",
    "                    total_itemset_2 += 1\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Remove leading & trailing chars\n",
    "        line = line.strip()\n",
    "        # Split the line by <TAB> delimeter\n",
    "        tokens = re.split(r'\\s+', line)\n",
    "    \n",
    "        # Split the key by <COMMA> delimeter\n",
    "        items = tokens[0].split(\",\")\n",
    "        i1 = items[0]\n",
    "        i2 = items[1]\n",
    "        \n",
    "        # Count\n",
    "        count = int(tokens[1])\n",
    "        \n",
    "        if not itemset_1_last:\n",
    "            itemset_1_last = i1\n",
    "        \n",
    "        if itemset_1_last != i1:\n",
    "            # Emit Contents of Dict\n",
    "            update_counts()\n",
    "            \n",
    "            if i2 == '*':\n",
    "                uniq.add(i1)\n",
    "                basket_len = int(tokens[2])\n",
    "                if basket_len > max_basket_len:\n",
    "                    max_basket_len = basket_len\n",
    "                            \n",
    "            d_counts.clear()\n",
    "            itemset_1_last = i1\n",
    "        else:\n",
    "            key = tokens[0]\n",
    "            d_counts[key] = d_counts.get(key, 0) + count\n",
    "            \n",
    "            if i2 == '*':\n",
    "                uniq.add(i1)\n",
    "                basket_len = int(tokens[2])\n",
    "                if basket_len > max_basket_len:\n",
    "                    max_basket_len = basket_len\n",
    "                    \n",
    "# Last Record\n",
    "update_counts()\n",
    "                    \n",
    "print '=== Statistics ==='\n",
    "print 'Total Unique Items: %d' %(len(uniq))\n",
    "print 'Maximum Basket Length: %d' %(max_basket_len)\n",
    "print 'Total # itemsets of size 1: %d' %(total_itemset_1)\n",
    "print 'Total # itemsets of size 2: %d' %(total_itemset_2)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer_hw31.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/cloudera/w261/wk3/hw31/output\n",
      "packageJobJar: [./mapper_hw31.py, ./reducer_hw31.py, /tmp/hadoop-cloudera/hadoop-unjar4802976138694835527/] [] /tmp/streamjob371666725523987824.jar tmpDir=null\n",
      "15/09/20 12:15:58 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n",
      "15/09/20 12:15:58 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/20 12:15:59 INFO streaming.StreamJob: getLocalDirs(): [/tmp/hadoop-cloudera/mapred/local]\n",
      "15/09/20 12:15:59 INFO streaming.StreamJob: Running job: job_201509191045_0012\n",
      "15/09/20 12:15:59 INFO streaming.StreamJob: To kill this job, run:\n",
      "15/09/20 12:15:59 INFO streaming.StreamJob: UNDEF/bin/hadoop job  -Dmapred.job.tracker=localhost.localdomain:8021 -kill job_201509191045_0012\n",
      "15/09/20 12:15:59 INFO streaming.StreamJob: Tracking URL: http://0.0.0.0:50030/jobdetails.jsp?jobid=job_201509191045_0012\n",
      "15/09/20 12:16:00 INFO streaming.StreamJob:  map 0%  reduce 0%\n",
      "15/09/20 12:16:15 INFO streaming.StreamJob:  map 30%  reduce 0%\n",
      "15/09/20 12:16:16 INFO streaming.StreamJob:  map 57%  reduce 0%\n",
      "15/09/20 12:16:18 INFO streaming.StreamJob:  map 72%  reduce 0%\n",
      "15/09/20 12:16:19 INFO streaming.StreamJob:  map 91%  reduce 0%\n",
      "15/09/20 12:16:22 INFO streaming.StreamJob:  map 100%  reduce 0%\n",
      "15/09/20 12:16:33 INFO streaming.StreamJob:  map 100%  reduce 68%\n",
      "15/09/20 12:16:35 INFO streaming.StreamJob:  map 100%  reduce 69%\n",
      "15/09/20 12:16:39 INFO streaming.StreamJob:  map 100%  reduce 70%\n",
      "15/09/20 12:16:42 INFO streaming.StreamJob:  map 100%  reduce 71%\n",
      "15/09/20 12:16:45 INFO streaming.StreamJob:  map 100%  reduce 72%\n",
      "15/09/20 12:16:46 INFO streaming.StreamJob:  map 100%  reduce 100%\n",
      "15/09/20 12:16:49 INFO streaming.StreamJob: Job complete: job_201509191045_0012\n",
      "15/09/20 12:16:49 INFO streaming.StreamJob: Output: /user/cloudera/w261/wk3/hw31/output\n",
      "=== Statistics ===\t\n",
      "Total Unique Items: 12592\t\n",
      "Maximum Basket Length: 37\t\n",
      "Total # itemsets of size 1: 642\t\n",
      "Total # itemsets of size 2: 1334\t\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HW3.1. Product Recommendations\n",
    "'''\n",
    "\n",
    "# Delete existing Output Dirs if available\n",
    "!hadoop fs -rm -r -skipTrash /user/cloudera/w261/wk3/hw31/output\n",
    "\n",
    "# Run the Hadoop Streaming Command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.7.0.jar \\\n",
    "-D map.output.key.field.separator=, \\\n",
    "-D mapred.text.key.partitioner.options=-k1,1 \\\n",
    "-input /user/cloudera/w261/wk3/hw31/input/ProductPurchaseData.txt \\\n",
    "-output /user/cloudera/w261/wk3/hw31/output \\\n",
    "-file ./mapper_hw31.py \\\n",
    "-mapper 'python mapper_hw31.py' \\\n",
    "-file ./reducer_hw31.py \\\n",
    "-reducer 'python reducer_hw31.py'\n",
    "\n",
    "# Show Output\n",
    "!hadoop fs -cat /user/cloudera/w261/wk3/hw31/output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.2. (Computationally prohibitive but then again Hadoop can handle this)\n",
    "---\n",
    "\n",
    "Note: for this part the writeup will require a specific rule ordering but the program need not sort the output.\n",
    "\n",
    "List the top 5 rules with corresponding confidence scores in decreasing order of confidence score \n",
    "for frequent (100>count) itemsets of size 2. \n",
    "A rule is of the form: \n",
    "\n",
    "(item1) ⇒ item2.\n",
    "\n",
    "Fix the ordering of the rule lexicographically (left to right), \n",
    "and break ties in confidence (between rules, if any exist) \n",
    "by taking the first ones in lexicographically increasing order. \n",
    "Use Hadoop MapReduce to complete this part of the assignment; \n",
    "use a single mapper and single reducer; use a combiner if you think it will help and justify. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_hw32.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_hw32.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        # Remove leading & trailing chars\n",
    "        line = line.strip()\n",
    "        # Split the line by <TAB> delimeter\n",
    "        items = re.split(r'\\s+', line)\n",
    "        #Sort the list\n",
    "        items.sort()\n",
    "        \n",
    "        l = len(items)\n",
    "        \n",
    "        for i in xrange(l):\n",
    "            print '%s,*\\t%d' %(items[i], 1)\n",
    "            for j in xrange(i+1, l):\n",
    "               print '%s,%s\\t%d' %(items[i], items[j], 1) \n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper_hw32.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_hw32.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_hw32.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import heapq\n",
    "\n",
    "itemset_1_cnt = 0\n",
    "itemset_2_cnt = 0\n",
    "\n",
    "itemset_1_last = None\n",
    "itemset_2_last = None\n",
    "\n",
    "'''\n",
    "a1,* 1\n",
    "a1,* 1\n",
    "a1,b1 1\n",
    "a1,b1 1\n",
    "a1,b2 1\n",
    "a1,b2 1\n",
    "a2,* 1\n",
    "'''\n",
    "\n",
    "THRESHOLD = 100\n",
    "# Store Itemsets 2\n",
    "dict = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Remove leading & trailing chars\n",
    "        line = line.strip()\n",
    "        # Split the line by <TAB> delimeter\n",
    "        tokens = re.split(r'\\s+', line)\n",
    "    \n",
    "        # Split the key by <COMMA> delimeter\n",
    "        items = tokens[0].split(\",\")\n",
    "        i1 = items[0]\n",
    "        i2 = items[1]\n",
    "        \n",
    "        if not itemset_1_last:\n",
    "            itemset_1_last = i1\n",
    "        \n",
    "        if itemset_1_last != i1:\n",
    "            '''\n",
    "            if itemset_1_cnt >= THRESHOLD:\n",
    "                confidence = (itemset_2_cnt * 1.0) / itemset_1_cnt\n",
    "                print '[%d,%d]%s\\t%f' %(itemset_1_cnt, itemset_2_cnt, tokens[0], confidence)\n",
    "                dict[tokens[0]] = confidence\n",
    "            '''\n",
    "                        \n",
    "            # Reset\n",
    "            itemset_1_last = i1\n",
    "            itemset_1_cnt = int(tokens[1])\n",
    "            itemset_2_last = None\n",
    "            itemset_2_cnt = 0\n",
    "        else:\n",
    "            if i2 == '*':\n",
    "                itemset_1_cnt += int(tokens[1])\n",
    "            else:\n",
    "                if itemset_2_last != tokens[0]:\n",
    "                    if itemset_1_cnt >= THRESHOLD and itemset_2_cnt >= THRESHOLD:\n",
    "                        confidence = (itemset_2_cnt * 1.0) / itemset_1_cnt\n",
    "                        #print '[%d,%d]%s\\t%f' %(itemset_1_cnt, itemset_2_cnt, itemset_2_last, confidence)\n",
    "                        dict[itemset_2_last] = confidence\n",
    "                    itemset_2_last = tokens[0]\n",
    "                    itemset_2_cnt = int(tokens[1]) \n",
    "                else:\n",
    "                    itemset_2_cnt += int(tokens[1])                    \n",
    "\n",
    "# Last Set of Counts\n",
    "if itemset_1_cnt >= THRESHOLD and itemset_2_cnt >= THRESHOLD:\n",
    "    confidence = (itemset_2_cnt * 1.0) / itemset_1_cnt\n",
    "    #print '[%d,%d]%s\\t%f' %(itemset_1_cnt, itemset_2_cnt, itemset_2_last, confidence)\n",
    "    dict[itemset_2_last] = confidence\n",
    "\n",
    "print '=== Top 5 Confidence ==='\n",
    "sorted_dict = sorted(dict.items(), key=lambda x:(-x[1], x[0]))\n",
    "for j,k in sorted_dict[:5]:\n",
    "    print '%s\\t%f' %(j,k)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer_hw32.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/cloudera/w261/wk3/hw32/output\n",
      "packageJobJar: [./mapper_hw32.py, ./reducer_hw32.py, /tmp/hadoop-cloudera/hadoop-unjar8486018449480148677/] [] /tmp/streamjob286663036493526101.jar tmpDir=null\n",
      "15/09/20 13:08:58 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n",
      "15/09/20 13:08:58 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/20 13:08:58 INFO streaming.StreamJob: getLocalDirs(): [/tmp/hadoop-cloudera/mapred/local]\n",
      "15/09/20 13:08:58 INFO streaming.StreamJob: Running job: job_201509191045_0014\n",
      "15/09/20 13:08:58 INFO streaming.StreamJob: To kill this job, run:\n",
      "15/09/20 13:08:58 INFO streaming.StreamJob: UNDEF/bin/hadoop job  -Dmapred.job.tracker=localhost.localdomain:8021 -kill job_201509191045_0014\n",
      "15/09/20 13:08:59 INFO streaming.StreamJob: Tracking URL: http://0.0.0.0:50030/jobdetails.jsp?jobid=job_201509191045_0014\n",
      "15/09/20 13:09:00 INFO streaming.StreamJob:  map 0%  reduce 0%\n",
      "15/09/20 13:09:17 INFO streaming.StreamJob:  map 53%  reduce 0%\n",
      "15/09/20 13:09:20 INFO streaming.StreamJob:  map 83%  reduce 0%\n",
      "15/09/20 13:09:23 INFO streaming.StreamJob:  map 100%  reduce 0%\n",
      "15/09/20 13:09:34 INFO streaming.StreamJob:  map 100%  reduce 68%\n",
      "15/09/20 13:09:37 INFO streaming.StreamJob:  map 100%  reduce 69%\n",
      "15/09/20 13:09:40 INFO streaming.StreamJob:  map 100%  reduce 70%\n",
      "15/09/20 13:09:43 INFO streaming.StreamJob:  map 100%  reduce 71%\n",
      "15/09/20 13:09:46 INFO streaming.StreamJob:  map 100%  reduce 72%\n",
      "15/09/20 13:09:49 INFO streaming.StreamJob:  map 100%  reduce 100%\n",
      "15/09/20 13:09:52 INFO streaming.StreamJob: Job complete: job_201509191045_0014\n",
      "15/09/20 13:09:52 INFO streaming.StreamJob: Output: /user/cloudera/w261/wk3/hw32/output\n",
      "=== Top 5 Confidence ===\t\n",
      "DAI93865,FRO40251\t1.000000\n",
      "ELE12951,FRO40251\t0.990566\n",
      "DAI88079,FRO40251\t0.986726\n",
      "DAI43868,SNA82528\t0.972973\n",
      "DAI23334,DAI62779\t0.954545\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HW3.2. Confidence Caculations\n",
    "List the top 5 rules with corresponding confidence scores in decreasing order of confidence score \n",
    "for frequent (100>count) itemsets of size 2\n",
    "'''\n",
    "\n",
    "# Delete existing Output Dirs if available\n",
    "!hadoop fs -rm -r -skipTrash /user/cloudera/w261/wk3/hw32/output\n",
    "\n",
    "# Run the Hadoop Streaming Command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.7.0.jar \\\n",
    "-D map.output.key.field.separator=, \\\n",
    "-D mapred.text.key.partitioner.options=-k1,1 \\\n",
    "-input /user/cloudera/w261/wk3/hw32/input/ProductPurchaseData.txt \\\n",
    "-output /user/cloudera/w261/wk3/hw32/output \\\n",
    "-file ./mapper_hw32.py \\\n",
    "-mapper 'python mapper_hw32.py' \\\n",
    "-file ./reducer_hw32.py \\\n",
    "-reducer 'python reducer_hw32.py'\n",
    "\n",
    "# Show Output\n",
    "!hadoop fs -cat /user/cloudera/w261/wk3/hw32/output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.3\n",
    "---\n",
    "\n",
    "Benchmark your results using the pyFIM implementation of the Apriori algorithm\n",
    "(Apriori - Association Rule Induction / Frequent Item Set Mining implemented by Christian Borgelt). \n",
    "You can download pyFIM from here:***\n",
    "\n",
    "http://www.borgelt.net/pyfim.html\n",
    "\n",
    "Comment on the results from both implementations (your Hadoop MapReduce of apriori versus pyFIM) \n",
    "in terms of results and execution times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this part, the following steps were performed:\n",
    "\n",
    "1. Since I was using a Mac, I spinned up a Ubuntu 1404 (Micro) VM in Amazon EC2 Cluster\n",
    "2. Installed all required libraries: pip, git, ipython\n",
    "3. Downloaded the fim.so file from the link & set PYTHONPATH & LD_LIBRARY_PATH\n",
    "4. Ran the following command in the VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python top5pyfim.py | sort -n -r -k 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "FRO40251\t('DAI93865',)\t1.0\n",
    "FRO40251\t('GRO85051',)\t0.999176276771\n",
    "FRO40251\t('GRO38636',)\t0.990654205607\n",
    "FRO40251\t('ELE12951',)\t0.990566037736\n",
    "FRO40251\t('DAI88079',)\t0.986725663717\n",
    "FRO40251\t('FRO92469',)\t0.983510011779\n",
    "SNA82528\t('DAI43868',)\t0.972972972973\n",
    "DAI62779\t('DAI23334',)\t0.954545454545\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.4 (Conceptual Exercise)\n",
    "\n",
    "Suppose that you wished to perform the Apriori algorithm once again,\n",
    "though this time now with the goal of listing the top 5 rules with corresponding confidence scores \n",
    "in decreasing order of confidence score for itemsets of size 3 using Hadoop MapReduce.\n",
    "A rule is now of the form: \n",
    "\n",
    "(item1, item2) ⇒ item3 \n",
    "\n",
    "Recall that the Apriori algorithm is iterative for increasing itemset size,\n",
    "working off of the frequent itemsets of the previous size to explore \n",
    "ONLY the NECESSARY subset of a large combinatorial space. \n",
    "Describe how you might design a framework to perform this exercise.\n",
    "\n",
    "In particular, focus on the following:\n",
    "  — map-reduce steps required\n",
    "  - enumeration of item sets and filtering for frequent candidates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
