{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sayantan Satpati**\n",
    "* **sayantan.satpati@ischool.berkeley.edu**\n",
    "* **W261**\n",
    "* **Week-2**\n",
    "* **Assignment-3**\n",
    "* **Date of Submission: 15-SEP-2015**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  === Week 2 ASSIGNMENTS using Hadoop Streaming and Python ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW2.0. What is a race condition in the context of parallel computation? Give an example.**\n",
    "\n",
    "In the context of parallel computation, a race condition signifies a programming fault producing undetermined program state and behavior due to un-synchronized parallel program executions. One example can be a shared variable in the memory - for example a HashMap - which is written to and read from by multiple threads.\n",
    "\n",
    "**What is MapReduce?**\n",
    "\n",
    "MapReduce is a programming model and an associated implementation for processing and generating large data sets with a parallel, distributed algorithm on a cluster. \n",
    "\n",
    "**How does it differ from Hadoop?**\n",
    "\n",
    "Google MapReduce and Hadoop are two different implementations of the MapReduce framework/concept. Hadoop is open source while Google MapReduce is not, and actually there are not so many available details about it. However, the basic undelying principles of both are the same.\n",
    "\n",
    "**Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running.**\n",
    "\n",
    "Hadoop is based on the paradigm of divide and conquer - dividing a large problem into chunks which can be solved in parallel, by moving compute to data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting random_num_generation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile random_num_generation.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "from random import randint\n",
    "with open('random.txt', 'w') as f:\n",
    "    for i in xrange(0,10000):\n",
    "        r_number = randint(0,10000)\n",
    "        f.write('{0},\"NA\"\\n'.format(r_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x random_num_generation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing identity_map_red_hw21.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile identity_map_red_hw21.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    tokens = line.strip().split(\",\")\n",
    "    print \"%s\" %(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x identity_map_red_hw21.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/cloudera/w261/wk2/hw21/output\n",
      "packageJobJar: [./identity_map_red_hw21.py, ./identity_map_red_hw21.py, /tmp/hadoop-cloudera/hadoop-unjar3253954625343120918/] [] /tmp/streamjob6843559487058912219.jar tmpDir=null\n",
      "15/09/13 21:25:23 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n",
      "15/09/13 21:25:23 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/13 21:25:24 INFO streaming.StreamJob: getLocalDirs(): [/tmp/hadoop-cloudera/mapred/local]\n",
      "15/09/13 21:25:24 INFO streaming.StreamJob: Running job: job_201509131822_0018\n",
      "15/09/13 21:25:24 INFO streaming.StreamJob: To kill this job, run:\n",
      "15/09/13 21:25:24 INFO streaming.StreamJob: UNDEF/bin/hadoop job  -Dmapred.job.tracker=localhost.localdomain:8021 -kill job_201509131822_0018\n",
      "15/09/13 21:25:24 INFO streaming.StreamJob: Tracking URL: http://0.0.0.0:50030/jobdetails.jsp?jobid=job_201509131822_0018\n",
      "15/09/13 21:25:25 INFO streaming.StreamJob:  map 0%  reduce 0%\n",
      "15/09/13 21:25:36 INFO streaming.StreamJob:  map 50%  reduce 0%\n",
      "15/09/13 21:25:37 INFO streaming.StreamJob:  map 100%  reduce 0%\n",
      "15/09/13 21:25:46 INFO streaming.StreamJob:  map 100%  reduce 100%\n",
      "15/09/13 21:25:49 INFO streaming.StreamJob: Job complete: job_201509131822_0018\n",
      "15/09/13 21:25:49 INFO streaming.StreamJob: Output: /user/cloudera/w261/wk2/hw21/output\n",
      "0\t\n",
      "2\t\n",
      "2\t\n",
      "3\t\n",
      "3\t\n",
      "3\t\n",
      "4\t\n",
      "6\t\n",
      "7\t\n",
      "9\t\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HW2.1. Sort in Hadoop MapReduce\n",
    "'''\n",
    "\n",
    "# Delete existing Output Dirs if available\n",
    "!hadoop fs -rm -r -skipTrash /user/cloudera/w261/wk2/hw21/output\n",
    "\n",
    "# Run the Hadoop Streaming Command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.7.0.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D  mapred.text.key.comparator.options=-n \\\n",
    "-input /user/cloudera/w261/wk2/hw21/input/random.txt \\\n",
    "-output /user/cloudera/w261/wk2/hw21/output \\\n",
    "-file ./identity_map_red_hw21.py \\\n",
    "-mapper ./identity_map_red_hw21.py \\\n",
    "-file ./identity_map_red_hw21.py \\\n",
    "-reducer ./identity_map_red_hw21.py\n",
    "\n",
    "# Show Output\n",
    "!hadoop fs -cat /user/cloudera/w261/wk2/hw21/output/part-00000 | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_hw22.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_hw22.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def strip_special_chars(word):\n",
    "    return re.sub('[^A-Za-z0-9]+', '', word)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        # Remove leading & trailing chars\n",
    "        line = line.strip()\n",
    "        # Split the line by <TAB> delimeter\n",
    "        email = re.split(r'\\t+', line)\n",
    "\n",
    "        # Check whether Content is present\n",
    "        if len(email) < 4:\n",
    "            continue\n",
    "\n",
    "        # Get the content as a list of words\n",
    "        content = email[len(email) - 1].split()\n",
    "\n",
    "        for w in content:\n",
    "            w = strip_special_chars(w)\n",
    "            if w == 'assistance':\n",
    "                print '%s\\t%d' % (w, 1)\n",
    "    except Exception as e:\n",
    "        print line\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper_hw22.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_hw22.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_hw22.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "word = None\n",
    "count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Remove leading & trailing chars\n",
    "    line = line.strip()\n",
    "    # Split the line by <TAB> delimeter\n",
    "    wc = re.split(r'\\t+', line)\n",
    "    \n",
    "    word = wc[0]\n",
    "    count += int(wc[1])\n",
    "    \n",
    "print '%s\\t%d' % (word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer_hw22.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/cloudera/w261/wk2/hw22/output\n",
      "-rwxrwxr-x 1 cloudera cloudera 695 Sep 13 23:09 mapper_hw22.py\n",
      "-rwxrwxr-x 1 cloudera cloudera 307 Sep 13 23:09 reducer_hw22.py\n",
      "packageJobJar: [./mapper_hw22.py, ./reducer_hw22.py, /tmp/hadoop-cloudera/hadoop-unjar5247318265398782487/] [] /tmp/streamjob9044651651929209018.jar tmpDir=null\n",
      "15/09/13 23:09:58 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n",
      "15/09/13 23:09:58 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/13 23:09:59 INFO streaming.StreamJob: getLocalDirs(): [/tmp/hadoop-cloudera/mapred/local]\n",
      "15/09/13 23:09:59 INFO streaming.StreamJob: Running job: job_201509131822_0033\n",
      "15/09/13 23:09:59 INFO streaming.StreamJob: To kill this job, run:\n",
      "15/09/13 23:09:59 INFO streaming.StreamJob: UNDEF/bin/hadoop job  -Dmapred.job.tracker=localhost.localdomain:8021 -kill job_201509131822_0033\n",
      "15/09/13 23:09:59 INFO streaming.StreamJob: Tracking URL: http://0.0.0.0:50030/jobdetails.jsp?jobid=job_201509131822_0033\n",
      "15/09/13 23:10:00 INFO streaming.StreamJob:  map 0%  reduce 0%\n",
      "15/09/13 23:10:10 INFO streaming.StreamJob:  map 100%  reduce 0%\n",
      "15/09/13 23:10:17 INFO streaming.StreamJob:  map 100%  reduce 100%\n",
      "15/09/13 23:10:20 INFO streaming.StreamJob: Job complete: job_201509131822_0033\n",
      "15/09/13 23:10:20 INFO streaming.StreamJob: Output: /user/cloudera/w261/wk2/hw22/output\n",
      "assistance\t9\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HW2.2. Using the Enron data from HW1 and Hadoop MapReduce streaming, \n",
    "write mapper/reducer pair that  will determine the number of occurrences of a single, \n",
    "user-specified word. Examine the word “assistance” and report your results.\n",
    "'''\n",
    "\n",
    "# Delete existing Output Dirs if available\n",
    "!hadoop fs -rm -r -skipTrash /user/cloudera/w261/wk2/hw22/output\n",
    "\n",
    "!ls -l *hw22.py\n",
    "\n",
    "# Run the Hadoop Streaming Command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.7.0.jar \\\n",
    "-D mapred.reduce.tasks = 1 \\\n",
    "-input /user/cloudera/w261/wk2/hw22/input/enronemail_1h.txt \\\n",
    "-output /user/cloudera/w261/wk2/hw22/output \\\n",
    "-file ./mapper_hw22.py \\\n",
    "-mapper 'python mapper_hw22.py' \\\n",
    "-file ./reducer_hw22.py \\\n",
    "-reducer 'python reducer_hw22.py'\n",
    "\n",
    "# Show Output\n",
    "!hadoop fs -cat /user/cloudera/w261/wk2/hw22/output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_hw23.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_hw23.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Output from mapp\n",
    "vocab = set()\n",
    "word_counts = {\n",
    "    \"1\": {},\n",
    "    \"0\": {}\n",
    "}\n",
    "total = 0\n",
    "total_spam = 0\n",
    "total_ham = 0\n",
    "\n",
    "word_list = os.environ['WORDS'].split(\",\")\n",
    "\n",
    "def strip_special_chars(word):\n",
    "    word = word.strip().lower()\n",
    "    return re.sub('[^A-Za-z0-9]+', '', word)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        # Remove leading & trailing chars\n",
    "        line = line.strip()\n",
    "        # Split the line by <TAB> delimeter\n",
    "        email = re.split(r'\\t+', line)\n",
    "\n",
    "        # Check whether Content is present\n",
    "        if len(email) < 4:\n",
    "            continue\n",
    "\n",
    "        # Get the content as a list of words\n",
    "        spam = email[1]\n",
    "        content = email[len(email) - 1].split()\n",
    "        \n",
    "        # Totals\n",
    "        total += 1\n",
    "        if spam == '1':\n",
    "            total_spam += 1\n",
    "        else:\n",
    "            total_ham += 1\n",
    "\n",
    "        for w in content:\n",
    "            w = strip_special_chars(w)\n",
    "            \n",
    "            # Add to category dict\n",
    "            word_counts[spam][w] = word_counts[spam].get(w, 0.0) + 1.0\n",
    "                \n",
    "            # Vocab Unique\n",
    "            vocab.add(w)\n",
    "    except Exception as e:\n",
    "        print line\n",
    "        print e\n",
    "        \n",
    "print 'TOTAL_DOCUMENTS\\t%d\\t%d\\t%d' % (total,total_spam,total_ham)\n",
    "print 'TOTAL_WORDS\\t%d\\t%d\\t%d' % (len(vocab), sum(word_counts['1'].values()), sum(word_counts['0'].values()))\n",
    "for w in word_list:\n",
    "    print '%s\\t%d\\t%d' %(w, word_counts['1'].get(w, 0.0), word_counts['0'].get(w, 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper_hw23.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_hw23.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_hw23.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "\n",
    "# Totals from Mapper\n",
    "total = 0\n",
    "total_spam = 0\n",
    "total_ham = 0\n",
    "\n",
    "vocab = 0\n",
    "vocab_spam = 0\n",
    "vocab_ham = 0\n",
    "word_count = {}\n",
    "\n",
    "word_list = os.environ['WORDS'].split(\",\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "         # Remove leading & trailing chars\n",
    "        line = line.strip()\n",
    "        # Split the line by <TAB> delimeter\n",
    "        tokens = re.split(r'\\t+', line)\n",
    "        \n",
    "        if tokens[0] == 'TOTAL_DOCUMENTS':\n",
    "            total += float(tokens[1])\n",
    "            total_spam += float(tokens[2])\n",
    "            total_ham += float(tokens[3])\n",
    "        elif tokens[0] == 'TOTAL_WORDS':\n",
    "            vocab = float(tokens[1])\n",
    "            vocab_spam = float(tokens[2])\n",
    "            vocab_ham = float(tokens[3])\n",
    "        else:\n",
    "            word_count[tokens[0]] = (float(tokens[1]), float(tokens[2]))\n",
    "    except Exception as e:\n",
    "        sys.exit(1)\n",
    "\n",
    "prior_spam = total_spam / total\n",
    "prior_ham = total_ham / total\n",
    "\n",
    "spam_lhood_denom = vocab_spam + vocab\n",
    "ham_lhood_denom = vocab_ham + vocab\n",
    "spam_lhood_log = 0.0\n",
    "ham_lhood_log = 0.0\n",
    "for w in word_list:\n",
    "    spam_lhood_log += math.log( (word_count[w][0] + 1.0) / spam_lhood_denom )\n",
    "    ham_lhood_log += math.log( (word_count[w][1] + 1.0) / ham_lhood_denom )\n",
    "spam_score = spam_lhood_log + math.log(prior_spam)\n",
    "ham_score = ham_lhood_log + math.log(prior_ham)\n",
    "\n",
    "classification = 'HAM'\n",
    "if spam_score > ham_score:\n",
    "    classification = 'SPAM'\n",
    "   \n",
    "print '#<Feature>\\t<Spam_Score>\\t<Ham_Score>\\t<Predicted_Class>'\n",
    "print '%s\\t%d\\t%d\\t%s' %(\",\".join(word_list), spam_score, ham_score, classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer_hw23.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/cloudera/w261/wk2/hw23/output': No such file or directory\n",
      "-rwxrwxr-x 1 cloudera cloudera 1468 Sep 14 11:41 mapper_hw23.py\n",
      "-rwxrwxr-x 1 cloudera cloudera 1570 Sep 14 11:42 reducer_hw23.py\n",
      "packageJobJar: [./mapper_hw23.py, ./reducer_hw23.py, /tmp/hadoop-cloudera/hadoop-unjar58141088971362535/] [] /tmp/streamjob1929000916472760381.jar tmpDir=null\n",
      "15/09/14 11:44:31 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n",
      "15/09/14 11:44:31 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/14 11:44:32 INFO streaming.StreamJob: getLocalDirs(): [/tmp/hadoop-cloudera/mapred/local]\n",
      "15/09/14 11:44:32 INFO streaming.StreamJob: Running job: job_201509131822_0037\n",
      "15/09/14 11:44:32 INFO streaming.StreamJob: To kill this job, run:\n",
      "15/09/14 11:44:32 INFO streaming.StreamJob: UNDEF/bin/hadoop job  -Dmapred.job.tracker=localhost.localdomain:8021 -kill job_201509131822_0037\n",
      "15/09/14 11:44:32 INFO streaming.StreamJob: Tracking URL: http://0.0.0.0:50030/jobdetails.jsp?jobid=job_201509131822_0037\n",
      "15/09/14 11:44:33 INFO streaming.StreamJob:  map 0%  reduce 0%\n",
      "15/09/14 11:44:43 INFO streaming.StreamJob:  map 100%  reduce 0%\n",
      "15/09/14 11:44:49 INFO streaming.StreamJob:  map 100%  reduce 100%\n",
      "15/09/14 11:44:53 INFO streaming.StreamJob: Job complete: job_201509131822_0037\n",
      "15/09/14 11:44:53 INFO streaming.StreamJob: Output: /user/cloudera/w261/wk2/hw23/output\n",
      "#<Feature>\t<Spam_Score>\t<Ham_Score>\t<Predicted_Class>\n",
      "assistance\t-8\t-10\tSPAM\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HW2.3. Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer pair that\n",
    "   will classify the email messages by a single, user-specified word. \n",
    "   Examine the word “assistance” and report your results.\n",
    "'''\n",
    "\n",
    "# Delete existing Output Dirs if available\n",
    "!hadoop fs -rm -r -skipTrash /user/cloudera/w261/wk2/hw23/output\n",
    "\n",
    "!ls -l *hw23.py\n",
    "\n",
    "# Run the Hadoop Streaming Command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.7.0.jar \\\n",
    "-D mapred.reduce.tasks = 1 \\\n",
    "-input /user/cloudera/w261/wk2/hw23/input/enronemail_1h.txt \\\n",
    "-output /user/cloudera/w261/wk2/hw23/output \\\n",
    "-file ./mapper_hw23.py \\\n",
    "-mapper 'python mapper_hw23.py' \\\n",
    "-file ./reducer_hw23.py \\\n",
    "-reducer 'python reducer_hw23.py' \\\n",
    "-cmdenv WORDS='assistance' \\\n",
    "\n",
    "# Show Output\n",
    "!hadoop fs -cat /user/cloudera/w261/wk2/hw23/output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/cloudera/w261/wk2/hw24/output\n",
      "-rwxrwxr-x 1 cloudera cloudera 1468 Sep 14 11:41 mapper_hw23.py\n",
      "-rwxrwxr-x 1 cloudera cloudera 1570 Sep 14 11:42 reducer_hw23.py\n",
      "packageJobJar: [./mapper_hw23.py, ./reducer_hw23.py, /tmp/hadoop-cloudera/hadoop-unjar5224740272158616354/] [] /tmp/streamjob310337174118204117.jar tmpDir=null\n",
      "15/09/14 11:51:19 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n",
      "15/09/14 11:51:20 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/14 11:51:20 INFO streaming.StreamJob: getLocalDirs(): [/tmp/hadoop-cloudera/mapred/local]\n",
      "15/09/14 11:51:20 INFO streaming.StreamJob: Running job: job_201509131822_0040\n",
      "15/09/14 11:51:20 INFO streaming.StreamJob: To kill this job, run:\n",
      "15/09/14 11:51:20 INFO streaming.StreamJob: UNDEF/bin/hadoop job  -Dmapred.job.tracker=localhost.localdomain:8021 -kill job_201509131822_0040\n",
      "15/09/14 11:51:20 INFO streaming.StreamJob: Tracking URL: http://0.0.0.0:50030/jobdetails.jsp?jobid=job_201509131822_0040\n",
      "15/09/14 11:51:21 INFO streaming.StreamJob:  map 0%  reduce 0%\n",
      "15/09/14 11:51:31 INFO streaming.StreamJob:  map 50%  reduce 0%\n",
      "15/09/14 11:51:32 INFO streaming.StreamJob:  map 100%  reduce 0%\n",
      "15/09/14 11:51:37 INFO streaming.StreamJob:  map 100%  reduce 100%\n",
      "15/09/14 11:51:40 INFO streaming.StreamJob: Job complete: job_201509131822_0040\n",
      "15/09/14 11:51:40 INFO streaming.StreamJob: Output: /user/cloudera/w261/wk2/hw24/output\n",
      "#<Feature>\t<Spam_Score>\t<Ham_Score>\t<Predicted_Class>\n",
      "assistance,valium,enlargementWithATypo\t-26\t-29\tSPAM\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HW2.4. Using the Enron data from HW1 and in the Hadoop MapReduce framework, write  a mapper/reducer pair that\n",
    "   will classify the email messages using multinomial Naive Bayes Classifier using a list of one or more \n",
    "   user-specified words.\n",
    "   (SAME MAPPER AND REDUCER AS IN HW2.3 IS USED)\n",
    "'''\n",
    "\n",
    "# Delete existing Output Dirs if available\n",
    "!hadoop fs -rm -r -skipTrash /user/cloudera/w261/wk2/hw24/output\n",
    "\n",
    "!ls -l *hw23.py\n",
    "\n",
    "# Run the Hadoop Streaming Command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.7.0.jar \\\n",
    "-D mapred.reduce.tasks = 1 \\\n",
    "-input /user/cloudera/w261/wk2/hw24/input/enronemail_1h.txt \\\n",
    "-output /user/cloudera/w261/wk2/hw24/output \\\n",
    "-file ./mapper_hw23.py \\\n",
    "-mapper 'python mapper_hw23.py' \\\n",
    "-file ./reducer_hw23.py \\\n",
    "-reducer 'python reducer_hw23.py' \\\n",
    "-cmdenv WORDS='assistance,valium,enlargementWithATypo' \\\n",
    "\n",
    "# Show Output\n",
    "!hadoop fs -cat /user/cloudera/w261/wk2/hw24/output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
