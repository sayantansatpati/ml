{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sayantan Satpati**\n",
    "* **sayantan.satpati@ischool.berkeley.edu**\n",
    "* **W261**\n",
    "* **Week-3**\n",
    "* **Assignment-4**\n",
    "* **Date of Submission: 22-SEP-2015**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  === Week 3 Hadoop & Apriori ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW3.0. What is a merge sort? Where is it used in Hadoop?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_hw31.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_hw31.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        # Remove leading & trailing chars\n",
    "        line = line.strip()\n",
    "        # Split the line by <TAB> delimeter\n",
    "        items = re.split(r'\\s+', line)\n",
    "        #Sort the list\n",
    "        items.sort()\n",
    "        \n",
    "        for c in itertools.combinations(items, 1):\n",
    "            print '%s,%s\\t%d\\t%d' %(c[0], '*', 1, len(items))\n",
    "            \n",
    "        for c in itertools.combinations(items, 2):\n",
    "            print '%s,%s\\t%d' %(c[0], c[1], 1)\n",
    "        \n",
    "        ''' Commenting out itemset-3 for the moment\n",
    "        for c in itertools.combinations(items, 3):\n",
    "            print '%s,%s,%s\\t%d' %(c[0], c[1], c[2], 1)\n",
    "        '''\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper_hw31.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_hw31.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_hw31.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import heapq\n",
    "from sets import Set\n",
    "\n",
    "'''\n",
    "a1,* 1 4\n",
    "a1,* 1 5\n",
    "a1,b1 1\n",
    "a1,b1 1\n",
    "a1,b2 1\n",
    "a1,b2 1\n",
    "a2,* 1 6\n",
    "'''\n",
    "\n",
    "itemset_1_cnt = 0\n",
    "itemset_2_cnt = 0\n",
    "\n",
    "itemset_1_last = None\n",
    "itemset_2_last = None\n",
    "\n",
    "THRESHOLD = 100\n",
    "\n",
    "# Statistics\n",
    "# Unique Items\n",
    "uniq = Set()\n",
    "# Max Basket Length\n",
    "max_basket_len = 0\n",
    "# Total Itemset Counts for Sizes: 1 & 2\n",
    "total_itemset_1 = 0\n",
    "total_itemset_2 = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Remove leading & trailing chars\n",
    "        line = line.strip()\n",
    "        # Split the line by <TAB> delimeter\n",
    "        tokens = re.split(r'\\s+', line)\n",
    "    \n",
    "        # Split the key by <COMMA> delimeter\n",
    "        items = tokens[0].split(\",\")\n",
    "        i1 = items[0]\n",
    "        i2 = items[1]\n",
    "        \n",
    "        # Count\n",
    "        count = int(tokens[1])\n",
    "        \n",
    "        if itemset_1_last != i1:\n",
    "            uniq.add(i1)\n",
    "            \n",
    "            basket_len = int(tokens[2])\n",
    "            if basket_len >= max_basket_len:\n",
    "                max_basket_len = basket_len\n",
    "                \n",
    "            if itemset_1_cnt >= THRESHOLD:\n",
    "                total_itemset_1 += 1\n",
    "                \n",
    "            # Reset\n",
    "            itemset_1_last = i1\n",
    "            itemset_1_cnt = count\n",
    "            itemset_2_last = None\n",
    "            itemset_2_cnt = 0\n",
    "        else:\n",
    "            if i2 == '*':\n",
    "                itemset_1_cnt += count\n",
    "                \n",
    "                basket_len = int(tokens[2])\n",
    "                if basket_len > max_basket_len:\n",
    "                    max_basket_len = basket_len\n",
    "            else:\n",
    "                if itemset_2_last != tokens[0]:\n",
    "                    if itemset_1_cnt >= THRESHOLD and itemset_2_cnt >= THRESHOLD:\n",
    "                        total_itemset_2 += 1\n",
    "                        \n",
    "                    itemset_2_last = tokens[0]\n",
    "                    itemset_2_cnt = count\n",
    "                else:\n",
    "                    itemset_2_cnt += count\n",
    "                    \n",
    "# Last Set of Counts\n",
    "if itemset_1_cnt >= THRESHOLD:\n",
    "    total_itemset_1 += 1\n",
    "if itemset_1_cnt >= THRESHOLD and itemset_2_cnt >= THRESHOLD:\n",
    "    total_itemset_2 += 1\n",
    "\n",
    "print '=== Statistics ==='\n",
    "print 'Total Unique Items: %d' %(len(uniq))\n",
    "print 'Maximum Basket Length: %d' %(max_basket_len)\n",
    "print 'Total # itemsets of size 1: %d' %(total_itemset_1)\n",
    "print 'Total # itemsets of size 2: %d' %(total_itemset_2)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_hw31.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_hw31.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import heapq\n",
    "from sets import Set\n",
    "\n",
    "'''\n",
    "a1,* 1 4\n",
    "a1,* 1 5\n",
    "a1,b1 1\n",
    "a1,b1 1\n",
    "a1,b2 1\n",
    "a1,b2 1\n",
    "a2,* 1 6\n",
    "'''\n",
    "\n",
    "itemset_1_cnt = 0\n",
    "itemset_2_cnt = 0\n",
    "\n",
    "itemset_1_last = None\n",
    "\n",
    "THRESHOLD = 100\n",
    "\n",
    "# Statistics\n",
    "# Unique Items\n",
    "uniq = Set()\n",
    "# Max Basket Length\n",
    "max_basket_len = 0\n",
    "# Total Itemset Counts for Sizes: 1 & 2\n",
    "total_itemset_1 = 0\n",
    "total_itemset_2 = 0\n",
    "\n",
    "d_counts = {}\n",
    "\n",
    "def update_counts():\n",
    "    global itemset_1_last\n",
    "    global d_counts\n",
    "    global total_itemset_1\n",
    "    global total_itemset_2\n",
    "    key = '{0},{1}'.format(itemset_1_last, '*')\n",
    "    if d_counts.get(key, 0) >= THRESHOLD:\n",
    "        total_itemset_1 += 1\n",
    "        for k,v in d_counts.iteritems():\n",
    "            if k != key:\n",
    "                if v >= THRESHOLD:\n",
    "                    total_itemset_2 += 1\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Remove leading & trailing chars\n",
    "        line = line.strip()\n",
    "        # Split the line by <TAB> delimeter\n",
    "        tokens = re.split(r'\\s+', line)\n",
    "    \n",
    "        # Split the key by <COMMA> delimeter\n",
    "        items = tokens[0].split(\",\")\n",
    "        i1 = items[0]\n",
    "        i2 = items[1]\n",
    "        \n",
    "        # Count\n",
    "        count = int(tokens[1])\n",
    "        \n",
    "        if not itemset_1_last:\n",
    "            itemset_1_last = i1\n",
    "        \n",
    "        if itemset_1_last != i1:\n",
    "            # Emit Contents of Dict\n",
    "            update_counts()\n",
    "            \n",
    "            if i2 == '*':\n",
    "                uniq.add(i1)\n",
    "                basket_len = int(tokens[2])\n",
    "                if basket_len > max_basket_len:\n",
    "                    max_basket_len = basket_len\n",
    "                            \n",
    "            d_counts.clear()\n",
    "            itemset_1_last = i1\n",
    "        else:\n",
    "            key = tokens[0]\n",
    "            d_counts[key] = d_counts.get(key, 0) + count\n",
    "            \n",
    "            if i2 == '*':\n",
    "                uniq.add(i1)\n",
    "                basket_len = int(tokens[2])\n",
    "                if basket_len > max_basket_len:\n",
    "                    max_basket_len = basket_len\n",
    "                    \n",
    "# Last Record\n",
    "update_counts()\n",
    "                    \n",
    "print '=== Statistics ==='\n",
    "print 'Total Unique Items: %d' %(len(uniq))\n",
    "print 'Maximum Basket Length: %d' %(max_basket_len)\n",
    "print 'Total # itemsets of size 1: %d' %(total_itemset_1)\n",
    "print 'Total # itemsets of size 2: %d' %(total_itemset_2)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer_hw31.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/cloudera/w261/wk3/hw31/output\n",
      "packageJobJar: [./mapper_hw31.py, ./reducer_hw31.py, /tmp/hadoop-cloudera/hadoop-unjar4802976138694835527/] [] /tmp/streamjob371666725523987824.jar tmpDir=null\n",
      "15/09/20 12:15:58 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n",
      "15/09/20 12:15:58 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/20 12:15:59 INFO streaming.StreamJob: getLocalDirs(): [/tmp/hadoop-cloudera/mapred/local]\n",
      "15/09/20 12:15:59 INFO streaming.StreamJob: Running job: job_201509191045_0012\n",
      "15/09/20 12:15:59 INFO streaming.StreamJob: To kill this job, run:\n",
      "15/09/20 12:15:59 INFO streaming.StreamJob: UNDEF/bin/hadoop job  -Dmapred.job.tracker=localhost.localdomain:8021 -kill job_201509191045_0012\n",
      "15/09/20 12:15:59 INFO streaming.StreamJob: Tracking URL: http://0.0.0.0:50030/jobdetails.jsp?jobid=job_201509191045_0012\n",
      "15/09/20 12:16:00 INFO streaming.StreamJob:  map 0%  reduce 0%\n",
      "15/09/20 12:16:15 INFO streaming.StreamJob:  map 30%  reduce 0%\n",
      "15/09/20 12:16:16 INFO streaming.StreamJob:  map 57%  reduce 0%\n",
      "15/09/20 12:16:18 INFO streaming.StreamJob:  map 72%  reduce 0%\n",
      "15/09/20 12:16:19 INFO streaming.StreamJob:  map 91%  reduce 0%\n",
      "15/09/20 12:16:22 INFO streaming.StreamJob:  map 100%  reduce 0%\n",
      "15/09/20 12:16:33 INFO streaming.StreamJob:  map 100%  reduce 68%\n",
      "15/09/20 12:16:35 INFO streaming.StreamJob:  map 100%  reduce 69%\n",
      "15/09/20 12:16:39 INFO streaming.StreamJob:  map 100%  reduce 70%\n",
      "15/09/20 12:16:42 INFO streaming.StreamJob:  map 100%  reduce 71%\n",
      "15/09/20 12:16:45 INFO streaming.StreamJob:  map 100%  reduce 72%\n",
      "15/09/20 12:16:46 INFO streaming.StreamJob:  map 100%  reduce 100%\n",
      "15/09/20 12:16:49 INFO streaming.StreamJob: Job complete: job_201509191045_0012\n",
      "15/09/20 12:16:49 INFO streaming.StreamJob: Output: /user/cloudera/w261/wk3/hw31/output\n",
      "=== Statistics ===\t\n",
      "Total Unique Items: 12592\t\n",
      "Maximum Basket Length: 37\t\n",
      "Total # itemsets of size 1: 642\t\n",
      "Total # itemsets of size 2: 1334\t\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HW3.1. Product Recommendations\n",
    "'''\n",
    "\n",
    "# Delete existing Output Dirs if available\n",
    "!hadoop fs -rm -r -skipTrash /user/cloudera/w261/wk3/hw31/output\n",
    "\n",
    "# Run the Hadoop Streaming Command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.7.0.jar \\\n",
    "-D map.output.key.field.separator=, \\\n",
    "-D mapred.text.key.partitioner.options=-k1,1 \\\n",
    "-input /user/cloudera/w261/wk3/hw31/input/ProductPurchaseData.txt \\\n",
    "-output /user/cloudera/w261/wk3/hw31/output \\\n",
    "-file ./mapper_hw31.py \\\n",
    "-mapper 'python mapper_hw31.py' \\\n",
    "-file ./reducer_hw31.py \\\n",
    "-reducer 'python reducer_hw31.py'\n",
    "\n",
    "# Show Output\n",
    "!hadoop fs -cat /user/cloudera/w261/wk3/hw31/output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_hw32.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_hw32.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        # Remove leading & trailing chars\n",
    "        line = line.strip()\n",
    "        # Split the line by <TAB> delimeter\n",
    "        items = re.split(r'\\s+', line)\n",
    "        #Sort the list\n",
    "        items.sort()\n",
    "        \n",
    "        l = len(items)\n",
    "        \n",
    "        for i in xrange(l):\n",
    "            print '%s,*\\t%d' %(items[i], 1)\n",
    "            for j in xrange(i+1, l):\n",
    "               print '%s,%s\\t%d' %(items[i], items[j], 1) \n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper_hw32.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_hw32.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_hw32.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import heapq\n",
    "\n",
    "itemset_1_cnt = 0\n",
    "itemset_2_cnt = 0\n",
    "\n",
    "itemset_1_last = None\n",
    "itemset_2_last = None\n",
    "\n",
    "'''\n",
    "a1,* 1\n",
    "a1,* 1\n",
    "a1,b1 1\n",
    "a1,b1 1\n",
    "a1,b2 1\n",
    "a1,b2 1\n",
    "a2,* 1\n",
    "'''\n",
    "\n",
    "THRESHOLD = 100\n",
    "# Store Itemsets 2\n",
    "dict = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Remove leading & trailing chars\n",
    "        line = line.strip()\n",
    "        # Split the line by <TAB> delimeter\n",
    "        tokens = re.split(r'\\s+', line)\n",
    "    \n",
    "        # Split the key by <COMMA> delimeter\n",
    "        items = tokens[0].split(\",\")\n",
    "        i1 = items[0]\n",
    "        i2 = items[1]\n",
    "        \n",
    "        if not itemset_1_last:\n",
    "            itemset_1_last = i1\n",
    "        \n",
    "        if itemset_1_last != i1:\n",
    "            '''\n",
    "            if itemset_1_cnt >= THRESHOLD:\n",
    "                confidence = (itemset_2_cnt * 1.0) / itemset_1_cnt\n",
    "                print '[%d,%d]%s\\t%f' %(itemset_1_cnt, itemset_2_cnt, tokens[0], confidence)\n",
    "                dict[tokens[0]] = confidence\n",
    "            '''\n",
    "                        \n",
    "            # Reset\n",
    "            itemset_1_last = i1\n",
    "            itemset_1_cnt = int(tokens[1])\n",
    "            itemset_2_last = None\n",
    "            itemset_2_cnt = 0\n",
    "        else:\n",
    "            if i2 == '*':\n",
    "                itemset_1_cnt += int(tokens[1])\n",
    "            else:\n",
    "                if itemset_2_last != tokens[0]:\n",
    "                    if itemset_1_cnt >= THRESHOLD and itemset_2_cnt >= THRESHOLD:\n",
    "                        confidence = (itemset_2_cnt * 1.0) / itemset_1_cnt\n",
    "                        #print '[%d,%d]%s\\t%f' %(itemset_1_cnt, itemset_2_cnt, itemset_2_last, confidence)\n",
    "                        dict[itemset_2_last] = confidence\n",
    "                    itemset_2_last = tokens[0]\n",
    "                    itemset_2_cnt = int(tokens[1]) \n",
    "                else:\n",
    "                    itemset_2_cnt += int(tokens[1])                    \n",
    "\n",
    "# Last Set of Counts\n",
    "if itemset_1_cnt >= THRESHOLD and itemset_2_cnt >= THRESHOLD:\n",
    "    confidence = (itemset_2_cnt * 1.0) / itemset_1_cnt\n",
    "    #print '[%d,%d]%s\\t%f' %(itemset_1_cnt, itemset_2_cnt, itemset_2_last, confidence)\n",
    "    dict[itemset_2_last] = confidence\n",
    "\n",
    "print '=== Top 5 Confidence ==='\n",
    "sorted_dict = sorted(dict.items(), key=lambda x:(-x[1], x[0]))\n",
    "for j,k in sorted_dict[:5]:\n",
    "    print '%s\\t%f' %(j,k)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer_hw32.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/cloudera/w261/wk3/hw32/output\n",
      "packageJobJar: [./mapper_hw32.py, ./reducer_hw32.py, /tmp/hadoop-cloudera/hadoop-unjar8486018449480148677/] [] /tmp/streamjob286663036493526101.jar tmpDir=null\n",
      "15/09/20 13:08:58 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n",
      "15/09/20 13:08:58 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/20 13:08:58 INFO streaming.StreamJob: getLocalDirs(): [/tmp/hadoop-cloudera/mapred/local]\n",
      "15/09/20 13:08:58 INFO streaming.StreamJob: Running job: job_201509191045_0014\n",
      "15/09/20 13:08:58 INFO streaming.StreamJob: To kill this job, run:\n",
      "15/09/20 13:08:58 INFO streaming.StreamJob: UNDEF/bin/hadoop job  -Dmapred.job.tracker=localhost.localdomain:8021 -kill job_201509191045_0014\n",
      "15/09/20 13:08:59 INFO streaming.StreamJob: Tracking URL: http://0.0.0.0:50030/jobdetails.jsp?jobid=job_201509191045_0014\n",
      "15/09/20 13:09:00 INFO streaming.StreamJob:  map 0%  reduce 0%\n",
      "15/09/20 13:09:17 INFO streaming.StreamJob:  map 53%  reduce 0%\n",
      "15/09/20 13:09:20 INFO streaming.StreamJob:  map 83%  reduce 0%\n",
      "15/09/20 13:09:23 INFO streaming.StreamJob:  map 100%  reduce 0%\n",
      "15/09/20 13:09:34 INFO streaming.StreamJob:  map 100%  reduce 68%\n",
      "15/09/20 13:09:37 INFO streaming.StreamJob:  map 100%  reduce 69%\n",
      "15/09/20 13:09:40 INFO streaming.StreamJob:  map 100%  reduce 70%\n",
      "15/09/20 13:09:43 INFO streaming.StreamJob:  map 100%  reduce 71%\n",
      "15/09/20 13:09:46 INFO streaming.StreamJob:  map 100%  reduce 72%\n",
      "15/09/20 13:09:49 INFO streaming.StreamJob:  map 100%  reduce 100%\n",
      "15/09/20 13:09:52 INFO streaming.StreamJob: Job complete: job_201509191045_0014\n",
      "15/09/20 13:09:52 INFO streaming.StreamJob: Output: /user/cloudera/w261/wk3/hw32/output\n",
      "=== Top 5 Confidence ===\t\n",
      "DAI93865,FRO40251\t1.000000\n",
      "ELE12951,FRO40251\t0.990566\n",
      "DAI88079,FRO40251\t0.986726\n",
      "DAI43868,SNA82528\t0.972973\n",
      "DAI23334,DAI62779\t0.954545\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HW3.2. Confidence Caculations\n",
    "List the top 5 rules with corresponding confidence scores in decreasing order of confidence score \n",
    "for frequent (100>count) itemsets of size 2\n",
    "'''\n",
    "\n",
    "# Delete existing Output Dirs if available\n",
    "!hadoop fs -rm -r -skipTrash /user/cloudera/w261/wk3/hw32/output\n",
    "\n",
    "# Run the Hadoop Streaming Command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.7.0.jar \\\n",
    "-D map.output.key.field.separator=, \\\n",
    "-D mapred.text.key.partitioner.options=-k1,1 \\\n",
    "-input /user/cloudera/w261/wk3/hw32/input/ProductPurchaseData.txt \\\n",
    "-output /user/cloudera/w261/wk3/hw32/output \\\n",
    "-file ./mapper_hw32.py \\\n",
    "-mapper 'python mapper_hw32.py' \\\n",
    "-file ./reducer_hw32.py \\\n",
    "-reducer 'python reducer_hw32.py'\n",
    "\n",
    "# Show Output\n",
    "!hadoop fs -cat /user/cloudera/w261/wk3/hw32/output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/cloudera/w261/wk2/hw21/output\n",
      "File: /home/cloudera/dev/ml/w261/wk3/./identity_map_red_hw21.py does not exist, or is not readable.\n",
      "Streaming Command Failed!\n",
      "cat: `/user/cloudera/w261/wk2/hw21/output/part-00000': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HW2.1. Sort in Hadoop MapReduce\n",
    "'''\n",
    "\n",
    "# Delete existing Output Dirs if available\n",
    "!hadoop fs -rm -r -skipTrash /user/cloudera/w261/wk2/hw21/output\n",
    "\n",
    "# Run the Hadoop Streaming Command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.7.0.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D  mapred.text.key.comparator.options=-n \\\n",
    "-input /user/cloudera/w261/wk2/hw21/input/random.txt \\\n",
    "-output /user/cloudera/w261/wk2/hw21/output \\\n",
    "-file ./identity_map_red_hw21.py \\\n",
    "-mapper ./identity_map_red_hw21.py \\\n",
    "-file ./identity_map_red_hw21.py \\\n",
    "-reducer ./identity_map_red_hw21.py\n",
    "\n",
    "# Show Output\n",
    "!hadoop fs -cat /user/cloudera/w261/wk2/hw21/output/part-00000 | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_hw22.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_hw22.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def strip_special_chars(word):\n",
    "    return re.sub('[^A-Za-z0-9]+', '', word)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        # Remove leading & trailing chars\n",
    "        line = line.strip()\n",
    "        # Split the line by <TAB> delimeter\n",
    "        email = re.split(r'\\t+', line)\n",
    "\n",
    "        # Check whether Content is present\n",
    "        if len(email) < 4:\n",
    "            continue\n",
    "\n",
    "        # Get the content as a list of words\n",
    "        content = email[len(email) - 1].split()\n",
    "\n",
    "        for w in content:\n",
    "            w = strip_special_chars(w)\n",
    "            if w == 'assistance':\n",
    "                print '%s\\t%d' % (w, 1)\n",
    "    except Exception as e:\n",
    "        print line\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper_hw22.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_hw22.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_hw22.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "word = None\n",
    "count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # Remove leading & trailing chars\n",
    "    line = line.strip()\n",
    "    # Split the line by <TAB> delimeter\n",
    "    wc = re.split(r'\\t+', line)\n",
    "    \n",
    "    word = wc[0]\n",
    "    count += int(wc[1])\n",
    "    \n",
    "print '%s\\t%d' % (word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer_hw22.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/cloudera/w261/wk2/hw22/output\n",
      "-rwxrwxr-x 1 cloudera cloudera 695 Sep 14 12:14 mapper_hw22.py\n",
      "-rwxrwxr-x 1 cloudera cloudera 307 Sep 14 12:14 reducer_hw22.py\n",
      "packageJobJar: [./mapper_hw22.py, ./reducer_hw22.py, /tmp/hadoop-cloudera/hadoop-unjar4404270376137637736/] [] /tmp/streamjob6249198798003576990.jar tmpDir=null\n",
      "15/09/14 12:14:26 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n",
      "15/09/14 12:14:26 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/14 12:14:27 INFO streaming.StreamJob: getLocalDirs(): [/tmp/hadoop-cloudera/mapred/local]\n",
      "15/09/14 12:14:27 INFO streaming.StreamJob: Running job: job_201509131822_0042\n",
      "15/09/14 12:14:27 INFO streaming.StreamJob: To kill this job, run:\n",
      "15/09/14 12:14:27 INFO streaming.StreamJob: UNDEF/bin/hadoop job  -Dmapred.job.tracker=localhost.localdomain:8021 -kill job_201509131822_0042\n",
      "15/09/14 12:14:27 INFO streaming.StreamJob: Tracking URL: http://0.0.0.0:50030/jobdetails.jsp?jobid=job_201509131822_0042\n",
      "15/09/14 12:14:28 INFO streaming.StreamJob:  map 0%  reduce 0%\n",
      "15/09/14 12:14:40 INFO streaming.StreamJob:  map 100%  reduce 0%\n",
      "15/09/14 12:14:47 INFO streaming.StreamJob:  map 100%  reduce 100%\n",
      "15/09/14 12:14:50 INFO streaming.StreamJob: Job complete: job_201509131822_0042\n",
      "15/09/14 12:14:50 INFO streaming.StreamJob: Output: /user/cloudera/w261/wk2/hw22/output\n",
      "assistance\t9\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HW2.2. Using the Enron data from HW1 and Hadoop MapReduce streaming, \n",
    "write mapper/reducer pair that  will determine the number of occurrences of a single, \n",
    "user-specified word. Examine the word “assistance” and report your results.\n",
    "'''\n",
    "\n",
    "# Delete existing Output Dirs if available\n",
    "!hadoop fs -rm -r -skipTrash /user/cloudera/w261/wk2/hw22/output\n",
    "\n",
    "!ls -l *hw22.py\n",
    "\n",
    "# Run the Hadoop Streaming Command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.7.0.jar \\\n",
    "-D mapred.reduce.tasks = 1 \\\n",
    "-input /user/cloudera/w261/wk2/hw22/input/enronemail_1h.txt \\\n",
    "-output /user/cloudera/w261/wk2/hw22/output \\\n",
    "-file ./mapper_hw22.py \\\n",
    "-mapper 'python mapper_hw22.py' \\\n",
    "-file ./reducer_hw22.py \\\n",
    "-reducer 'python reducer_hw22.py'\n",
    "\n",
    "# Show Output\n",
    "!hadoop fs -cat /user/cloudera/w261/wk2/hw22/output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW2.2: The word assistance has 9 occurrences if we perform the following steps:\n",
    "1. Tokenization\n",
    "2. Remove Special Chars\n",
    "3. Don't include emails which have a bad format (3 cols as opposed to 4 cols)\n",
    "4. Include only Email Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_hw23.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_hw23.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Output from mapp\n",
    "vocab = set()\n",
    "word_counts = {\n",
    "    \"1\": {},\n",
    "    \"0\": {}\n",
    "}\n",
    "total = 0\n",
    "total_spam = 0\n",
    "total_ham = 0\n",
    "\n",
    "word_list = os.environ['WORDS'].split(\",\")\n",
    "\n",
    "def strip_special_chars(word):\n",
    "    word = word.strip().lower()\n",
    "    return re.sub('[^A-Za-z0-9]+', '', word)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        # Remove leading & trailing chars\n",
    "        line = line.strip()\n",
    "        # Split the line by <TAB> delimeter\n",
    "        email = re.split(r'\\t+', line)\n",
    "\n",
    "        # Check whether Content is present\n",
    "        if len(email) < 4:\n",
    "            continue\n",
    "\n",
    "        # Get the content as a list of words\n",
    "        spam = email[1]\n",
    "        content = email[len(email) - 1].split()\n",
    "        \n",
    "        # Totals\n",
    "        total += 1\n",
    "        if spam == '1':\n",
    "            total_spam += 1\n",
    "        else:\n",
    "            total_ham += 1\n",
    "\n",
    "        for w in content:\n",
    "            w = strip_special_chars(w)\n",
    "            \n",
    "            # Add to category dict\n",
    "            word_counts[spam][w] = word_counts[spam].get(w, 0) + 1\n",
    "                \n",
    "            # Vocab Unique\n",
    "            vocab.add(w)\n",
    "    except Exception as e:\n",
    "        print line\n",
    "        print e\n",
    "        \n",
    "print 'TOTAL_DOCUMENTS\\t%d\\t%d\\t%d' % (total,total_spam,total_ham)\n",
    "print 'TOTAL_WORDS\\t%d\\t%d\\t%d' % (len(vocab), sum(word_counts['1'].values()), sum(word_counts['0'].values()))\n",
    "for w in word_list:\n",
    "    print '%s\\t%d\\t%d' %(w, word_counts['1'].get(w, 0.0), word_counts['0'].get(w, 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper_hw23.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_hw23.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_hw23.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "\n",
    "# Totals from Mapper\n",
    "total = 0\n",
    "total_spam = 0\n",
    "total_ham = 0\n",
    "\n",
    "vocab = 0\n",
    "vocab_spam = 0\n",
    "vocab_ham = 0\n",
    "word_count = {}\n",
    "\n",
    "word_list = os.environ['WORDS'].split(\",\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "         # Remove leading & trailing chars\n",
    "        line = line.strip()\n",
    "        # Split the line by <TAB> delimeter\n",
    "        tokens = re.split(r'\\t+', line)\n",
    "        \n",
    "        if tokens[0] == 'TOTAL_DOCUMENTS':\n",
    "            total += int(tokens[1])\n",
    "            total_spam += int(tokens[2])\n",
    "            total_ham += int(tokens[3])\n",
    "        elif tokens[0] == 'TOTAL_WORDS':\n",
    "            vocab = int(tokens[1])\n",
    "            vocab_spam = int(tokens[2])\n",
    "            vocab_ham = int(tokens[3])\n",
    "        else:\n",
    "            word_count[tokens[0]] = (int(tokens[1]), int(tokens[2]))\n",
    "    except Exception as e:\n",
    "        sys.exit(1)\n",
    "\n",
    "prior_spam = (total_spam * 1.0) / total\n",
    "prior_ham = (total_ham * 1.0) / total\n",
    "\n",
    "spam_lhood_denom = vocab_spam + vocab\n",
    "ham_lhood_denom = vocab_ham + vocab\n",
    "spam_lhood_log = 0.0\n",
    "ham_lhood_log = 0.0\n",
    "for w in word_list:\n",
    "    spam_lhood_log += math.log( (word_count[w][0] + 1.0) * 1.0 / spam_lhood_denom )\n",
    "    ham_lhood_log += math.log( (word_count[w][1] + 1.0) * 1.0 / ham_lhood_denom )\n",
    "spam_score = spam_lhood_log + math.log(prior_spam)\n",
    "ham_score = ham_lhood_log + math.log(prior_ham)\n",
    "\n",
    "classification = 'HAM'\n",
    "if spam_score > ham_score:\n",
    "    classification = 'SPAM'\n",
    "   \n",
    "print '#<Feature>\\t<Spam_Score>\\t<Ham_Score>\\t<Predicted_Class>'\n",
    "print '%s\\t%f\\t%f\\t%s' %(\",\".join(word_list), spam_score, ham_score, classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer_hw23.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/cloudera/w261/wk2/hw23/output\n",
      "-rwxrwxr-x 1 cloudera cloudera 1464 Sep 14 13:24 mapper_hw23.py\n",
      "-rwxrwxr-x 1 cloudera cloudera 1582 Sep 14 13:26 reducer_hw23.py\n",
      "packageJobJar: [./mapper_hw23.py, ./reducer_hw23.py, /tmp/hadoop-cloudera/hadoop-unjar2678392748320022312/] [] /tmp/streamjob7567663044371259044.jar tmpDir=null\n",
      "15/09/14 13:27:09 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n",
      "15/09/14 13:27:09 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "15/09/14 13:27:10 INFO streaming.StreamJob: getLocalDirs(): [/tmp/hadoop-cloudera/mapred/local]\n",
      "15/09/14 13:27:10 INFO streaming.StreamJob: Running job: job_201509131822_0054\n",
      "15/09/14 13:27:10 INFO streaming.StreamJob: To kill this job, run:\n",
      "15/09/14 13:27:10 INFO streaming.StreamJob: UNDEF/bin/hadoop job  -Dmapred.job.tracker=localhost.localdomain:8021 -kill job_201509131822_0054\n",
      "15/09/14 13:27:10 INFO streaming.StreamJob: Tracking URL: http://0.0.0.0:50030/jobdetails.jsp?jobid=job_201509131822_0054\n",
      "15/09/14 13:27:11 INFO streaming.StreamJob:  map 0%  reduce 0%\n",
      "15/09/14 13:27:23 INFO streaming.StreamJob:  map 100%  reduce 0%\n",
      "15/09/14 13:27:28 INFO streaming.StreamJob:  map 100%  reduce 100%\n",
      "15/09/14 13:27:31 INFO streaming.StreamJob: Job complete: job_201509131822_0054\n",
      "15/09/14 13:27:31 INFO streaming.StreamJob: Output: /user/cloudera/w261/wk2/hw23/output\n",
      "#<Feature>\t<Spam_Score>\t<Ham_Score>\t<Predicted_Class>\n",
      "assistance\t-8.631765\t-10.055939\tSPAM\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HW2.3. Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer pair that\n",
    "   will classify the email messages by a single, user-specified word. \n",
    "   Examine the word “assistance” and report your results.\n",
    "   \n",
    "RESULT: The document is classified as a SPAM.\n",
    "'''\n",
    "\n",
    "# Delete existing Output Dirs if available\n",
    "!hadoop fs -rm -r -skipTrash /user/cloudera/w261/wk2/hw23/output\n",
    "\n",
    "!ls -l *hw23.py\n",
    "\n",
    "# Run the Hadoop Streaming Command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.7.0.jar \\\n",
    "-D mapred.reduce.tasks = 1 \\\n",
    "-input /user/cloudera/w261/wk2/hw23/input/enronemail_1h.txt \\\n",
    "-output /user/cloudera/w261/wk2/hw23/output \\\n",
    "-file ./mapper_hw23.py \\\n",
    "-mapper 'python mapper_hw23.py' \\\n",
    "-file ./reducer_hw23.py \\\n",
    "-reducer 'python reducer_hw23.py' \\\n",
    "-cmdenv WORDS='assistance' \\\n",
    "\n",
    "# Show Output\n",
    "!hadoop fs -cat /user/cloudera/w261/wk2/hw23/output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW2.3: Document is classified as a SPAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "HW2.4. Using the Enron data from HW1 and in the Hadoop MapReduce framework, write  a mapper/reducer pair that\n",
    "   will classify the email messages using multinomial Naive Bayes Classifier using a list of one or more \n",
    "   user-specified words.\n",
    "   (SAME MAPPER AND REDUCER AS IN HW2.3 IS USED, BUT WITH DIFFERENT PARAMETERS PASSED IN -cmdenv)\n",
    "   \n",
    "RESULT: The document is classified as a SPAM.\n",
    "'''\n",
    "\n",
    "# Delete existing Output Dirs if available\n",
    "!hadoop fs -rm -r -skipTrash /user/cloudera/w261/wk2/hw24/output\n",
    "\n",
    "!ls -l *hw23.py\n",
    "\n",
    "# Run the Hadoop Streaming Command\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.7.0.jar \\\n",
    "-D mapred.reduce.tasks = 1 \\\n",
    "-input /user/cloudera/w261/wk2/hw24/input/enronemail_1h.txt \\\n",
    "-output /user/cloudera/w261/wk2/hw24/output \\\n",
    "-file ./mapper_hw23.py \\\n",
    "-mapper 'python mapper_hw23.py' \\\n",
    "-file ./reducer_hw23.py \\\n",
    "-reducer 'python reducer_hw23.py' \\\n",
    "-cmdenv WORDS='assistance,valium,enlargementWithATypo' \\\n",
    "\n",
    "# Show Output\n",
    "!hadoop fs -cat /user/cloudera/w261/wk2/hw24/output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### HW2.4: Document is classified as a SPAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
